[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Instructor: Trevor Ruiz\nTeaching assistants: Mengye Liu, Harry Yu, Gabrielle Salo\nClass meetings: M-W 12:30pm – 1:45pm Buchanan 1920\nSection meetings:\n\nM 2:00pm – 2:50pm Phelps 1525 (Mengye)\nM 3:00pm – 3:50pm Phelps 1525 (Mengye)\nM 4:00pm – 4:50pm Phelps 1513 (Harry)\nM 5:00pm – 5:50pm Phelps 1525 (Harry)\nM 6:00pm – 6:50pm Phelps 1525 (Gabrielle)\n\nOffice hours:\n\nMengye M 9:00am – 11:00am on Zoom\nGabrielle Tu 10:00am – 11:00am on Zoom\nHarry W 2:00pm – 4:00pm Building 434 Room 113\nTrevor W 2:00pm – 3:00pm ILP 2207"
  },
  {
    "objectID": "about.html#content-and-materials",
    "href": "about.html#content-and-materials",
    "title": "Course syllabus",
    "section": "Content and materials",
    "text": "Content and materials\nData Science Concepts and Analysis (PSTAT100) is a hands-on introduction to data science intended for intermediate-level students from any discipline with some exposure to probability and basic computing skills, but few or no upper-division courses in statistics or computer science. The course introduces central concepts in statistics – such as sampling variation, uncertainty, and inference – in an applied setting together with techniques for data exploration and analysis. Applications emphasize end-to-end data analyses. Course activities model standard data science workflow practices by example, and successful students acquire programming skills, project management skills, and subject exposure that will serve them well in upper-division courses as well as in independent research or projects.\n\nCatalog description\nOverview of data science key concepts and the use of tools for data retrieval, analysis, visualization, and reproducible research. Topics include an introduction to inference and prediction, principles of measurement, missing data, and notions of causality, statistical “traps”, and concepts in data ethics and privacy. Case studies will illustrate the importance of domain knowledge. Credit units: 4.\n\n\nPrerequisites\n\nProbability and Statistics I (PSTAT 120A)\nLinear Algebra (MATH 4A)\nPrior experience with Python or another programming language (CMPSC 9 or CMPSC 16).\n\n\n\nLearning outcomes\nSuccessful students will establish foundational data science skills:\n\ncritical assessment of data quality and sampling design\nretrieval, inspection, and cleaning of raw data\nexploratory, descriptive, visual, and inferential techniques\ninterpretation and communication of results in context\n\nThese skills will be discussed in depth during course lectures; students will practice them through lab activities, homework assignments, and project work.\n\n\nAssessments\nAttainment of course learning outcomes will be measured by assessment of submitted work. Submitted work falls into four categories:\n\nLabs will be assigned weekly in most weeks. These are structured coding assignments with small exercises throughout that introduce the programming skills needed to complete homework assignments.\nHomeworks will be assigned biweekly. These are fairly involved assignments which apply concepts and techniques from the lectures and programming skills from the labs to real data sets in order to reproduce an analysis and answer substantive questions. Collaboration is encouraged, and group submissions will be allowed for groups of up to 3 students.\nMini projects will be assigned biweekly in alternation with homeworks. These assignments prompt students to use skills from the course in an unstructured setting to answer high-level questions pertaining to one or more datasets. Mini projects should be completed collaboratively.\nA course project will be assigned requiring students to carry out an open-ended data analysis. This will be completed in teams. Each team will prepare a project plan for initial feedback a few weeks before the end of the quarter, and submit a final report of work and findings by the end of the quarter.\n\nOverall scores in the course will be calculated for each student as the weighted average of scores on all submitted work; the relative weighting and letter grade assignments will depend entirely on the score distribution of the class as a whole and as such reflect each student’s performance relative to their peers.\n\n\nSchedule\nThe tentative topic and assignment schedule is given below. Assignments are indicated by due date: all assignments are due by Monday 11:59pm in the week indicated. Late submissions are allowed, with a possible penalty, for up to 48 hours.\nThe schedule is subject to change based on the progress of the class.\n\n\n\nWeek\nTopic\nLab\nHomework\nProject\n\n\n\n\n1\nData science life cycle\n\n\n\n\n\n2\nTidy data\nL0\n\n\n\n\n3\nSampling and bias\nL1\n\n\n\n\n4\nStatistical graphics\nL2\nH1\n\n\n\n5\nKernel density estimation\nL3\n\nMP1\n\n\n6\nPrincipal components\nL4\nH2\n\n\n\n7\nSimple regression\n\n\nMP2\n\n\n8\nMultiple regression\nL5\nH3\n\n\n\n9\nClassification and clustering\n\n\nCP1\n\n\n10\nCase study\n\nH4\n\n\n\n11\nFinals\n\n\nCP2\n\n\n\n\nL: lab\nH: homework\nMP: mini project\nCP: course project\n\n\n\nMaterials\nThe course website ruizt.github.io/pstat100 will link to all course content and resources. Readings for the course will draw on multiple sources, including:\n\nPython Data Science Handbook (PDSH);\nLearning Data Science (LDS);\ncollected articles distributed as assigned.\n\n\n\nSoftware\nComputing will be hosted via the course LSIT server pstat100.lsit.ucsb.edu. Students need only a web browser and stable internet connection to complete all course work. It is strongly recommended that students download backup copies of their assignments from the LSIT server.\nInterested students are encouraged to install the software needed to open, edit, and execute notebooks on their own machine, in particular:\n\na Python install;\n(recommended) Miniconda\nJupyter;\npackages utilized in course materials (primarily numpy, pandas, altair, and scikit-learn).\n\nManaging package installations will require some (straightforward) use of the package installer pip or pip3 in the terminal to retrieve/install packages from the Python Package Index repository. Documentation for specific packages (or a Google search) will indicate the appropriate pip command."
  },
  {
    "objectID": "about.html#policies",
    "href": "about.html#policies",
    "title": "Course syllabus",
    "section": "Policies",
    "text": "Policies\n\nCommunication\nThere are two primary means of communication outside of scheduled class meetings: office hours and a discussion board.\nCourse staff have limited availability via email. Course staff will make every effort to respond to individual communication within 48 weekday-hours on the following (or similar) matters:\n\naccommodations/extensions due to personal circumstances;\nlogistical issues such as access to materials or missing scores;\ngeneral advising.\n\nEmail should not be used to ask content questions or submit assignments (unless specifically requested). Emails related to the following (or similar) matters may not receive replies and should be redirected:\n\n\n\nTopic\nRedirect to…\n\n\n\n\nTroubleshooting codes\nDiscussion board\n\n\nChecking answers\nOffice hours or discussion board\n\n\nClarifying assignment content\nOffice hours or discussion board\n\n\nAssignment submission\nGradescope\n\n\nRe-evaluation request\nGradescope\n\n\n\n\n\nExpected time commitment\nThe course is 4 credit units; each credit unit corresponds to an approximate time commitment of 3 hours. So, students should expect to allocate 12 hours per week to the course on average. Course staff are available to help any students spending considerably more time on the class balance the workload.\n\n\nScores and grades\nScores on submitted work can be monitored on Gradescope to ensure fair assignment of course grades. On any individual assignment, re-evaluation can be requested within one week of receiving a score. Requests for re-evaluation made beyond one week after publication of scores may or may not be considered on a discretionary basis.\nDetermination of letter grade assignments is made entirely at the discretion of the instructor based on the assessments outlined above and consistent with university policy. Students are not permitted to negotiate their grades, and are discouraged from requesting audits, recalculations, or verification of self-calculations after the course has concluded. The instructor is under no obligation to share the details of grade calculations with students or to respond to such requests.\nIf at the end of the course a student believes their grade was unfairly assigned, either due to discrimination or without basis in coursework, they are entitled to contest it according to the procedure outlined here.\n\n\nConduct\nStudents are expected to uphold the student code of conduct and to maintain integrity. All individually-submitted work must be an honest reflection of individual effort. Evidence of dishonest conduct will be discussed with the student(s) involved and reported to the Office of Student Conduct (OSC). Depending on the nature of the evidence and the violation, penalty in the course may range from a warning to loss of credit to automatic failure. For a definition and examples of dishonesty, a discussion of what constitutes an appropriate response from faculty, and an explanation of the reporting and investigation process, see the OSC page on academic integrity.\n\n\nDeadlines and late work\nThere is a one-hour grace period on all submission deadlines. After that, work may be submitted within 48 hours of the original deadline (not the deadline plus grace period) and will be considered late. Every student can submit two late assignments without penalty. Subsequent late submissions will be evaluated for 75% credit.\n\n\nAccommodations\nReasonable accommodations will be made for any student with a qualifying disability. Such requests should be made through the Disabled Students Program (DSP). More information, instructions on how to access accommodations, and information on related resources can be found on the DSP website.\n\n\nFeedback\nToward the end of the term students will be given an opportunity to provide feedback about the course via ESCI. This feedback is valuable for improvement of the course in future terms, and students are strongly encouraged to provide thoughtful course evaluations. The identities of student respondents to ESCI surveys are not disclosed to instructors."
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Materials",
    "section": "",
    "text": "Attendance form (fill out once per class meeting, including sections)\nGradescope (for assignment submissions)\nLSIT server (for course computing)"
  },
  {
    "objectID": "content.html#getting-started-checklist",
    "href": "content.html#getting-started-checklist",
    "title": "Materials",
    "section": "Getting started checklist",
    "text": "Getting started checklist\n\nConfirm access to all course pages\nRead syllabus\nFill out intake survey"
  },
  {
    "objectID": "content.html#week-1",
    "href": "content.html#week-1",
    "title": "Materials",
    "section": "Week 1",
    "text": "Week 1\nReadings:\n\nLDS1 The Data Science Lifecycle\nLDS5 Case Study: Why is my Bus Always Late?\nPDSH2.1 Understanding data types in python\nPDSH2.2 The basics of numpy arrays\nPDSH2.4 Aggregations: min, max, and everything in between\n\nMonday: Course introduction [slides]\nLab sections: Orientation to Jupyter notebooks [html] [notebook]\nWednesday: Data science lifecycle [slides]"
  },
  {
    "objectID": "content.html#week-2",
    "href": "content.html#week-2",
    "title": "Materials",
    "section": "Week 2",
    "text": "Week 2\nReadings:\n\nWickham (2014). Tidy data. Journal of statistical software 59(10). [link to paper]\nPDSH3.1 Introducing pandas objects\nPDSH3.2 Data indexing and selection\nPDSH3.7 Merge and join\nPDSH3.8 Aggregation and grouping\n\nMonday: Tidy data [slides]\nLab sections: Pandas [html] [notebook]\nWednesday: Dataframe transformations [slides]\nAssignment: HW1, BRFSS case study, due Monday, April 24 [html] [notebook]"
  },
  {
    "objectID": "hw/hw1-brfss/hw1-brfss-soln.html",
    "href": "hw/hw1-brfss/hw1-brfss-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"hw1-brfss.ipynb\")"
  },
  {
    "objectID": "hw/hw1-brfss/hw1-brfss-soln.html#recoding-categorical-variables",
    "href": "hw/hw1-brfss/hw1-brfss-soln.html#recoding-categorical-variables",
    "title": "PSTAT100",
    "section": "Recoding categorical variables",
    "text": "Recoding categorical variables\nThe variable entries are coded numerically to represent certain responses. These should be replaced by more informative entries. We can use the codebook to determine which number means what, and replace the values accordingly.\nThe cell below replaces the numeric values for _AGEG5YR by their meanings, illustrating how to use .replace() with a dictionary to convert the numeric coding to interpretable values. The basic strategy is:\n\nStore the variable coding for VAR as a dictionary var_codes.\nUse .replace({'VAR': var_codes}) to modify values.\n\nIf you need additional examples, check the pandas documentation for .replace().\n\n# dictionary representing variable coding\nage_codes = {\n    1: '18-24', 2: '25-29', 3: '30-34',\n    4: '35-39', 5: '40-44', 6: '45-49',\n    7: '50-54', 8: '55-59', 9: '60-64',\n    10: '65-69', 11: '70-74', 12: '75-79',\n    13: '80+', 14: 'Unsure/refused/missing'\n}\n\n# recode age categories\nsamp_mod1 = samp.replace({'_AGEG5YR': age_codes})\n\n# check result\nsamp_mod1.head()\n\n\n\n\n\n  \n    \n      \n      GENHLTH\n      ADDEPEV3\n      ACEDEPRS\n      ACEDRINK\n      ACEDRUGS\n      ACEPRISN\n      _LLCPWT\n      _SEX\n      _AGEG5YR\n      _SMOKER3\n    \n  \n  \n    \n      237125\n      5.0\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.057004\n      2.0\n      25-29\n      3.0\n    \n    \n      329116\n      5.0\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.108336\n      2.0\n      80+\n      3.0\n    \n    \n      178937\n      3.0\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.000998\n      1.0\n      18-24\n      4.0\n    \n    \n      410081\n      4.0\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.021973\n      2.0\n      45-49\n      2.0\n    \n    \n      184555\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.027175\n      2.0\n      80+\n      3.0\n    \n  \n\n\n\n\n\nQuestion 5: Recoding variables\nFollowing the example immediately above and referring to the 2019 BRFSS codebook, replace the numeric codings with response categories for each of the following variables:\n\n_SEX\nGENHLTH\n_SMOKER3\n\nNotice that above, the first modification (slicing) was stored as samp_mod1, and was a function of samp. You’ll follow this pattern, creating samp_mod2, samp_mod3, and so on so that each step (modification) of your data manipulations is stored separately, for easy troubleshooting.\n\nRecode _SEX: define a new dataframe samp_mod2 that is the same as samp_mod1 but with the _SEX variable recoded as M and F.\nRecode GENHLTH: define a new dataframe samp_mod3 that is the same as samp_mod2 but with the GENHLTH variable recoded as Excellent, Very good, Good, Fair, Poor, Unsure, and Refused.\nRecode _SMOKER3: define a new dataframe samp_mod4 that is the same as samp_mod3 but with _SMOKER3 recoded as Daily, Some days, Former, Never, and Unsure/refused/missing.\nPrint the first few rows of samp_mod4.\n\n\n# define dictionary for sex\nsex_codes = {1: 'M', 2: 'F'} # SOLUTION\n\n# recode sex\nsamp_mod2 = samp_mod1.replace({'_SEX': sex_codes}) # SOLUTION\n\n# define dictionary for health\nhealth_codes = { 1: 'Excellent', 2: 'Very good', 3: 'Good', 4: 'Fair', 5: 'Poor', 7: 'Unsure', 9: 'Refused'} # SOLUTION\n\n# recode health\nsamp_mod3 = samp_mod2.replace({'GENHLTH': health_codes}) # SOLUTION\n\n# define dictionary for smoking\nsmoke_codes = { 1: 'Daily', 2: 'Some days', 3: 'Former', 4: 'Never', 9: 'Unsure/refused/missing'} # SOLUTION\n\n# recode smoking\nsamp_mod4 = samp_mod3.replace({'_SMOKER3': smoke_codes}) # SOLUTION\n\n# print a few rows\nsamp_mod4.head() # SOLUTION\n\n\n\n\n\n  \n    \n      \n      GENHLTH\n      ADDEPEV3\n      ACEDEPRS\n      ACEDRINK\n      ACEDRUGS\n      ACEPRISN\n      _LLCPWT\n      _SEX\n      _AGEG5YR\n      _SMOKER3\n    \n  \n  \n    \n      237125\n      Poor\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.057004\n      F\n      25-29\n      Former\n    \n    \n      329116\n      Poor\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.108336\n      F\n      80+\n      Former\n    \n    \n      178937\n      Good\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.000998\n      M\n      18-24\n      Never\n    \n    \n      410081\n      Fair\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.021973\n      F\n      45-49\n      Some days\n    \n    \n      184555\n      Very good\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.027175\n      F\n      80+\n      Former\n    \n  \n\n\n\n\n\ngrader.check(\"q5\")\n\n\n\nQuestion 6: Value replacement\nNow all the variables except the adverse childhood experience and depressive disorder question responses are represented interpretably. In the codebook that the answer key is identical for these remaining variables.\nThe numeric codings can be replaced all at once by applying .replace() to the dataframe with an argument of the form\n\ndf.replace({'var1': varcodes1, 'var2': varcodes1, ..., 'varp': varcodesp})\n\nDefine a new dataframe samp_mod5 that is the same as samp_mod4 but with the remaining variables recoded according to the answer key Yes, No, Unsure, Refused. Print the first few rows of the result using .head().\n\n# define dictionary\nanswer_codes = {1: 'Yes', 2: 'No', 7: 'Unsure', 9: 'Refused'} #SOLUTION\n\n# recode\nsamp_mod5 =  samp_mod4.replace({'ACEPRISN': answer_codes, 'ACEDRUGS': answer_codes, 'ACEDRINK': answer_codes, 'ACEDEPRS': answer_codes, 'ADDEPEV3': answer_codes}) #SOLUTION\n\n# check using head()\nsamp_mod5.head() #SOLUTION\n\n\n\n\n\n  \n    \n      \n      GENHLTH\n      ADDEPEV3\n      ACEDEPRS\n      ACEDRINK\n      ACEDRUGS\n      ACEPRISN\n      _LLCPWT\n      _SEX\n      _AGEG5YR\n      _SMOKER3\n    \n  \n  \n    \n      237125\n      Poor\n      No\n      NaN\n      NaN\n      NaN\n      NaN\n      0.057004\n      F\n      25-29\n      Former\n    \n    \n      329116\n      Poor\n      No\n      NaN\n      NaN\n      NaN\n      NaN\n      0.108336\n      F\n      80+\n      Former\n    \n    \n      178937\n      Good\n      No\n      NaN\n      NaN\n      NaN\n      NaN\n      0.000998\n      M\n      18-24\n      Never\n    \n    \n      410081\n      Fair\n      Yes\n      NaN\n      NaN\n      NaN\n      NaN\n      0.021973\n      F\n      45-49\n      Some days\n    \n    \n      184555\n      Very good\n      No\n      No\n      No\n      No\n      No\n      0.027175\n      F\n      80+\n      Former\n    \n  \n\n\n\n\n\ngrader.check(\"q6\")\n\nFinally, all the variables in the dataset are categorical. Notice that the current data types do not reflect this.\n\nsamp_mod5.dtypes\n\nGENHLTH      object\nADDEPEV3     object\nACEDEPRS     object\nACEDRINK     object\nACEDRUGS     object\nACEPRISN     object\n_LLCPWT     float64\n_SEX         object\n_AGEG5YR     object\n_SMOKER3     object\ndtype: object\n\n\nLet’s coerce the variables to category data types using .astype().\n\n# coerce to categorical\nsamp_mod6 = samp_mod5.astype('category')\n\n# check new data types\nsamp_mod6.dtypes\n\nGENHLTH     category\nADDEPEV3    category\nACEDEPRS    category\nACEDRINK    category\nACEDRUGS    category\nACEPRISN    category\n_LLCPWT     category\n_SEX        category\n_AGEG5YR    category\n_SMOKER3    category\ndtype: object\n\n\n\n\nQuestion 7: Define ACE indicator variable\nDownstream analysis of ACEs will be facilitated by having an indicator variable that is a 1 if the respondent answered ‘Yes’ to any ACE question, and a 0 otherwise – that way, you can easily count the number of respondents reporting ACEs by summing up the indicator or compute the proportion by taking an average.\nTo this end, define a new logical variable:\n\nadverse_conditions: did the respondent answer yes to any of the adverse childhood condition questions?\n\nYou can accomplish this task in several steps:\n\nObtain a logical array indicating the positions of the ACE variables (hint: use .columns to obtain the column index and operate on the result with .str.startswith(...).). Store this as ace_positions.\nUse the logical array ace_positions to select the ACE columns via .loc[]. Store this as ace_data.\nObtain a dataframe that indicates whether each entry is a ‘Yes’ (hint: use the boolean operator ==, which is a vectorized operation). Store this as ace_yes.\nCompute the row sums using .sum(). Store this as ace_numyes.\nDefine the new variable as ace_numyes > 0.\n\nStore the result as samp_mod7, and print the first few rows using .head().\n\n# BEGIN SOLUTION NO PROMPT\n# copy samp_mod6\nsamp_mod7 = samp_mod6.copy()\n\n# ace column positions\nace_positions = samp_mod7.columns.str.startswith('ACE')\n\n# ace data\nace_data = samp_mod7.loc[:, ace_positions]\n\n# ace yes indicators\nace_yes = (ace_data == 'Yes')\n\n# number of yesses\nace_numyes = ace_yes.sum(axis = 1)\n\n# assign new variable\nsamp_mod7['adverse_conditions'] = (ace_numyes > 0)\n\n# check result\nsamp_mod7.head()\n# END SOLUTION\n\n\"\"\" # BEGIN PROMPT\n# copy samp_mod6\nsamp_mod8 = samp_mod6.copy()\n\n# ace column positions\nace_positions = ...\n\n# ace data\nace_data = ...\n\n# ace yes indicators\nace_yes = ...\n\n# number of yesses\nace_numyes = ...\n\n# assign new variable\nsamp_mod8['adverse_conditions'] = ...\n\n# check result using .head()\n...\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q7\")\n\n\n\nQuestion 8: Define missingness indicator variable\nAs you saw earlier, there are some missing values for the ACE questions. These arise whenever a respondent is not asked these questions. In fact, answers are missing for nearly 80% of the respondents in our subsample. We should keep track of this information. Define a missing indicator:\n\nadverse_missing: is a response missing for at least one of the ACE questions?\n\n\n# BEGIN SOLUTION NO PROMPT\n# copy modification 7\nsamp_mod8 = samp_mod7.copy()\n\n# define missing indicator using loc\nsamp_mod8.loc[:, 'adverse_missing'] = samp_mod8.loc[:, samp_mod8.columns.str.startswith('ACE').tolist()].isna().sum(axis = 1) > 0\n\n# check\nsamp_mod8.head()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# copy modification 7\nsamp_mod8 = samp_mod7.copy()\n\n# define missing indicator using loc\n...\n\n# check using head()\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q8\")\n\n\n\nQuestion 9: Filter respondents who did not answer ACE questions\nSince values are missing for the ACE question if a respondent was not asked, we can remove these observations and do any analysis conditional on respondents having been asked the ACE questions. Use your indicator variable adverse_missing to filter out respondents who were not asked the ACE questions.\nNote that this dramatically limits the scope of inference for subsequent analyses to only those locations where the ACE module was included in the survey.\n\nsamp_mod9 = samp_mod8[~samp_mod8.adverse_missing] #SOLUTION\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10: Define depression indicator variable\nIt will prove similarly helpful to define an indicator for reported depression:\n\ndepression: did the respondent report having been diagnosed with a depressive disorder?\n\nFollow the same strategy as above for the ACE variables, and store the result as samp_mod10. See if you can perform the calculation of the new variable in a single line of code. Print the first few rows using .head().\n\n# BEGIN SOLUTION NO PROMPT\n# copy samp_mod9\nsamp_mod10 = samp_mod9.copy()\n\n# define missing indicator using loc\nsamp_mod10['depression'] = ((samp_mod10.loc[:, 'ADDEPEV3'] == 'Yes') > 0)\n\n# check\nsamp_mod10.head()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# copy samp_mod9\nsamp_mod10 = samp_mod9.copy()\n\n# define new variable using loc\n...\n\n# check using .head()\n...\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q10\")\n\n\n\nQuestion 11: Final dataset\nFor the final dataset, drop the respondent answers to individual questions, the missingness indicator, and select just the derived indicator variables along with state, general health, sex, age, and smoking status. Check the pandas documentation for .rename() and follow the examples to rename the latter variables:\n\ngeneral_health\nsex\nage\nsmoking\n\nSee if you can perform both operations (slicing and renaming) in a single chain. Store the result as data.\n\nsamp_mod10.columns\n\nIndex(['GENHLTH', 'ADDEPEV3', 'ACEDEPRS', 'ACEDRINK', 'ACEDRUGS', 'ACEPRISN',\n       '_LLCPWT', '_SEX', '_AGEG5YR', '_SMOKER3', 'adverse_conditions',\n       'adverse_missing', 'depression'],\n      dtype='object')\n\n\n\n# BEGIN SOLUTION NO PROMPT\n# slice and rename\ndata = samp_mod10.iloc[:, [0, 7, 8, 9, 10, 12]].rename( #dropping some variables is the same as only selecting the remaining variables\n    columns = {'GENHLTH': 'general_health',\n               '_SEX': 'sex',\n               '_AGEG5YR': 'age',\n               '_SMOKER3': 'smoking'}\n)\n\n# preview\ndata.head()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# slice and rename\ndata = ...\n\n# check using .head()\n\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q11\")"
  },
  {
    "objectID": "hw/hw1-brfss/hw1-brfss-soln.html#communicating-results",
    "href": "hw/hw1-brfss/hw1-brfss-soln.html#communicating-results",
    "title": "PSTAT100",
    "section": "3. Communicating results",
    "text": "3. Communicating results\nHere you’ll be asked to reflect briefly on your findings.\n\n\nQuestion 17: Summary\nIs there an observed association between reporting ACEs and general health, smoking status, and depression among survey respondents who answered the ACE questions?\nWrite a two to three sentence answer to the above question summarizing your findings. State an answer to the question in your first sentence, and then in your second/third sentences describe exactly what you observed in the foregoing descriptive analysis of the BRFSS data. Be precise, but also concise. There is no need to describe any of the data manipulations, survey design, or the like.\nType your answer here, replacing this text.\nSOLUTION\nYes, there are observed associations between reported adverse childhood experiences and general health, smoking status, and depression. The proportion of respondents reporting ACEs generally increases with smoking frequency for both men and women; there are higher observed rates of ACE reports among respondents in poorer health for both men and women; and there are higher observed rates of ACE reports among respondents with a diagnosed depressive disorder.\n\n\n\n\nQuestion 18: Scope of inference\nRecall from the overview documentation all the care that the BRFSS dedicates to collecting a representative sample of the U.S. adult population with phone numbers. Do you think that your findings provide evidence of an association among the general public (not just the individuals survey)? Why or why not? Answer in two sentences.\nType your answer here, replacing this text.\nSOLUTION\nThe sample is a probability sample of the study population, so results are in principle generalizable; however, many ACE responses were missing because certain states did not ask those questions. As a result, the observed proportions are likely underestimates of the rates among the general public (U.S. adults with phone numbers in private or college housing) and may misrepresent the overall pattern of association. More narrowly, the findings do provide evidence of associations between adverse childhood experiences and health, depression, and smoking among a subset of states.\n\n\n\n\nQuestion 19: Bias\nWhat is a potential source of bias in the survey results, and how might this affect the proportions you’ve calculated?\nAnswer in one or two sentences.\nType your answer here, replacing this text.\nSOLUTION\nAdverse childhood experience is a sensitive matter; respondents may not be comfortable responding truthfully to some of these questions. This would likely produce negative bias – the sample proportions may be underestimates if this is common.\n\n\n\nComment\nNotice that the language ‘association’ is non-causual: we don’t say that ACEs cause (or don’t cause) poorer health outcomes. This is intentional, because the BRFSS data are what are known as ‘observational’ data, i.e. not originating from a controlled experiment. There could be unobserved factors that explain the association.\nTo take a simple example, dog owners live longer, but the reason is simply that dog owners walk more – so it’s the exercise, not the dogs, that cause an increase in longevity. An observational study that doesn’t measure exercise would show a positive association between dog ownership and lifespan, but it’s a non-causal relationship.\n(As an interesting/amusing aside, there is a well known study that established an association between birdkeeping and lung cancer; obviously this is non-causal, yet the study authors recommended that individuals at high risk for cancer avoid ‘avian exposure’, as they were unsure of the mechanism.)\nSo there could easily be unobserved factors that account for the observed association in the BRFSS data. We guard against over-interpreting the results by using causally-neutral language."
  },
  {
    "objectID": "hw/hw1-brfss/hw1-brfss.html",
    "href": "hw/hw1-brfss/hw1-brfss.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"hw1-brfss.ipynb\")"
  },
  {
    "objectID": "hw/hw1-brfss/hw1-brfss.html#recoding-categorical-variables",
    "href": "hw/hw1-brfss/hw1-brfss.html#recoding-categorical-variables",
    "title": "PSTAT100",
    "section": "Recoding categorical variables",
    "text": "Recoding categorical variables\nThe variable entries are coded numerically to represent certain responses. These should be replaced by more informative entries. We can use the codebook to determine which number means what, and replace the values accordingly.\nThe cell below replaces the numeric values for _AGEG5YR by their meanings, illustrating how to use .replace() with a dictionary to convert the numeric coding to interpretable values. The basic strategy is:\n\nStore the variable coding for VAR as a dictionary var_codes.\nUse .replace({'VAR': var_codes}) to modify values.\n\nIf you need additional examples, check the pandas documentation for .replace().\n\n# dictionary representing variable coding\nage_codes = {\n    1: '18-24', 2: '25-29', 3: '30-34',\n    4: '35-39', 5: '40-44', 6: '45-49',\n    7: '50-54', 8: '55-59', 9: '60-64',\n    10: '65-69', 11: '70-74', 12: '75-79',\n    13: '80+', 14: 'Unsure/refused/missing'\n}\n\n# recode age categories\nsamp_mod1 = samp.replace({'_AGEG5YR': age_codes})\n\n# check result\nsamp_mod1.head()\n\n\nQuestion 5: Recoding variables\nFollowing the example immediately above and referring to the 2019 BRFSS codebook, replace the numeric codings with response categories for each of the following variables:\n\n_SEX\nGENHLTH\n_SMOKER3\n\nNotice that above, the first modification (slicing) was stored as samp_mod1, and was a function of samp. You’ll follow this pattern, creating samp_mod2, samp_mod3, and so on so that each step (modification) of your data manipulations is stored separately, for easy troubleshooting.\n\nRecode _SEX: define a new dataframe samp_mod2 that is the same as samp_mod1 but with the _SEX variable recoded as M and F.\nRecode GENHLTH: define a new dataframe samp_mod3 that is the same as samp_mod2 but with the GENHLTH variable recoded as Excellent, Very good, Good, Fair, Poor, Unsure, and Refused.\nRecode _SMOKER3: define a new dataframe samp_mod4 that is the same as samp_mod3 but with _SMOKER3 recoded as Daily, Some days, Former, Never, and Unsure/refused/missing.\nPrint the first few rows of samp_mod4.\n\n\n# define dictionary for sex\nsex_codes = ...\n\n# recode sex\nsamp_mod2 = ...\n\n# define dictionary for health\nhealth_codes = ...\n\n# recode health\nsamp_mod3 = ...\n\n# define dictionary for smoking\nsmoke_codes = ...\n\n# recode smoking\nsamp_mod4 = ...\n\n# print a few rows\n...\n\n\ngrader.check(\"q5\")\n\n\n\nQuestion 6: Value replacement\nNow all the variables except the adverse childhood experience and depressive disorder question responses are represented interpretably. In the codebook that the answer key is identical for these remaining variables.\nThe numeric codings can be replaced all at once by applying .replace() to the dataframe with an argument of the form\n\ndf.replace({'var1': varcodes1, 'var2': varcodes1, ..., 'varp': varcodesp})\n\nDefine a new dataframe samp_mod5 that is the same as samp_mod4 but with the remaining variables recoded according to the answer key Yes, No, Unsure, Refused. Print the first few rows of the result using .head().\n\n# define dictionary\nanswer_codes = ...\n\n# recode\nsamp_mod5 = ...\n\n# check using head()\n...\n\n\ngrader.check(\"q6\")\n\nFinally, all the variables in the dataset are categorical. Notice that the current data types do not reflect this.\n\nsamp_mod5.dtypes\n\nLet’s coerce the variables to category data types using .astype().\n\n# coerce to categorical\nsamp_mod6 = samp_mod5.astype('category')\n\n# check new data types\nsamp_mod6.dtypes\n\n\n\nQuestion 7: Define ACE indicator variable\nDownstream analysis of ACEs will be facilitated by having an indicator variable that is a 1 if the respondent answered ‘Yes’ to any ACE question, and a 0 otherwise – that way, you can easily count the number of respondents reporting ACEs by summing up the indicator or compute the proportion by taking an average.\nTo this end, define a new logical variable:\n\nadverse_conditions: did the respondent answer yes to any of the adverse childhood condition questions?\n\nYou can accomplish this task in several steps:\n\nObtain a logical array indicating the positions of the ACE variables (hint: use .columns to obtain the column index and operate on the result with .str.startswith(...).). Store this as ace_positions.\nUse the logical array ace_positions to select the ACE columns via .loc[]. Store this as ace_data.\nObtain a dataframe that indicates whether each entry is a ‘Yes’ (hint: use the boolean operator ==, which is a vectorized operation). Store this as ace_yes.\nCompute the row sums using .sum(). Store this as ace_numyes.\nDefine the new variable as ace_numyes > 0.\n\nStore the result as samp_mod7, and print the first few rows using .head().\n\n\n# copy samp_mod6\nsamp_mod8 = samp_mod6.copy()\n\n# ace column positions\nace_positions = ...\n\n# ace data\nace_data = ...\n\n# ace yes indicators\nace_yes = ...\n\n# number of yesses\nace_numyes = ...\n\n# assign new variable\nsamp_mod8['adverse_conditions'] = ...\n\n# check result using .head()\n...\n\n\ngrader.check(\"q7\")\n\n\n\nQuestion 8: Define missingness indicator variable\nAs you saw earlier, there are some missing values for the ACE questions. These arise whenever a respondent is not asked these questions. In fact, answers are missing for nearly 80% of the respondents in our subsample. We should keep track of this information. Define a missing indicator:\n\nadverse_missing: is a response missing for at least one of the ACE questions?\n\n\n# copy modification 7\nsamp_mod8 = samp_mod7.copy()\n\n# define missing indicator using loc\n...\n\n# check using head()\n\n\ngrader.check(\"q8\")\n\n\n\nQuestion 9: Filter respondents who did not answer ACE questions\nSince values are missing for the ACE question if a respondent was not asked, we can remove these observations and do any analysis conditional on respondents having been asked the ACE questions. Use your indicator variable adverse_missing to filter out respondents who were not asked the ACE questions.\nNote that this dramatically limits the scope of inference for subsequent analyses to only those locations where the ACE module was included in the survey.\n\nsamp_mod9 = ...\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10: Define depression indicator variable\nIt will prove similarly helpful to define an indicator for reported depression:\n\ndepression: did the respondent report having been diagnosed with a depressive disorder?\n\nFollow the same strategy as above for the ACE variables, and store the result as samp_mod10. See if you can perform the calculation of the new variable in a single line of code. Print the first few rows using .head().\n\n# copy samp_mod9\nsamp_mod10 = samp_mod9.copy()\n\n# define new variable using loc\n...\n\n# check using .head()\n...\n\n\ngrader.check(\"q10\")\n\n\n\nQuestion 11: Final dataset\nFor the final dataset, drop the respondent answers to individual questions, the missingness indicator, and select just the derived indicator variables along with state, general health, sex, age, and smoking status. Check the pandas documentation for .rename() and follow the examples to rename the latter variables:\n\ngeneral_health\nsex\nage\nsmoking\n\nSee if you can perform both operations (slicing and renaming) in a single chain. Store the result as data.\n\nsamp_mod10.columns\n\n\n# slice and rename\ndata = ...\n\n# check using .head()\n\n\ngrader.check(\"q11\")"
  },
  {
    "objectID": "hw/hw1-brfss/hw1-brfss.html#communicating-results",
    "href": "hw/hw1-brfss/hw1-brfss.html#communicating-results",
    "title": "PSTAT100",
    "section": "3. Communicating results",
    "text": "3. Communicating results\nHere you’ll be asked to reflect briefly on your findings.\n\n\nQuestion 17: Summary\nIs there an observed association between reporting ACEs and general health, smoking status, and depression among survey respondents who answered the ACE questions?\nWrite a two to three sentence answer to the above question summarizing your findings. State an answer to the question in your first sentence, and then in your second/third sentences describe exactly what you observed in the foregoing descriptive analysis of the BRFSS data. Be precise, but also concise. There is no need to describe any of the data manipulations, survey design, or the like.\nType your answer here, replacing this text.\n\n\n\n\nQuestion 18: Scope of inference\nRecall from the overview documentation all the care that the BRFSS dedicates to collecting a representative sample of the U.S. adult population with phone numbers. Do you think that your findings provide evidence of an association among the general public (not just the individuals survey)? Why or why not? Answer in two sentences.\nType your answer here, replacing this text.\n\n\n\n\nQuestion 19: Bias\nWhat is a potential source of bias in the survey results, and how might this affect the proportions you’ve calculated?\nAnswer in one or two sentences.\nType your answer here, replacing this text.\n\n\n\nComment\nNotice that the language ‘association’ is non-causual: we don’t say that ACEs cause (or don’t cause) poorer health outcomes. This is intentional, because the BRFSS data are what are known as ‘observational’ data, i.e. not originating from a controlled experiment. There could be unobserved factors that explain the association.\nTo take a simple example, dog owners live longer, but the reason is simply that dog owners walk more – so it’s the exercise, not the dogs, that cause an increase in longevity. An observational study that doesn’t measure exercise would show a positive association between dog ownership and lifespan, but it’s a non-causal relationship.\n(As an interesting/amusing aside, there is a well known study that established an association between birdkeeping and lung cancer; obviously this is non-causal, yet the study authors recommended that individuals at high risk for cancer avoid ‘avian exposure’, as they were unsure of the mechanism.)\nSo there could easily be unobserved factors that account for the observed association in the BRFSS data. We guard against over-interpreting the results by using causally-neutral language."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Concepts and Analysis",
    "section": "",
    "text": "This is the course website for UCSB’s Data Science Concepts and Analysis class (PSTAT100). Content is directed towards currently enrolled students. Please ask permission before using course materials in any other capacity."
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab0-gettingstarted.ipynb\")"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#jupyter-notebooks",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#jupyter-notebooks",
    "title": "PSTAT100",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nJupyter notebooks are organized into ‘cells’ that can contain either text or codes. For example, this is a text cell.\nTechnically, Jupyter is an application/interface that runs atop a kernel – a programming-language-specific independent environment in which code cells are executed. This basic organization allows for interactive computing with text integration.\nSelecting a cell and pressing Enter will enter edit mode and allow you to edit the cell. From edit mode, pressing Esc will revert to command mode and allow you to navigate the notebook’s cells.\nIn edit mode, most of the keyboard is dedicated to typing into the cell’s editor. Thus, in edit mode there are relatively few shortcuts. In command mode, the entire keyboard is available for shortcuts, so there are many more. Here are a few useful ones:\n\nCtrl + Return : Evaluate the current cell\nShift + Return: Evaluate the current cell and move to the next\nSaving the notebook: s\nBasic navigation: up one cell k, down one cell j\na : create a cell above\nb : create a cell below\ndd : delete a cell\nz : undo the last cell operation\nm : convert a cell to markdown\ny : convert a cell to code\n\nTake a moment to find out what the following commands do:\n\nCell editing: x, c, v, z\nKernel operations: i, 0 (press twice)\n\n\n# Practice the above commands on this cell\n\n\nRunning Cells and Displaying Output\nRun the following cell.\n\nprint(\"Hello, World!\")\n\nHello, World!\n\n\nIn Jupyter notebooks, all print statements are displayed below the cell. Furthermore, the output of only the last line is displayed following the cell upon execution.\n\n\"Will this line be displayed?\"\n\nprint(\"Hello\" + \",\", \"world!\")\n\n5 + 3\n\nHello, world!\n\n\n8\n\n\n\n\nViewing Documentation\nTo output the documentation for a function, use the help() function.\n\nhelp(print)\n\nHelp on built-in function print in module builtins:\n\nprint(...)\n    print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\n    \n    Prints the values to a stream, or to sys.stdout by default.\n    Optional keyword arguments:\n    file:  a file-like object (stream); defaults to the current sys.stdout.\n    sep:   string inserted between values, default a space.\n    end:   string appended after the last value, default a newline.\n    flush: whether to forcibly flush the stream.\n\n\n\nYou can also use Jupyter to view function documentation inside your notebook. The function must already be defined in the kernel for this to work.\nBelow, click your mouse anywhere on print() and use Shift + Tab to view the function’s documentation.\n\nprint('Welcome to this course!')\n\nWelcome to this course!\n\n\n\n\nImporting Libraries\nIn this course, we will be using common Python libraries to help us retrieve, manipulate, and perform operations on data. By convention, we import all libraries at the very top of the notebook. There are also a set of standard aliases that are used to shorten the library names. Below are some of the libraries that you may encounter throughout the course, along with their respective aliases.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#practice-questions-and-numpy-review",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#practice-questions-and-numpy-review",
    "title": "PSTAT100",
    "section": "Practice questions and numpy review",
    "text": "Practice questions and numpy review\nMost assignments for this class will be given as notebooks organized into explanation and prompts followed by response cells; you will complete assignments by filling in all of the response cells.\nMany response cells are followed by a test cell that performs a few checks on your work. Please be aware that test cells don’t always confirm that your response is correct or incorrect. They are meant to give you some useful feedback, but it’s your responsibility to interpret the feedback – please be sure to read and think about test output if tests fail, and make your own assessment of whether you need to revise your response.\nBelow are a few practice questions for you to familiarize yourself with the process. These assume familiarity with basic python syntax and the numpy package.\n\nQuestion 1\nWrite a function summation that evaluates the following summation for \\(n \\geq 1\\):\n\\[\\sum_{i=1}^{n} \\left(i^3 + 5 i^3\\right)\\]\nHint: np.arange(5).sum() will generate an array comprising \\(1, 2, \\dots, 5\\) and then add up the elements of the array.\n\ndef summation(n):\n    \"\"\"Compute the summation i^3 + 5 * i^3 for 1 <= i <= n.\"\"\"\n    # BEGIN SOLUTION\n    out = (6*(np.arange(n + 1)**3)).sum()\n    return out\n    # END SOLUTION\n\n\ngrader.check(\"q1\")\n\nUse your function to compute the sum for…\n\n# n = 2\n...\n\n\n# n = 20\n...\n\n\n\nQuestion 2\nThe core of numpy is the array. Let’s use np.array to create an array. It takes a sequence, such as a list or range (remember that list elements are included between the square brackets [ and ], such as [1, 5, 3]).\nBelow, create an array containing the values 1, 2, 3, 4, and 5 (in that order) and assign it the name my_array.\n\nmy_array = np.array([1, 2, 3, 4, 5]) #SOLUTION\n\n\ngrader.check(\"q2\")\n\nNumpy arrays are integer-indexed by position, with the first element indexed as position 0. Elements can be retrieved by enclosing the desired positions in brackets [].\n\nmy_array[3]\n\n4\n\n\nTo retrieve consecutive positions, specify the starting index and the ending index separated by :, for instance, arr[from:to]. This syntax is non-inclusive of the left endpoint, meaning that the starting index is not included in the output.\n\nmy_array[2:4]\n\narray([3, 4])\n\n\nIn addition to values in the array, we can access attributes such as array’s shape and data type that can be retrieved by name using syntax of the form array.attr. Some useful attributes are:\n\n.shape, a tuple with the length of each array dimension\n.size, the length of the first array dimension\n.dtype, the data type of the entries (float, integer, etc.)\n\nA full list of attributes is here.\n\nmy_array.shape\n\n(5,)\n\n\n\nmy_array.size\n\n5\n\n\n\nmy_array.dtype\n\ndtype('int32')\n\n\nArrays, unlike Python lists, cannot store items of different data types.\n\n# A regular Python list can store items of different data types\n[1, '3']\n\n[1, '3']\n\n\n\n# Arrays will convert everything to the same data type\nnp.array([1, '3'])\n\narray(['1', '3'], dtype='<U11')\n\n\n\n# Another example of array type conversion\nnp.array([5, 8.3])\n\narray([5. , 8.3])\n\n\nArrays are also useful in performing vectorized operations. Given two or more arrays of equal length, arithmetic will perform element-wise computations across the arrays.\nFor example, observe the following:\n\n# Python list addition will concatenate the two lists\n[1, 2, 3] + [4, 5, 6]\n\n[1, 2, 3, 4, 5, 6]\n\n\n\n# NumPy array addition will add them element-wise\nnp.array([1, 2, 3]) + np.array([4, 5, 6])\n\narray([5, 7, 9])\n\n\nArrays can be subsetted by index position, as shown above, or by a logical vector of the same length. For example:\n\nexample_arr = np.arange(4, 10)\nexample_arr\n\narray([4, 5, 6, 7, 8, 9])\n\n\nSuppose we want the last three elements. One option is to use index position:\n\nexample_arr[3:6]\n\narray([7, 8, 9])\n\n\nOr a logical vector:\n\nexample_arr[np.array([False, False, False, True, True, True])]\n\narray([7, 8, 9])\n\n\nThe latter approach allows one to subset based on a condition defined by the values of the vector. For example, we can use the condition \\(x \\geq 7\\) to obtain the logical vector used above.\n\nexample_arr >= 7\n\narray([False, False, False,  True,  True,  True])\n\n\nAnd then we can subset just as before:\n\nexample_arr[example_arr >= 7]\n\narray([7, 8, 9])\n\n\nYou’ll see this done frequently, and it’s sometimes referred to as filtering, because we’re selectively removing values.\n\n\nQuestion 3\nGiven the array random_arr, create an array containing all values \\(x\\) such that \\(2x^4 > 1\\). Name the array valid_values.\n\n# for reproducibility - setting the seed will result in the same random draw each time\nnp.random.seed(42)\n\n# draw 60 uniformly random integers between 0 and 1\nrandom_arr = np.random.rand(60)\n\n# solution here\nvalid_values = random_arr[2*(random_arr**4) > 1] # SOLUTION\n\n\ngrader.check(\"q3\")\n\n\n\nA note on np.arange and np.linspace\nUsually we use np.arange to return an array that steps from a to b with a fixed step size s. While this is fine in some cases, we sometimes prefer to use np.linspace(a, b, N), which divides the interval [a, b] into N equally spaced points.\nnp.arange(start, stop, step) produces an array with all the numbers starting at start, incremendted up by step, stopping before stop is reached. For example, the value of np.arange(1, 6, 2) is an array with elements 1, 3, and 5 – it starts at 1 and counts up by 2, then stops before 6. np.arange(4, 9, 1) is an array with elements 4, 5, 6, 7, and 8. (It doesn’t contain 9 because np.arange stops before the stop value is reached.)\nnp.linspace always includes both end points while np.arange will not include the second end point b. For this reason, especially when we are plotting ranges of values we tend to prefer np.linspace.\nNotice how the following two statements have different parameters but return the same result.\n\nnp.arange(-5, 6, 1.0)\n\narray([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])\n\n\n\nnp.linspace(-5, 5, 11)\n\narray([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])\n\n\nCheck your understanding. Will np.arange(1, 10) produce an array that contains 10? Add a cell below and check to confirm your answer."
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab0-gettingstarted.ipynb\")"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html#jupyter-notebooks",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html#jupyter-notebooks",
    "title": "PSTAT100",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nJupyter notebooks are organized into ‘cells’ that can contain either text or codes. For example, this is a text cell.\nTechnically, Jupyter is an application/interface that runs atop a kernel – a programming-language-specific independent environment in which code cells are executed. This basic organization allows for interactive computing with text integration.\nSelecting a cell and pressing Enter will enter edit mode and allow you to edit the cell. From edit mode, pressing Esc will revert to command mode and allow you to navigate the notebook’s cells.\nIn edit mode, most of the keyboard is dedicated to typing into the cell’s editor. Thus, in edit mode there are relatively few shortcuts. In command mode, the entire keyboard is available for shortcuts, so there are many more. Here are a few useful ones:\n\nCtrl + Return : Evaluate the current cell\nShift + Return: Evaluate the current cell and move to the next\nSaving the notebook: s\nBasic navigation: up one cell k, down one cell j\na : create a cell above\nb : create a cell below\ndd : delete a cell\nz : undo the last cell operation\nm : convert a cell to markdown\ny : convert a cell to code\n\nTake a moment to find out what the following commands do:\n\nCell editing: x, c, v, z\nKernel operations: i, 0 (press twice)\n\n\n# Practice the above commands on this cell\n\n\nRunning Cells and Displaying Output\nRun the following cell.\n\nprint(\"Hello, World!\")\n\nIn Jupyter notebooks, all print statements are displayed below the cell. Furthermore, the output of only the last line is displayed following the cell upon execution.\n\n\"Will this line be displayed?\"\n\nprint(\"Hello\" + \",\", \"world!\")\n\n5 + 3\n\n\n\nViewing Documentation\nTo output the documentation for a function, use the help() function.\n\nhelp(print)\n\nYou can also use Jupyter to view function documentation inside your notebook. The function must already be defined in the kernel for this to work.\nBelow, click your mouse anywhere on print() and use Shift + Tab to view the function’s documentation.\n\nprint('Welcome to this course!')\n\n\n\nImporting Libraries\nIn this course, we will be using common Python libraries to help us retrieve, manipulate, and perform operations on data. By convention, we import all libraries at the very top of the notebook. There are also a set of standard aliases that are used to shorten the library names. Below are some of the libraries that you may encounter throughout the course, along with their respective aliases.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html#practice-questions-and-numpy-review",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html#practice-questions-and-numpy-review",
    "title": "PSTAT100",
    "section": "Practice questions and numpy review",
    "text": "Practice questions and numpy review\nMost assignments for this class will be given as notebooks organized into explanation and prompts followed by response cells; you will complete assignments by filling in all of the response cells.\nMany response cells are followed by a test cell that performs a few checks on your work. Please be aware that test cells don’t always confirm that your response is correct or incorrect. They are meant to give you some useful feedback, but it’s your responsibility to interpret the feedback – please be sure to read and think about test output if tests fail, and make your own assessment of whether you need to revise your response.\nBelow are a few practice questions for you to familiarize yourself with the process. These assume familiarity with basic python syntax and the numpy package.\n\nQuestion 1\nWrite a function summation that evaluates the following summation for \\(n \\geq 1\\):\n\\[\\sum_{i=1}^{n} \\left(i^3 + 5 i^3\\right)\\]\nHint: np.arange(5).sum() will generate an array comprising \\(1, 2, \\dots, 5\\) and then add up the elements of the array.\n\ndef summation(n):\n    \"\"\"Compute the summation i^3 + 5 * i^3 for 1 <= i <= n.\"\"\"\n    ...\n\n\ngrader.check(\"q1\")\n\nUse your function to compute the sum for…\n\n# n = 2\n...\n\n\n# n = 20\n...\n\n\n\nQuestion 2\nThe core of numpy is the array. Let’s use np.array to create an array. It takes a sequence, such as a list or range (remember that list elements are included between the square brackets [ and ], such as [1, 5, 3]).\nBelow, create an array containing the values 1, 2, 3, 4, and 5 (in that order) and assign it the name my_array.\n\nmy_array = ...\n\n\ngrader.check(\"q2\")\n\nNumpy arrays are integer-indexed by position, with the first element indexed as position 0. Elements can be retrieved by enclosing the desired positions in brackets [].\n\nmy_array[3]\n\nTo retrieve consecutive positions, specify the starting index and the ending index separated by :, for instance, arr[from:to]. This syntax is non-inclusive of the left endpoint, meaning that the starting index is not included in the output.\n\nmy_array[2:4]\n\nIn addition to values in the array, we can access attributes such as array’s shape and data type that can be retrieved by name using syntax of the form array.attr. Some useful attributes are:\n\n.shape, a tuple with the length of each array dimension\n.size, the length of the first array dimension\n.dtype, the data type of the entries (float, integer, etc.)\n\nA full list of attributes is here.\n\nmy_array.shape\n\n\nmy_array.size\n\n\nmy_array.dtype\n\nArrays, unlike Python lists, cannot store items of different data types.\n\n# A regular Python list can store items of different data types\n[1, '3']\n\n\n# Arrays will convert everything to the same data type\nnp.array([1, '3'])\n\n\n# Another example of array type conversion\nnp.array([5, 8.3])\n\nArrays are also useful in performing vectorized operations. Given two or more arrays of equal length, arithmetic will perform element-wise computations across the arrays.\nFor example, observe the following:\n\n# Python list addition will concatenate the two lists\n[1, 2, 3] + [4, 5, 6]\n\n\n# NumPy array addition will add them element-wise\nnp.array([1, 2, 3]) + np.array([4, 5, 6])\n\nArrays can be subsetted by index position, as shown above, or by a logical vector of the same length. For example:\n\nexample_arr = np.arange(4, 10)\nexample_arr\n\nSuppose we want the last three elements. One option is to use index position:\n\nexample_arr[3:6]\n\nOr a logical vector:\n\nexample_arr[np.array([False, False, False, True, True, True])]\n\nThe latter approach allows one to subset based on a condition defined by the values of the vector. For example, we can use the condition \\(x \\geq 7\\) to obtain the logical vector used above.\n\nexample_arr >= 7\n\nAnd then we can subset just as before:\n\nexample_arr[example_arr >= 7]\n\nYou’ll see this done frequently, and it’s sometimes referred to as filtering, because we’re selectively removing values.\n\n\nQuestion 3\nGiven the array random_arr, create an array containing all values \\(x\\) such that \\(2x^4 > 1\\). Name the array valid_values.\n\n# for reproducibility - setting the seed will result in the same random draw each time\nnp.random.seed(42)\n\n# draw 60 uniformly random integers between 0 and 1\nrandom_arr = np.random.rand(60)\n\n# solution here\nvalid_values = ...\n\n\ngrader.check(\"q3\")\n\n\n\nA note on np.arange and np.linspace\nUsually we use np.arange to return an array that steps from a to b with a fixed step size s. While this is fine in some cases, we sometimes prefer to use np.linspace(a, b, N), which divides the interval [a, b] into N equally spaced points.\nnp.arange(start, stop, step) produces an array with all the numbers starting at start, incremendted up by step, stopping before stop is reached. For example, the value of np.arange(1, 6, 2) is an array with elements 1, 3, and 5 – it starts at 1 and counts up by 2, then stops before 6. np.arange(4, 9, 1) is an array with elements 4, 5, 6, 7, and 8. (It doesn’t contain 9 because np.arange stops before the stop value is reached.)\nnp.linspace always includes both end points while np.arange will not include the second end point b. For this reason, especially when we are plotting ranges of values we tend to prefer np.linspace.\nNotice how the following two statements have different parameters but return the same result.\n\nnp.arange(-5, 6, 1.0)\n\n\nnp.linspace(-5, 5, 11)\n\nCheck your understanding. Will np.arange(1, 10) produce an array that contains 10? Add a cell below and check to confirm your answer."
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab1-pandas.ipynb\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#creating-dataframes-basic-manipulations",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#creating-dataframes-basic-manipulations",
    "title": "PSTAT100",
    "section": "Creating DataFrames & Basic Manipulations",
    "text": "Creating DataFrames & Basic Manipulations\nA dataframe is a table in which each column has a type; there is an index over the columns (typically string labels) and an index over the rows (typically ordinal numbers). An index is represented by a series object, which is a one-dimensional labeled array. Here you’ll cover:\n\ncreating dataframes from scratch;\nretrieving attributes;\ndataframe indexing;\nadding, removing, and renaming columns.\n\n\nCreating dataframes from scratch\nThe documentation for the pandas DataFrame class provide two primary syntaxes to create a data frame from scratch:\n\nfrom a dictionary\nrow-wise tuples\n\nSyntax 1 (dictionary): You can create a data frame by specifying the columns and values using a dictionary (a concatenation of named lists) as shown below.\nThe keys of the dictionary are the column names, and the values of the dictionary are lists containing the row entries.\n\n# define a dataframe using dictionary syntax\nfruit_info = pd.DataFrame( \n    data = { 'fruit': ['apple', 'orange', 'banana', 'raspberry'],\n             'color': ['red', 'orange', 'yellow', 'pink']\n           })\n\n# print\nfruit_info\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      0\n      apple\n      red\n    \n    \n      1\n      orange\n      orange\n    \n    \n      2\n      banana\n      yellow\n    \n    \n      3\n      raspberry\n      pink\n    \n  \n\n\n\n\nSyntax 2 (row tuples): You can also define a dataframe by specifying the rows as tuples.\nEach row corresponds to a distinct tuple, and the column indices are specified separately.\n\n# define the same dataframe using tuple syntax \nfruit_info2 = pd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"]\n)\n\n# print\nfruit_info2\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      0\n      apple\n      red\n    \n    \n      1\n      orange\n      orange\n    \n    \n      2\n      banana\n      yellow\n    \n    \n      3\n      raspberry\n      pink\n    \n  \n\n\n\n\n\n\nDataframe Attributes\nDataFrames have several basic attributes:\n\n.shape contains dimensions;\n.dtypes contains data types (float, integer, object, etc.)\n.size first (row) dimension;\n.values contains an array comprising each entry in the dataframe.\n.columns contains the column index;\n.index contains the row index.\n\nYou can obtain these attributes by appending the attribute name to the dataframe name. For instance, the dimensions of a dataframe df can be retrieved by df.shape.\n\n# dimensions\nfruit_info.shape\n\n(4, 2)\n\n\nTo retrieve a two-dimensional numpy array with the values of the dataframe, use df.values. It is sometimes useful to extract this data structure in order to perform vectorized operations, linear algebra, and the like.\n\n# as array\nfruit_info.values\n\narray([['apple', 'red'],\n       ['orange', 'orange'],\n       ['banana', 'yellow'],\n       ['raspberry', 'pink']], dtype=object)\n\n\n\n\nDataframe Indexing\nThe entries in a dataframe are indexed. Indices for rows and columns are stored as the .index. and .columns attributes, respectively.\n\nfruit_info.columns\n\nIndex(['fruit', 'color'], dtype='object')\n\n\n\nfruit_info.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\nBy default, the row indexing is simply numbering by consecutive integers.\n\nfruit_info.index.values\n\narray([0, 1, 2, 3], dtype=int64)\n\n\nHowever, rows can alternatively be indexed by labels:\n\n# define with a row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = [\"fruit 1\", \"fruit 2\", \"fruit 3\", 'fruit 4']\n)\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      fruit 1\n      apple\n      red\n    \n    \n      fruit 2\n      orange\n      orange\n    \n    \n      fruit 3\n      banana\n      yellow\n    \n    \n      fruit 4\n      raspberry\n      pink\n    \n  \n\n\n\n\nUnlike data frames in R, the row index label figures prominently in certain operations. The elements of the dataframe can be retrived using .loc[ROW-INDEX, COL-INDEX] which specifies the location of data values by name (not by position).\n\n# retrieve row 0, column 'fruit'\nfruit_info.loc[0, 'fruit']\n\n'apple'\n\n\nMost of the time rows are indexed numerically, and somewhat confusingly, the syntax for .loc does not require putting the row index 0 in quotes, even though it refers to the row label and not the row number. This is important to remember, because often operations will scramble the order of rows. To see the difference, consider the following:\n\n# non-consecutive row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n)\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      8\n      apple\n      red\n    \n    \n      6\n      orange\n      orange\n    \n    \n      4\n      banana\n      yellow\n    \n    \n      2\n      raspberry\n      pink\n    \n  \n\n\n\n\nNow adding .loc[2, 'color'] will retrieve pink, the last row of the dataframe.\n\n# subset\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n).loc[2, 'color']\n\n'pink'\n\n\nTo retrieve values by position, use .iloc. For many, this is more intuitive, as it is most similar to matrix or array indexing in mathematical notation.\n\n# retrieve 0, 0 entry\nfruit_info.iloc[0, 0]\n\n'apple'\n\n\n\n\nAdding, removing, and renaming columns\nThere are two ways to add new columns:\n\ndirect specification;\nusing .loc[].\n\nDirect specification: For a dataFrame df, you can add a column with df['new column name'] = ... and assign a list or array of values to the column.\nUsing .loc[]: For a dataframe df, you can add a column with df.loc[:, 'new column name'] = ... and assign a list or array of values to the column.\nBoth accomplish the same task – adding a new column index and populating values for each row – but .loc[] is a little faster.\n\n\nQuestion 1\nUsing direct specification, add to the fruit_info table a new column called rank1 containing integers 1, 2, 3, and 4, which express your personal preference about the taste ordering for each fruit (1 is tastiest; 4 is least tasty). Make sure that the numbers utilized are unique - no ties are allowed.\n\n\nfruit_info['rank1'] = [1, 3, 4, 2] # SOLUTION\n\n# print\nfruit_info\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n      rank1\n    \n  \n  \n    \n      0\n      apple\n      red\n      1\n    \n    \n      1\n      orange\n      orange\n      3\n    \n    \n      2\n      banana\n      yellow\n      4\n    \n    \n      3\n      raspberry\n      pink\n      2\n    \n  \n\n\n\n\n\ngrader.check(\"q1\")\n\nNow, create a new dataframe fruit_info_mod1 with the same information as fruit_info_original, but has the additional column rank2. Let’s start off with making fruit_info_mod1 as a copy of fruit_info:\n\nfruit_info_mod1 = fruit_info.copy()\n\n\n\nQuestion 2\nUsing .loc[], add a column called rank2 to the fruit_info_mod1 table that contains the same values in the same order as the rank1 column.\nHint: .loc will parse : as shorthand for ‘all indices’.\n\n\nfruit_info_mod1.loc[:, 'rank2']  = [1, 3, 4, 2] #SOLUTION\n\n# print\nfruit_info_mod1\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n      rank1\n      rank2\n    \n  \n  \n    \n      0\n      apple\n      red\n      1\n      1\n    \n    \n      1\n      orange\n      orange\n      3\n      3\n    \n    \n      2\n      banana\n      yellow\n      4\n      4\n    \n    \n      3\n      raspberry\n      pink\n      2\n      2\n    \n  \n\n\n\n\n\ngrader.check(\"q2\")\n\nWhen using the .loc[] approach, the : specifies that values are assigned to all rows of the data frame, so the array assigned to the new variable must be the same length as the data frame. What if we only assign values to certain rows? Try running the cell below.\n\n# define new variable just for rows 1 and 2\nfruit_info_mod1.loc[1:2, 'rank3'] = [1, 2]\n\n# check result\nfruit_info_mod1\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n      rank1\n      rank2\n      rank3\n    \n  \n  \n    \n      0\n      apple\n      red\n      1\n      1\n      NaN\n    \n    \n      1\n      orange\n      orange\n      3\n      3\n      1.0\n    \n    \n      2\n      banana\n      yellow\n      4\n      4\n      2.0\n    \n    \n      3\n      raspberry\n      pink\n      2\n      2\n      NaN\n    \n  \n\n\n\n\nThe remaining rows are assigned missing values. Notice what this does to the data type:\n\n# check data types\nfruit_info_mod1.dtypes\n\nfruit     object\ncolor     object\nrank1      int64\nrank2      int64\nrank3    float64\ndtype: object\n\n\nWe can detect these missing values using .isna():\n\n# returns a logical data frame indicating whether each entry is missing or not\nfruit_info_mod1.isna()\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n      rank1\n      rank2\n      rank3\n    \n  \n  \n    \n      0\n      False\n      False\n      False\n      False\n      True\n    \n    \n      1\n      False\n      False\n      False\n      False\n      False\n    \n    \n      2\n      False\n      False\n      False\n      False\n      False\n    \n    \n      3\n      False\n      False\n      False\n      False\n      True\n    \n  \n\n\n\n\nIt would be more helpful to simply see by column whether there are missing values. Appending a .any() to the above command will do the trick:\n\n# detects whether any column has missing entries\nfruit_info_mod1.isna().any()\n\nfruit    False\ncolor    False\nrank1    False\nrank2    False\nrank3     True\ndtype: bool\n\n\nNow that we’ve had a bit of fun let’s remove those rank variables. Columns can be removed using .drop() with a list of column names to drop as its argument. For example:\n\n# first syntax for .drop()\nfruit_info_mod1.drop(columns = 'color')\n\n\n\n\n\n  \n    \n      \n      fruit\n      rank1\n      rank2\n      rank3\n    \n  \n  \n    \n      0\n      apple\n      1\n      1\n      NaN\n    \n    \n      1\n      orange\n      3\n      3\n      1.0\n    \n    \n      2\n      banana\n      4\n      4\n      2.0\n    \n    \n      3\n      raspberry\n      2\n      2\n      NaN\n    \n  \n\n\n\n\nThere is an alternate syntax to that shown above, which involves specifying the axis (row vs. column) and index name to drop:\n\n# second syntax for .drop()\nfruit_info_mod1.drop('color', axis = 1)\n\n\n\n\n\n  \n    \n      \n      fruit\n      rank1\n      rank2\n      rank3\n    \n  \n  \n    \n      0\n      apple\n      1\n      1\n      NaN\n    \n    \n      1\n      orange\n      3\n      3\n      1.0\n    \n    \n      2\n      banana\n      4\n      4\n      2.0\n    \n    \n      3\n      raspberry\n      2\n      2\n      NaN\n    \n  \n\n\n\n\n\n\nQuestion 3\nUse the .drop() method to drop all rank columns you created in fruit_info_mod1. Note that drop does not change the table, but instead returns a new table with fewer columns or rows. To store the result, assign a new name (or write over the old dataframe). Here, assign the result to fruit_info_original.\nHint: Look through the documentation to see how you can drop multiple columns of a Pandas dataframe at once using a list of column names.\n\n\nfruit_info_original = fruit_info_mod1.drop(columns = ['rank1', 'rank2', 'rank3']) #SOLUTION\n\n# print\nfruit_info_original\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      0\n      apple\n      red\n    \n    \n      1\n      orange\n      orange\n    \n    \n      2\n      banana\n      yellow\n    \n    \n      3\n      raspberry\n      pink\n    \n  \n\n\n\n\n\ngrader.check(\"q3\")\n\nNifty trick: Use df.columns[df.columns.str.startswith('STRING')] to retrieve all indices starting with STRING and ix.values.tolist() to convert an index to an array of index names to obtain a list of column names to drop. Combining these gives df.columns[df.columns.str.startswith('STRING')].values.tolist(), and will return a list of all column names starting with STRING. This can be used in conjunction with the hint to remove all columns starting with rank.\n\n# try the nifty trick here\n\nNow create a new dataframe fruit_info_mod2with the same information as fruit_info_original, but has the column names capitalized. Begin by creating a copy fruit_info_mod2 of fruit_info_original:\n\nfruit_info_mod2 = fruit_info_original.copy()\n\n\n\nQuestion 4\nReview the documentation for .rename(). Based on the examples, rename the columns of fruit_info_mod2 so they begin with capital letters.\nFor many operations, you can change the dataframe ‘in place’ without reassigning the result of the operation to a new name by setting the inplace parameter to True. Use that strategy here.\n\n\nfruit_info_mod2.rename(columns = {'fruit': 'Fruit', 'color': 'Color'}, inplace = True) #SOLUTION\n\n# print\nfruit_info_mod2\n\n\n\n\n\n  \n    \n      \n      Fruit\n      Color\n    \n  \n  \n    \n      0\n      apple\n      red\n    \n    \n      1\n      orange\n      orange\n    \n    \n      2\n      banana\n      yellow\n    \n    \n      3\n      raspberry\n      pink\n    \n  \n\n\n\n\n\ngrader.check(\"q4\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#operations-on-data-frames",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#operations-on-data-frames",
    "title": "PSTAT100",
    "section": "Operations on Data Frames",
    "text": "Operations on Data Frames\nWith some basics in place, here you’ll see how to perform subsetting operations on data frames that are useful for tidying up datasets.\n\nSlicing: selecting columns or rows in chunks or by position.\n\nOften imported data contain columns that are either superfluous or not of interest for a particular project.\nYou may also want to examine particular portions of a data frame.\n\nFiltering: selecting rows that meet certain criteria\n\nOften you’ll want to remove duplicate rows, filter missing observations, or select a structured subset of a data frame.\nAlso helpful for inspection.\n\n\nTo illustrate these operations, you’ll use a dataset comprising counts of the given names of babies born in California each year from 1990 - 2018. The cell below imports the baby names data as a data frame from a .csv file. .head() prints the first few rows of the dataset.\n\n# import baby names data\nbaby_names = pd.read_csv('data/baby_names.csv')\n\n# preview first few rows\nbaby_names.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1990\n      Jessica\n      6635\n    \n    \n      1\n      CA\n      F\n      1990\n      Ashley\n      4537\n    \n    \n      2\n      CA\n      F\n      1990\n      Stephanie\n      4001\n    \n    \n      3\n      CA\n      F\n      1990\n      Amanda\n      3856\n    \n    \n      4\n      CA\n      F\n      1990\n      Jennifer\n      3611\n    \n  \n\n\n\n\nYour focus here isn’t on analyzing this data, so we won’t ask you to spend too much effort getting acquainted with it. However, a brief inspection is always a good idea. Let’s check:\n\ndimensions (number of rows and columns);\nhow many distinct states, sexes, and years.\n\nNote that the above dataframe displayed is a preview of the full dataframe.\n\nQuestion 5\nYou’ve already seen how to examine dimensions using dataframe attributes. Check the dimensions of baby_names and store them in dimensions_baby_names.\n\n\ndimensions_baby_names = baby_names.shape #SOLUTION\n\n\ngrader.check(\"q5\")\n\nYou haven’t yet seen how to retrieve the distinct values of an array or series, without duplicates. There are a few different ways to go about this, but one is to count the number of occurrences of each distinct entry in a column. This can be done by retrieving the column as a series using syntax of the form df.colname, and then pass the result to .value_counts():\n\n# count distinct values\nbaby_names.Sex.value_counts()\n\nF    112196\nM     78566\nName: Sex, dtype: int64\n\n\n\n\nQuestion 6\nCount the number of occurences of each distinct year. Create a series occur_per_year that displays the number of occurrences, ordered by year (so that the years are displayed in order). If you add sort = False as an argument to value_counts, the distinct values will be displayed in the order they appear in the dataset.\nHow many years are represented in the dataset? Store your answer as num_years.\n\n\noccur_per_year = baby_names.Year.value_counts(sort = False) #SOLUTION \n\nnum_years = len(occur_per_year) #SOLUTION\n\nprint(occur_per_year)\nprint(num_years)\n\n1990    6261\n1991    6226\n1992    6304\n1993    6314\n1994    6241\n1995    6092\n1996    6036\n1997    5961\n1998    5976\n1999    6052\n2000    6284\n2001    6333\n2002    6414\n2003    6533\n2004    6708\n2005    6874\n2006    7075\n2007    7250\n2008    7158\n2009    7119\n2010    7010\n2011    6880\n2012    7007\n2013    6861\n2014    6952\n2015    6871\n2016    6770\n2017    6684\n2018    6516\nName: Year, dtype: int64\n29\n\n\n\ngrader.check(\"q6\")\n\n\n\nSlicing: selecting rows and columns\nThere are two fast and simple ways to slice dataframes:\n\nusing .loc to specify rows and columns by index;\nusing .iloc to specify rows and columns by position.\n\nYou have seen simple examples of both of these above. Here we’ll show how to use these two commands to retrieve multiple rows and columns.\n\nSlicing with .loc: specifying index names\nThis method retrieves entries by specifying row and column indexes using syntax of the form df.loc[rows, cols]. The rows and columns can be single indices, a list of indices, or a set of adjacent indices using a colon :. Examples of these usages are shown below.\n\n# single indices -- small slice\nbaby_names.loc[2, 'Name']\n\n'Stephanie'\n\n\n\n# a list of indices -- larger slice\nbaby_names.loc[[2, 3], ['Name', 'Count']]\n\n\n\n\n\n  \n    \n      \n      Name\n      Count\n    \n  \n  \n    \n      2\n      Stephanie\n      4001\n    \n    \n      3\n      Amanda\n      3856\n    \n  \n\n\n\n\n\n# consecutive indices -- a chunk\nbaby_names.loc[2:10, 'Year':'Count']\n\n\n\n\n\n  \n    \n      \n      Year\n      Name\n      Count\n    \n  \n  \n    \n      2\n      1990\n      Stephanie\n      4001\n    \n    \n      3\n      1990\n      Amanda\n      3856\n    \n    \n      4\n      1990\n      Jennifer\n      3611\n    \n    \n      5\n      1990\n      Elizabeth\n      3170\n    \n    \n      6\n      1990\n      Sarah\n      2843\n    \n    \n      7\n      1990\n      Brittany\n      2737\n    \n    \n      8\n      1990\n      Samantha\n      2720\n    \n    \n      9\n      1990\n      Michelle\n      2453\n    \n    \n      10\n      1990\n      Melissa\n      2442\n    \n  \n\n\n\n\n\n\n\nSlicing with .iloc: specifying entry positions\nAn alternative to specifying the indices in order to slice a dataframe is to specify the entry positions using .iloc (‘integer location’). You have seen an example of this too. As with .loc, .iloc can be used to select multiple rows/columns using either lists of positions or a consecutive set with from:to syntax.\n\n# single position\nbaby_names.iloc[2, 3]\n\n'Stephanie'\n\n\n\n# a list of positions\nbaby_names.iloc[[2, 3], [3, 4]]\n\n\n\n\n\n  \n    \n      \n      Name\n      Count\n    \n  \n  \n    \n      2\n      Stephanie\n      4001\n    \n    \n      3\n      Amanda\n      3856\n    \n  \n\n\n\n\n\n# consecutive positions\nbaby_names.iloc[2:11, 2:5]\n\n\n\n\n\n  \n    \n      \n      Year\n      Name\n      Count\n    \n  \n  \n    \n      2\n      1990\n      Stephanie\n      4001\n    \n    \n      3\n      1990\n      Amanda\n      3856\n    \n    \n      4\n      1990\n      Jennifer\n      3611\n    \n    \n      5\n      1990\n      Elizabeth\n      3170\n    \n    \n      6\n      1990\n      Sarah\n      2843\n    \n    \n      7\n      1990\n      Brittany\n      2737\n    \n    \n      8\n      1990\n      Samantha\n      2720\n    \n    \n      9\n      1990\n      Michelle\n      2453\n    \n    \n      10\n      1990\n      Melissa\n      2442\n    \n  \n\n\n\n\nWhile these commands may look very similar to their .loc analogs, there are some subtle but important differences. The row selection looks nearly identical, but recall that .loc uses the index and .iloc uses the position; they look so similar because typically index and position coincide.\nHowever, sorting the baby_names dataframe helps to reveal how the position of a row is not necessarily equal to the index of a row. For example, the first row is not necessarily the row associated with index 1. This distinction is important in understanding the difference between .loc[] and .iloc[].\n\n# sort and display\nsorted_baby_names = baby_names.sort_values(by=['Name'])\nsorted_baby_names.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      160797\n      CA\n      M\n      2008\n      Aadan\n      7\n    \n    \n      178791\n      CA\n      M\n      2014\n      Aadan\n      5\n    \n    \n      163914\n      CA\n      M\n      2009\n      Aadan\n      6\n    \n    \n      171112\n      CA\n      M\n      2012\n      Aaden\n      38\n    \n    \n      179928\n      CA\n      M\n      2015\n      Aaden\n      34\n    \n  \n\n\n\n\nHere is an example of how we would get the 2nd, 3rd, and 4th rows with only the Name column of the baby_names dataframe using both iloc[] and loc[]. Observe the difference, especially after sorting baby_names by name.\n\n# example iloc usage\nsorted_baby_names.iloc[1:4, 3]\n\n178791    Aadan\n163914    Aadan\n171112    Aaden\nName: Name, dtype: object\n\n\nNotice that using loc[] with 1:4 gives different results, since it selects using the index. The index gets moved around when you perform an operation like sort on the dataframe.\n\n# same syntax, different result\nsorted_baby_names.loc[1:4, \"Name\"]\n\n1           Ashley\n22219       Ashley\n138598      Ashley\n151978      Ashley\n120624      Ashley\n            ...   \n74380       Jennie\n19395       Jennie\n23061       Jennie\n91825       Jennie\n4         Jennifer\nName: Name, Length: 68640, dtype: object\n\n\nAbove, the .loc method retrieves all indexes between index 1 and index 4 in the order they appear in the sorted dataset. If instead we want to retrieve the same rows returned by the .iloc command, we need to specify the row indices explicitly as a list:\n\n# retrieve the same rows as iloc using loc\nsorted_baby_names.loc[[178791, 163914, 171112], 'Name']\n\n178791    Aadan\n163914    Aadan\n171112    Aaden\nName: Name, dtype: object\n\n\nSometimes it’s useful for slicing (and other operations) to set one of the columns to be a row index, effectively treating one column as a collection of row labels. This can be accomplished using set_index.\n\n# change the (row) index from 0,1,2,... to the name column\nbaby_names_nameindexed = baby_names.set_index(\"Name\") \nbaby_names_nameindexed.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Count\n    \n    \n      Name\n      \n      \n      \n      \n    \n  \n  \n    \n      Jessica\n      CA\n      F\n      1990\n      6635\n    \n    \n      Ashley\n      CA\n      F\n      1990\n      4537\n    \n    \n      Stephanie\n      CA\n      F\n      1990\n      4001\n    \n    \n      Amanda\n      CA\n      F\n      1990\n      3856\n    \n    \n      Jennifer\n      CA\n      F\n      1990\n      3611\n    \n  \n\n\n\n\nWe can now slice by name directly:\n\n# slice rows for ashley and jennifer\nbaby_names_nameindexed.loc[['Ashley', 'Jennifer'], :]\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Count\n    \n    \n      Name\n      \n      \n      \n      \n    \n  \n  \n    \n      Ashley\n      CA\n      F\n      1990\n      4537\n    \n    \n      Ashley\n      CA\n      F\n      1991\n      4233\n    \n    \n      Ashley\n      CA\n      F\n      1992\n      3966\n    \n    \n      Ashley\n      CA\n      F\n      1993\n      3591\n    \n    \n      Ashley\n      CA\n      F\n      1994\n      3202\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Jennifer\n      CA\n      M\n      1998\n      10\n    \n    \n      Jennifer\n      CA\n      M\n      1999\n      12\n    \n    \n      Jennifer\n      CA\n      M\n      2000\n      10\n    \n    \n      Jennifer\n      CA\n      M\n      2001\n      8\n    \n    \n      Jennifer\n      CA\n      M\n      2002\n      7\n    \n  \n\n88 rows Ã— 4 columns\n\n\n\n\n\nQuestion 7\nLook up the name of a friend! Store the name as friend_name. Use the name-indexed data frame to slice rows for the name of your choice and the Count, Sex, and Year columns in that order, and store the data frame as friend_slice.\n\n\n# if your friend's name is not in the database, use another name\n\nfriend_name = \"Trevor\" # SOLUTION\nfriend_slice = baby_names_nameindexed.loc[\"Trevor\", ['Count', 'Sex', 'Year']] #SOLUTION\n\n#print\nfriend_slice\n\n\n\n\n\n  \n    \n      \n      Count\n      Sex\n      Year\n    \n    \n      Name\n      \n      \n      \n    \n  \n  \n    \n      Trevor\n      5\n      F\n      1990\n    \n    \n      Trevor\n      823\n      M\n      1990\n    \n    \n      Trevor\n      836\n      M\n      1991\n    \n    \n      Trevor\n      897\n      M\n      1992\n    \n    \n      Trevor\n      737\n      M\n      1993\n    \n    \n      Trevor\n      675\n      M\n      1994\n    \n    \n      Trevor\n      682\n      M\n      1995\n    \n    \n      Trevor\n      609\n      M\n      1996\n    \n    \n      Trevor\n      590\n      M\n      1997\n    \n    \n      Trevor\n      647\n      M\n      1998\n    \n    \n      Trevor\n      673\n      M\n      1999\n    \n    \n      Trevor\n      545\n      M\n      2000\n    \n    \n      Trevor\n      535\n      M\n      2001\n    \n    \n      Trevor\n      488\n      M\n      2002\n    \n    \n      Trevor\n      425\n      M\n      2003\n    \n    \n      Trevor\n      369\n      M\n      2004\n    \n    \n      Trevor\n      372\n      M\n      2005\n    \n    \n      Trevor\n      335\n      M\n      2006\n    \n    \n      Trevor\n      302\n      M\n      2007\n    \n    \n      Trevor\n      281\n      M\n      2008\n    \n    \n      Trevor\n      252\n      M\n      2009\n    \n    \n      Trevor\n      219\n      M\n      2010\n    \n    \n      Trevor\n      194\n      M\n      2011\n    \n    \n      Trevor\n      161\n      M\n      2012\n    \n    \n      Trevor\n      142\n      M\n      2013\n    \n    \n      Trevor\n      126\n      M\n      2014\n    \n    \n      Trevor\n      114\n      M\n      2015\n    \n    \n      Trevor\n      103\n      M\n      2016\n    \n    \n      Trevor\n      90\n      M\n      2017\n    \n    \n      Trevor\n      78\n      M\n      2018\n    \n  \n\n\n\n\n\ngrader.check(\"q7\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#filtering",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#filtering",
    "title": "PSTAT100",
    "section": "Filtering",
    "text": "Filtering\nFiltering is sifting out rows according to a criterion, and can be accomplished using an array or series of Trues and Falses defined by a comparison. To take a simple example, say you wanted to filter out all names with fewer than 1000 occurrences. First you could define a logical series:\n\n# true if filtering criterion is met, false otherwise\narr = baby_names.Count > 1000\n\nThen you can filter using that array:\n\n# filter\nbaby_names_filtered = baby_names[arr]\nbaby_names_filtered.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1990\n      Jessica\n      6635\n    \n    \n      1\n      CA\n      F\n      1990\n      Ashley\n      4537\n    \n    \n      2\n      CA\n      F\n      1990\n      Stephanie\n      4001\n    \n    \n      3\n      CA\n      F\n      1990\n      Amanda\n      3856\n    \n    \n      4\n      CA\n      F\n      1990\n      Jennifer\n      3611\n    \n  \n\n\n\n\nNotice that the filtered array is much smaller than the overall array – only about 2000 rows correspond to a name occurring more than 1000 times in a year for a gender.\n\n# compare dimensions\nprint(baby_names_filtered.shape)\nprint(baby_names.shape)\n\n(2517, 5)\n(190762, 5)\n\n\nYou have already encountered this concept in lab 0 when subsetting an array. For your reference, some commonly used comparison operators are given below.\n\n\n\nSymbol\nUsage\nMeaning\n\n\n\n\n==\na == b\nDoes a equal b?\n\n\n<=\na <= b\nIs a less than or equal to b?\n\n\n>=\na >= b\nIs a greater than or equal to b?\n\n\n<\na < b\nIs a less than b?\n\n\n>\na > b\nIs a greater than b?\n\n\n~\n~p\nReturns negation of p\n\n\n|\np | q\np OR q\n\n\n&\np & q\np AND q\n\n\n^\np ^ q\np XOR q (exclusive or)\n\n\n\nWhat if instead you wanted to filter using multiple conditions? Here’s an example of retrieving rows with counts exceeding 1000 for only the year 2001:\n\n# filter using two conditions\nbaby_names[(baby_names.Year == 2000) & (baby_names.Count > 1000)]\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      36416\n      CA\n      F\n      2000\n      Emily\n      2958\n    \n    \n      36417\n      CA\n      F\n      2000\n      Ashley\n      2831\n    \n    \n      36418\n      CA\n      F\n      2000\n      Samantha\n      2579\n    \n    \n      36419\n      CA\n      F\n      2000\n      Jessica\n      2484\n    \n    \n      36420\n      CA\n      F\n      2000\n      Jennifer\n      2263\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      137298\n      CA\n      M\n      2000\n      Oscar\n      1089\n    \n    \n      137299\n      CA\n      M\n      2000\n      Thomas\n      1061\n    \n    \n      137300\n      CA\n      M\n      2000\n      Cameron\n      1052\n    \n    \n      137301\n      CA\n      M\n      2000\n      Austin\n      1010\n    \n    \n      137302\n      CA\n      M\n      2000\n      Richard\n      1001\n    \n  \n\n98 rows Ã— 5 columns\n\n\n\n\nQuestion 8\nSelect the girl names in 2010 that were given more than 3000 times, and store them as common_girl_names_2010.\nNote: Any time you use p & q to filter the dataframe, make sure to use df[df[(p) & (q)]] or df.loc[df[(p) & (q)]]). That is, make sure to wrap conditions with parentheses to ensure the intended order of operations.\n\n\ncommon_girl_names_2010 = baby_names[(baby_names.Sex == 'F') & (baby_names.Year == 2010) & (baby_names.Count > 3000)] #SOLUTION\n\ncommon_girl_names_2010\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      76793\n      CA\n      F\n      2010\n      Isabella\n      3368\n    \n    \n      76794\n      CA\n      F\n      2010\n      Sophia\n      3361\n    \n  \n\n\n\n\n\ngrader.check(\"q8\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#grouping-and-aggregation",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#grouping-and-aggregation",
    "title": "PSTAT100",
    "section": "Grouping and aggregation",
    "text": "Grouping and aggregation\nGrouping and aggregation are useful in generating data summaries, which are often important starting points in exploring a dataset.\n\nAggregation\nAggregation literally means ‘putting together’ (etymologically the word means ‘joining the herd’) – in statistics and data science, this refers to data summaries like an average, a minimum, or a measure of spread such as the sample variance or mean absolute deviation (data herding!). From a technical point of view, operations that take multiple values as inputs and return a single output are considered summaries – in other words, statistics. Some of the most common aggregations are:\n\nsum\nproduct\ncount\nnumber of distinct values\nmean\nmedian\nvariance\nstandard deviation\nminimum/maximum\nquantiles\n\nPandas has built-in dataframe operations that compute most of these summaries across either axis (column-wise or row-wise):\n\n.sum()\n.prod()\n.mean()\n.median()\n.var()\n.std()\n.nunique()\n.min() and .max()\n.quantile()\n\nTo illustrate these operations, let’s filter out all names in 1995.\n\n# filter 1995 names\nnames_95 = baby_names[baby_names.Year == 1995]\n\nHow many individuals were counted in total in 1995? We can address that by computing a sum of the counts:\n\n# n for 1995\nnames_95.Count.sum()\n\n494580\n\n\nWhat is the typical frequency of all names in 1995? We can address that by computing the average count:\n\n# average count for a name in 1995\nnames_95.Count.mean()\n\n81.18516086671043\n\n\n\n\nQuestion 9\nFind how often the most common name 1995 was given and store this as names_95_max_count. Use this value to filter names_95 and find which name was most common that year. Store the filtered dataframe as names_95_most_common_name.\n\n\nnames_95_max_count = names_95.Count.max() #SOLUTION\nnames_95_most_common_name = (names_95.loc[names_95.Count == names_95.Count.max(),  'Name']) #SOLUTION\n\n\nprint(\"Number of people with the most frequent name in 1995 is :\", names_95_max_count, \"people\")\nprint(\"Most frequent name in 1995 is:\", names_95_most_common_name.values[0])\n\nNumber of people with the most frequent name in 1995 is : 5003 people\nMost frequent name in 1995 is: Daniel\n\n\n\ngrader.check(\"q9\")\n\nCaution! If applied to the entire dataframe, the operation df.max() (or any other aggregation) will return the maximum of each column. Notice that the cell below does not return the row you found just now, but could easily be misinterpreted as such. The cell does tell you that the maximum value of sex (alphabetically last) is M and the maximum name (alphabetically last) is Zyanya and the maximum count is 5003; it does not tell you that 5003 boys were named Zyanya in 1995.\n\n# maximum of each variable\nnames_95.max()\n\nState        CA\nSex           M\nYear       1995\nName     Zyanya\nCount      5003\ndtype: object\n\n\n\n\nGrouping\nWhat if you want to know the most frequent male and female names? If so, you’ll need to repeat the above operations group-wise by sex.\nIn general, any variable in a dataframe can be used to define a grouping structure on the rows (or, less commonly, columns). After grouping, any dataframe operations will be executed within each group, but not across groups. This can be used to generate grouped summaries, such as the maximum count for boys and girls; as a point of terminology, we’d describe this summary as ‘maximum count by sex’.\nThe .groupby() function defines such a structure; here is the documentation. The cell below groups the names_95 dataframe by sex. Notice that when the grouped dataframe is previewed with .head(), the first few rows are returned for each group.\n\n# grouped dataframe\nnames_95_bysex = names_95.groupby('Sex')\n\n# print\nnames_95_bysex.head(2)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      18604\n      CA\n      F\n      1995\n      Jessica\n      4620\n    \n    \n      18605\n      CA\n      F\n      1995\n      Ashley\n      2903\n    \n    \n      124938\n      CA\n      M\n      1995\n      Daniel\n      5003\n    \n    \n      124939\n      CA\n      M\n      1995\n      Michael\n      4783\n    \n  \n\n\n\n\nAny aggregation operations applied to the grouped dataframe will be applied separately to the rows where Sex == M and the rows where Sex == F. For example, computing .sum() on the grouped dataframe will show the total number of individuals in the data for 1995 by sex:\n\n# number of individuals by sex\nnames_95_bysex.Count.sum()\n\nSex\nF    234552\nM    260028\nName: Count, dtype: int64\n\n\nThe most frequent boy and girl names can be found using .idxmax() groupwise to obtain the index of the first occurence of the maximum count for each sex, and then slicing with .loc:\n\n# first most common names by sex\nnames_95.loc[names_95_bysex.Count.idxmax(), :]\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      18604\n      CA\n      F\n      1995\n      Jessica\n      4620\n    \n    \n      124938\n      CA\n      M\n      1995\n      Daniel\n      5003\n    \n  \n\n\n\n\nSince .idxmax() gives the index of the first occurrence, these are the alphabetically first most common names; there could be ties. You know from your work so far that there are no ties for the male names; another filtering step can be used to check for ties among the female names.\n\n# ties?\nnames_95[names_95_bysex.Count.max().values[0] == names_95['Count']]\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      18604\n      CA\n      F\n      1995\n      Jessica\n      4620\n    \n  \n\n\n\n\nSo, no ties.\n\n\nQuestion 10\nAre there more girl names or boy names in 1995? Use the grouped dataframe names_95_bysex with the .count() aggregation to find the total number of names for each sex. Store the female and male counts as girl_name_count and boy_name_count, respectfully.\n\n\ngirl_name_count = names_95_bysex.Count.count()['F'] #SOLUTION\nboy_name_count = names_95_bysex.Count.count()['M'] #SOLUTION\n\n#print\nprint(girl_name_count)\nprint(boy_name_count)\n\n3614\n2478\n\n\n\ngrader.check(\"q10\")\n\n\n\nChaining operations\nYou have already seen examples of this, but pandas and numpy operations can be chained together in sequence. For example, names_95.Count.max() is a chain with two steps: first select the Count column (.count); then compute the maximum (.max()).\nGrouped summaries are often convenient to compute in a chained fashion, rather than by assigning the grouped dataframe a new name and performing operations on the resulting object. For example, finding the total number of boys and girls recorded in the 1995 data can be done with the following chain:\n\n# repeating previous calculation, more streamlined\nnames_95.groupby('Sex').Count.sum()\n\nSex\nF    234552\nM    260028\nName: Count, dtype: int64\n\n\nWe can take this even one step further and also perform the filtering in sequence as part of the chain:\n\n# longer chain\nbaby_names[baby_names.Year == 1995].groupby('Sex').Count.sum()\n\nSex\nF    234552\nM    260028\nName: Count, dtype: int64\n\n\nChains can get somewhat long, but they have the advantage of making codes more efficient, and often more readable. We did above in one step what took several lines before. Further, this chain can almost be read aloud:\n“Take baby names, filter on year, then group by sex, then select name counts, then compute the sum.”\nLet’s now consider computing the average counts of boy and girl names for each year 1990-1995. This can be accomplished by the following chain (notice it is possible to group by multiple variables).\n\n# average counts by sex and year\nbaby_names[baby_names.Year <= 1995].groupby(['Year', 'Sex']).mean(numeric_only = True)\n\n\n\n\n\n  \n    \n      \n      \n      Count\n    \n    \n      Year\n      Sex\n      \n    \n  \n  \n    \n      1990\n      F\n      70.085760\n    \n    \n      M\n      115.231930\n    \n    \n      1991\n      F\n      70.380888\n    \n    \n      M\n      114.608124\n    \n    \n      1992\n      F\n      68.744510\n    \n    \n      M\n      110.601556\n    \n    \n      1993\n      F\n      66.330675\n    \n    \n      M\n      107.896552\n    \n    \n      1994\n      F\n      66.426301\n    \n    \n      M\n      102.967966\n    \n    \n      1995\n      F\n      64.900941\n    \n    \n      M\n      104.934625\n    \n  \n\n\n\n\nThis display is not ideal. We can ‘pivot’ the table into a wide format by adding a few extra steps in the chain: change the indices to columns; then define a new shape by specifying which column should be the new row index, which should be the new column index, and which values should populate the table.\n\n# average counts by sex and year\nbaby_names[baby_names.Year <= 1995].groupby(\n    ['Year', 'Sex']\n    ).mean(\n    numeric_only = True\n    ).reset_index().pivot(\n    index = 'Sex', columns = 'Year', values = 'Count'\n    )\n\n\n\n\n\n  \n    \n      Year\n      1990\n      1991\n      1992\n      1993\n      1994\n      1995\n    \n    \n      Sex\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      F\n      70.08576\n      70.380888\n      68.744510\n      66.330675\n      66.426301\n      64.900941\n    \n    \n      M\n      115.23193\n      114.608124\n      110.601556\n      107.896552\n      102.967966\n      104.934625\n    \n  \n\n\n\n\nStyle comment: break long chains over multiple lines with indentation. The above chain is too long to be readable in one line. To balance the readability of codes with the efficiency of chaining, it is good practice to break long chains over several lines, with appropriate indentations.\nHere are some rules of thumb on style.\n\nSeparate comparisons by spaces (a<b as a < b)\nSplit chains longer than 30-40 characters over multiple lines\nSplit lines between delimiters (, )\nIncrease indentation for lines between delimiters\nFor chained operations, try to get each step in the chain shown on a separate line\nFor functions with multiple arguments, split lines so that each argument is on its own line\n\n\n\nQuestion 11\nWrite a chain with appropriate style to display the (first) most common boy and girl names in each of the years 2005-2015. Do this in two steps:\n\nFirst filter baby_names by year, then group by year and sex, and then find the indices of first occurence of the largest counts. Store these indices as ind.\nThen use .loc[] with your stored indices to slice baby_names so as to retrieve the rows corresponding to each most frequent name each year and for each sex; then pivot this table so that the columns are years, the rows are sexes, and the entries are names. Store this as pivot_names.\n\n\n\n# BEGIN SOLUTION\nind =  baby_names[(baby_names.Year <= 2015) & (baby_names.Year >= 2005)].groupby(\n    ['Sex', 'Year']\n).Count.idxmax(\n).values\n\npivot_names = baby_names.loc[ind, :].pivot(\n    index = 'Sex', \n    columns = 'Year', \n    values = 'Name'\n)\n\n# END SOLUTION\nprint(ind)\npivot_names\n\n[ 55767  59866  64073  68355  72602  76793  80890  84883  88981  92944\n  96958 150164 152939 155807 158775 161686 164614 167527 170414 173323\n 176221 179159]\n\n\n\n\n\n\n  \n    \n      Year\n      2005\n      2006\n      2007\n      2008\n      2009\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n    \n    \n      Sex\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      F\n      Emily\n      Emily\n      Emily\n      Isabella\n      Isabella\n      Isabella\n      Sophia\n      Sophia\n      Sophia\n      Sophia\n      Sophia\n    \n    \n      M\n      Daniel\n      Daniel\n      Daniel\n      Daniel\n      Daniel\n      Jacob\n      Jacob\n      Jacob\n      Jacob\n      Noah\n      Noah\n    \n  \n\n\n\n\n\ngrader.check(\"q11\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html",
    "href": "labs/lab1-pandas/lab1-pandas.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab1-pandas.ipynb\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#creating-dataframes-basic-manipulations",
    "href": "labs/lab1-pandas/lab1-pandas.html#creating-dataframes-basic-manipulations",
    "title": "PSTAT100",
    "section": "Creating DataFrames & Basic Manipulations",
    "text": "Creating DataFrames & Basic Manipulations\nA dataframe is a table in which each column has a type; there is an index over the columns (typically string labels) and an index over the rows (typically ordinal numbers). An index is represented by a series object, which is a one-dimensional labeled array. Here you’ll cover:\n\ncreating dataframes from scratch;\nretrieving attributes;\ndataframe indexing;\nadding, removing, and renaming columns.\n\n\nCreating dataframes from scratch\nThe documentation for the pandas DataFrame class provide two primary syntaxes to create a data frame from scratch:\n\nfrom a dictionary\nrow-wise tuples\n\nSyntax 1 (dictionary): You can create a data frame by specifying the columns and values using a dictionary (a concatenation of named lists) as shown below.\nThe keys of the dictionary are the column names, and the values of the dictionary are lists containing the row entries.\n\n# define a dataframe using dictionary syntax\nfruit_info = pd.DataFrame( \n    data = { 'fruit': ['apple', 'orange', 'banana', 'raspberry'],\n             'color': ['red', 'orange', 'yellow', 'pink']\n           })\n\n# print\nfruit_info\n\nSyntax 2 (row tuples): You can also define a dataframe by specifying the rows as tuples.\nEach row corresponds to a distinct tuple, and the column indices are specified separately.\n\n# define the same dataframe using tuple syntax \nfruit_info2 = pd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"]\n)\n\n# print\nfruit_info2\n\n\n\nDataframe Attributes\nDataFrames have several basic attributes:\n\n.shape contains dimensions;\n.dtypes contains data types (float, integer, object, etc.)\n.size first (row) dimension;\n.values contains an array comprising each entry in the dataframe.\n.columns contains the column index;\n.index contains the row index.\n\nYou can obtain these attributes by appending the attribute name to the dataframe name. For instance, the dimensions of a dataframe df can be retrieved by df.shape.\n\n# dimensions\nfruit_info.shape\n\nTo retrieve a two-dimensional numpy array with the values of the dataframe, use df.values. It is sometimes useful to extract this data structure in order to perform vectorized operations, linear algebra, and the like.\n\n# as array\nfruit_info.values\n\n\n\nDataframe Indexing\nThe entries in a dataframe are indexed. Indices for rows and columns are stored as the .index. and .columns attributes, respectively.\n\nfruit_info.columns\n\n\nfruit_info.index\n\nBy default, the row indexing is simply numbering by consecutive integers.\n\nfruit_info.index.values\n\nHowever, rows can alternatively be indexed by labels:\n\n# define with a row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = [\"fruit 1\", \"fruit 2\", \"fruit 3\", 'fruit 4']\n)\n\nUnlike data frames in R, the row index label figures prominently in certain operations. The elements of the dataframe can be retrived using .loc[ROW-INDEX, COL-INDEX] which specifies the location of data values by name (not by position).\n\n# retrieve row 0, column 'fruit'\nfruit_info.loc[0, 'fruit']\n\nMost of the time rows are indexed numerically, and somewhat confusingly, the syntax for .loc does not require putting the row index 0 in quotes, even though it refers to the row label and not the row number. This is important to remember, because often operations will scramble the order of rows. To see the difference, consider the following:\n\n# non-consecutive row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n)\n\nNow adding .loc[2, 'color'] will retrieve pink, the last row of the dataframe.\n\n# subset\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n).loc[2, 'color']\n\nTo retrieve values by position, use .iloc. For many, this is more intuitive, as it is most similar to matrix or array indexing in mathematical notation.\n\n# retrieve 0, 0 entry\nfruit_info.iloc[0, 0]\n\n\n\nAdding, removing, and renaming columns\nThere are two ways to add new columns:\n\ndirect specification;\nusing .loc[].\n\nDirect specification: For a dataFrame df, you can add a column with df['new column name'] = ... and assign a list or array of values to the column.\nUsing .loc[]: For a dataframe df, you can add a column with df.loc[:, 'new column name'] = ... and assign a list or array of values to the column.\nBoth accomplish the same task – adding a new column index and populating values for each row – but .loc[] is a little faster.\n\n\nQuestion 1\nUsing direct specification, add to the fruit_info table a new column called rank1 containing integers 1, 2, 3, and 4, which express your personal preference about the taste ordering for each fruit (1 is tastiest; 4 is least tasty). Make sure that the numbers utilized are unique - no ties are allowed.\n\n\n...\n\n# print\nfruit_info\n\n\ngrader.check(\"q1\")\n\nNow, create a new dataframe fruit_info_mod1 with the same information as fruit_info_original, but has the additional column rank2. Let’s start off with making fruit_info_mod1 as a copy of fruit_info:\n\nfruit_info_mod1 = fruit_info.copy()\n\n\n\nQuestion 2\nUsing .loc[], add a column called rank2 to the fruit_info_mod1 table that contains the same values in the same order as the rank1 column.\nHint: .loc will parse : as shorthand for ‘all indices’.\n\n\n...\n\n# print\nfruit_info_mod1\n\n\ngrader.check(\"q2\")\n\nWhen using the .loc[] approach, the : specifies that values are assigned to all rows of the data frame, so the array assigned to the new variable must be the same length as the data frame. What if we only assign values to certain rows? Try running the cell below.\n\n# define new variable just for rows 1 and 2\nfruit_info_mod1.loc[1:2, 'rank3'] = [1, 2]\n\n# check result\nfruit_info_mod1\n\nThe remaining rows are assigned missing values. Notice what this does to the data type:\n\n# check data types\nfruit_info_mod1.dtypes\n\nWe can detect these missing values using .isna():\n\n# returns a logical data frame indicating whether each entry is missing or not\nfruit_info_mod1.isna()\n\nIt would be more helpful to simply see by column whether there are missing values. Appending a .any() to the above command will do the trick:\n\n# detects whether any column has missing entries\nfruit_info_mod1.isna().any()\n\nNow that we’ve had a bit of fun let’s remove those rank variables. Columns can be removed using .drop() with a list of column names to drop as its argument. For example:\n\n# first syntax for .drop()\nfruit_info_mod1.drop(columns = 'color')\n\nThere is an alternate syntax to that shown above, which involves specifying the axis (row vs. column) and index name to drop:\n\n# second syntax for .drop()\nfruit_info_mod1.drop('color', axis = 1)\n\n\n\nQuestion 3\nUse the .drop() method to drop all rank columns you created in fruit_info_mod1. Note that drop does not change the table, but instead returns a new table with fewer columns or rows. To store the result, assign a new name (or write over the old dataframe). Here, assign the result to fruit_info_original.\nHint: Look through the documentation to see how you can drop multiple columns of a Pandas dataframe at once using a list of column names.\n\n\nfruit_info_original = ...\n\n# print\nfruit_info_original\n\n\ngrader.check(\"q3\")\n\nNifty trick: Use df.columns[df.columns.str.startswith('STRING')] to retrieve all indices starting with STRING and ix.values.tolist() to convert an index to an array of index names to obtain a list of column names to drop. Combining these gives df.columns[df.columns.str.startswith('STRING')].values.tolist(), and will return a list of all column names starting with STRING. This can be used in conjunction with the hint to remove all columns starting with rank.\n\n# try the nifty trick here\n\nNow create a new dataframe fruit_info_mod2with the same information as fruit_info_original, but has the column names capitalized. Begin by creating a copy fruit_info_mod2 of fruit_info_original:\n\nfruit_info_mod2 = fruit_info_original.copy()\n\n\n\nQuestion 4\nReview the documentation for .rename(). Based on the examples, rename the columns of fruit_info_mod2 so they begin with capital letters.\nFor many operations, you can change the dataframe ‘in place’ without reassigning the result of the operation to a new name by setting the inplace parameter to True. Use that strategy here.\n\n\n...\n\n# print\nfruit_info_mod2\n\n\ngrader.check(\"q4\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#operations-on-data-frames",
    "href": "labs/lab1-pandas/lab1-pandas.html#operations-on-data-frames",
    "title": "PSTAT100",
    "section": "Operations on Data Frames",
    "text": "Operations on Data Frames\nWith some basics in place, here you’ll see how to perform subsetting operations on data frames that are useful for tidying up datasets.\n\nSlicing: selecting columns or rows in chunks or by position.\n\nOften imported data contain columns that are either superfluous or not of interest for a particular project.\nYou may also want to examine particular portions of a data frame.\n\nFiltering: selecting rows that meet certain criteria\n\nOften you’ll want to remove duplicate rows, filter missing observations, or select a structured subset of a data frame.\nAlso helpful for inspection.\n\n\nTo illustrate these operations, you’ll use a dataset comprising counts of the given names of babies born in California each year from 1990 - 2018. The cell below imports the baby names data as a data frame from a .csv file. .head() prints the first few rows of the dataset.\n\n# import baby names data\nbaby_names = pd.read_csv('data/baby_names.csv')\n\n# preview first few rows\nbaby_names.head()\n\nYour focus here isn’t on analyzing this data, so we won’t ask you to spend too much effort getting acquainted with it. However, a brief inspection is always a good idea. Let’s check:\n\ndimensions (number of rows and columns);\nhow many distinct states, sexes, and years.\n\nNote that the above dataframe displayed is a preview of the full dataframe.\n\nQuestion 5\nYou’ve already seen how to examine dimensions using dataframe attributes. Check the dimensions of baby_names and store them in dimensions_baby_names.\n\n\ndimensions_baby_names = ...\n\n\ngrader.check(\"q5\")\n\nYou haven’t yet seen how to retrieve the distinct values of an array or series, without duplicates. There are a few different ways to go about this, but one is to count the number of occurrences of each distinct entry in a column. This can be done by retrieving the column as a series using syntax of the form df.colname, and then pass the result to .value_counts():\n\n# count distinct values\nbaby_names.Sex.value_counts()\n\n\n\nQuestion 6\nCount the number of occurences of each distinct year. Create a series occur_per_year that displays the number of occurrences, ordered by year (so that the years are displayed in order). If you add sort = False as an argument to value_counts, the distinct values will be displayed in the order they appear in the dataset.\nHow many years are represented in the dataset? Store your answer as num_years.\n\n\noccur_per_year = ...\n\nnum_years = ...\n\nprint(occur_per_year)\nprint(num_years)\n\n\ngrader.check(\"q6\")\n\n\n\nSlicing: selecting rows and columns\nThere are two fast and simple ways to slice dataframes:\n\nusing .loc to specify rows and columns by index;\nusing .iloc to specify rows and columns by position.\n\nYou have seen simple examples of both of these above. Here we’ll show how to use these two commands to retrieve multiple rows and columns.\n\nSlicing with .loc: specifying index names\nThis method retrieves entries by specifying row and column indexes using syntax of the form df.loc[rows, cols]. The rows and columns can be single indices, a list of indices, or a set of adjacent indices using a colon :. Examples of these usages are shown below.\n\n# single indices -- small slice\nbaby_names.loc[2, 'Name']\n\n\n# a list of indices -- larger slice\nbaby_names.loc[[2, 3], ['Name', 'Count']]\n\n\n# consecutive indices -- a chunk\nbaby_names.loc[2:10, 'Year':'Count']\n\n\n\n\nSlicing with .iloc: specifying entry positions\nAn alternative to specifying the indices in order to slice a dataframe is to specify the entry positions using .iloc (‘integer location’). You have seen an example of this too. As with .loc, .iloc can be used to select multiple rows/columns using either lists of positions or a consecutive set with from:to syntax.\n\n# single position\nbaby_names.iloc[2, 3]\n\n\n# a list of positions\nbaby_names.iloc[[2, 3], [3, 4]]\n\n\n# consecutive positions\nbaby_names.iloc[2:11, 2:5]\n\nWhile these commands may look very similar to their .loc analogs, there are some subtle but important differences. The row selection looks nearly identical, but recall that .loc uses the index and .iloc uses the position; they look so similar because typically index and position coincide.\nHowever, sorting the baby_names dataframe helps to reveal how the position of a row is not necessarily equal to the index of a row. For example, the first row is not necessarily the row associated with index 1. This distinction is important in understanding the difference between .loc[] and .iloc[].\n\n# sort and display\nsorted_baby_names = baby_names.sort_values(by=['Name'])\nsorted_baby_names.head()\n\nHere is an example of how we would get the 2nd, 3rd, and 4th rows with only the Name column of the baby_names dataframe using both iloc[] and loc[]. Observe the difference, especially after sorting baby_names by name.\n\n# example iloc usage\nsorted_baby_names.iloc[1:4, 3]\n\nNotice that using loc[] with 1:4 gives different results, since it selects using the index. The index gets moved around when you perform an operation like sort on the dataframe.\n\n# same syntax, different result\nsorted_baby_names.loc[1:4, \"Name\"]\n\nAbove, the .loc method retrieves all indexes between index 1 and index 4 in the order they appear in the sorted dataset. If instead we want to retrieve the same rows returned by the .iloc command, we need to specify the row indices explicitly as a list:\n\n# retrieve the same rows as iloc using loc\nsorted_baby_names.loc[[178791, 163914, 171112], 'Name']\n\nSometimes it’s useful for slicing (and other operations) to set one of the columns to be a row index, effectively treating one column as a collection of row labels. This can be accomplished using set_index.\n\n# change the (row) index from 0,1,2,... to the name column\nbaby_names_nameindexed = baby_names.set_index(\"Name\") \nbaby_names_nameindexed.head()\n\nWe can now slice by name directly:\n\n# slice rows for ashley and jennifer\nbaby_names_nameindexed.loc[['Ashley', 'Jennifer'], :]\n\n\n\nQuestion 7\nLook up the name of a friend! Store the name as friend_name. Use the name-indexed data frame to slice rows for the name of your choice and the Count, Sex, and Year columns in that order, and store the data frame as friend_slice.\n\n\n# if your friend's name is not in the database, use another name\n\nfriend_name = ...\nfriend_slice = ...\n\n#print\nfriend_slice\n\n\ngrader.check(\"q7\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#filtering",
    "href": "labs/lab1-pandas/lab1-pandas.html#filtering",
    "title": "PSTAT100",
    "section": "Filtering",
    "text": "Filtering\nFiltering is sifting out rows according to a criterion, and can be accomplished using an array or series of Trues and Falses defined by a comparison. To take a simple example, say you wanted to filter out all names with fewer than 1000 occurrences. First you could define a logical series:\n\n# true if filtering criterion is met, false otherwise\narr = baby_names.Count > 1000\n\nThen you can filter using that array:\n\n# filter\nbaby_names_filtered = baby_names[arr]\nbaby_names_filtered.head()\n\nNotice that the filtered array is much smaller than the overall array – only about 2000 rows correspond to a name occurring more than 1000 times in a year for a gender.\n\n# compare dimensions\nprint(baby_names_filtered.shape)\nprint(baby_names.shape)\n\nYou have already encountered this concept in lab 0 when subsetting an array. For your reference, some commonly used comparison operators are given below.\n\n\n\nSymbol\nUsage\nMeaning\n\n\n\n\n==\na == b\nDoes a equal b?\n\n\n<=\na <= b\nIs a less than or equal to b?\n\n\n>=\na >= b\nIs a greater than or equal to b?\n\n\n<\na < b\nIs a less than b?\n\n\n>\na > b\nIs a greater than b?\n\n\n~\n~p\nReturns negation of p\n\n\n|\np | q\np OR q\n\n\n&\np & q\np AND q\n\n\n^\np ^ q\np XOR q (exclusive or)\n\n\n\nWhat if instead you wanted to filter using multiple conditions? Here’s an example of retrieving rows with counts exceeding 1000 for only the year 2001:\n\n# filter using two conditions\nbaby_names[(baby_names.Year == 2000) & (baby_names.Count > 1000)]\n\n\nQuestion 8\nSelect the girl names in 2010 that were given more than 3000 times, and store them as common_girl_names_2010.\nNote: Any time you use p & q to filter the dataframe, make sure to use df[df[(p) & (q)]] or df.loc[df[(p) & (q)]]). That is, make sure to wrap conditions with parentheses to ensure the intended order of operations.\n\n\ncommon_girl_names_2010 = ...\n\ncommon_girl_names_2010\n\n\ngrader.check(\"q8\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#grouping-and-aggregation",
    "href": "labs/lab1-pandas/lab1-pandas.html#grouping-and-aggregation",
    "title": "PSTAT100",
    "section": "Grouping and aggregation",
    "text": "Grouping and aggregation\nGrouping and aggregation are useful in generating data summaries, which are often important starting points in exploring a dataset.\n\nAggregation\nAggregation literally means ‘putting together’ (etymologically the word means ‘joining the herd’) – in statistics and data science, this refers to data summaries like an average, a minimum, or a measure of spread such as the sample variance or mean absolute deviation (data herding!). From a technical point of view, operations that take multiple values as inputs and return a single output are considered summaries – in other words, statistics. Some of the most common aggregations are:\n\nsum\nproduct\ncount\nnumber of distinct values\nmean\nmedian\nvariance\nstandard deviation\nminimum/maximum\nquantiles\n\nPandas has built-in dataframe operations that compute most of these summaries across either axis (column-wise or row-wise):\n\n.sum()\n.prod()\n.mean()\n.median()\n.var()\n.std()\n.nunique()\n.min() and .max()\n.quantile()\n\nTo illustrate these operations, let’s filter out all names in 1995.\n\n# filter 1995 names\nnames_95 = baby_names[baby_names.Year == 1995]\n\nHow many individuals were counted in total in 1995? We can address that by computing a sum of the counts:\n\n# n for 1995\nnames_95.Count.sum()\n\nWhat is the typical frequency of all names in 1995? We can address that by computing the average count:\n\n# average count for a name in 1995\nnames_95.Count.mean()\n\n\n\nQuestion 9\nFind how often the most common name 1995 was given and store this as names_95_max_count. Use this value to filter names_95 and find which name was most common that year. Store the filtered dataframe as names_95_most_common_name.\n\n\nnames_95_max_count = ...\nnames_95_most_common_name = ...\n\n\nprint(\"Number of people with the most frequent name in 1995 is :\", names_95_max_count, \"people\")\nprint(\"Most frequent name in 1995 is:\", names_95_most_common_name.values[0])\n\n\ngrader.check(\"q9\")\n\nCaution! If applied to the entire dataframe, the operation df.max() (or any other aggregation) will return the maximum of each column. Notice that the cell below does not return the row you found just now, but could easily be misinterpreted as such. The cell does tell you that the maximum value of sex (alphabetically last) is M and the maximum name (alphabetically last) is Zyanya and the maximum count is 5003; it does not tell you that 5003 boys were named Zyanya in 1995.\n\n# maximum of each variable\nnames_95.max()\n\n\n\nGrouping\nWhat if you want to know the most frequent male and female names? If so, you’ll need to repeat the above operations group-wise by sex.\nIn general, any variable in a dataframe can be used to define a grouping structure on the rows (or, less commonly, columns). After grouping, any dataframe operations will be executed within each group, but not across groups. This can be used to generate grouped summaries, such as the maximum count for boys and girls; as a point of terminology, we’d describe this summary as ‘maximum count by sex’.\nThe .groupby() function defines such a structure; here is the documentation. The cell below groups the names_95 dataframe by sex. Notice that when the grouped dataframe is previewed with .head(), the first few rows are returned for each group.\n\n# grouped dataframe\nnames_95_bysex = names_95.groupby('Sex')\n\n# print\nnames_95_bysex.head(2)\n\nAny aggregation operations applied to the grouped dataframe will be applied separately to the rows where Sex == M and the rows where Sex == F. For example, computing .sum() on the grouped dataframe will show the total number of individuals in the data for 1995 by sex:\n\n# number of individuals by sex\nnames_95_bysex.Count.sum()\n\nThe most frequent boy and girl names can be found using .idxmax() groupwise to obtain the index of the first occurence of the maximum count for each sex, and then slicing with .loc:\n\n# first most common names by sex\nnames_95.loc[names_95_bysex.Count.idxmax(), :]\n\nSince .idxmax() gives the index of the first occurrence, these are the alphabetically first most common names; there could be ties. You know from your work so far that there are no ties for the male names; another filtering step can be used to check for ties among the female names.\n\n# ties?\nnames_95[names_95_bysex.Count.max().values[0] == names_95['Count']]\n\nSo, no ties.\n\n\nQuestion 10\nAre there more girl names or boy names in 1995? Use the grouped dataframe names_95_bysex with the .count() aggregation to find the total number of names for each sex. Store the female and male counts as girl_name_count and boy_name_count, respectfully.\n\n\ngirl_name_count = ...\nboy_name_count = ...\n\n#print\nprint(girl_name_count)\nprint(boy_name_count)\n\n\ngrader.check(\"q10\")\n\n\n\nChaining operations\nYou have already seen examples of this, but pandas and numpy operations can be chained together in sequence. For example, names_95.Count.max() is a chain with two steps: first select the Count column (.count); then compute the maximum (.max()).\nGrouped summaries are often convenient to compute in a chained fashion, rather than by assigning the grouped dataframe a new name and performing operations on the resulting object. For example, finding the total number of boys and girls recorded in the 1995 data can be done with the following chain:\n\n# repeating previous calculation, more streamlined\nnames_95.groupby('Sex').Count.sum()\n\nWe can take this even one step further and also perform the filtering in sequence as part of the chain:\n\n# longer chain\nbaby_names[baby_names.Year == 1995].groupby('Sex').Count.sum()\n\nChains can get somewhat long, but they have the advantage of making codes more efficient, and often more readable. We did above in one step what took several lines before. Further, this chain can almost be read aloud:\n“Take baby names, filter on year, then group by sex, then select name counts, then compute the sum.”\nLet’s now consider computing the average counts of boy and girl names for each year 1990-1995. This can be accomplished by the following chain (notice it is possible to group by multiple variables).\n\n# average counts by sex and year\nbaby_names[baby_names.Year <= 1995].groupby(['Year', 'Sex']).mean(numeric_only = True)\n\nThis display is not ideal. We can ‘pivot’ the table into a wide format by adding a few extra steps in the chain: change the indices to columns; then define a new shape by specifying which column should be the new row index, which should be the new column index, and which values should populate the table.\n\n# average counts by sex and year\nbaby_names[baby_names.Year <= 1995].groupby(\n    ['Year', 'Sex']\n    ).mean(\n    numeric_only = True\n    ).reset_index().pivot(\n    index = 'Sex', columns = 'Year', values = 'Count'\n    )\n\nStyle comment: break long chains over multiple lines with indentation. The above chain is too long to be readable in one line. To balance the readability of codes with the efficiency of chaining, it is good practice to break long chains over several lines, with appropriate indentations.\nHere are some rules of thumb on style.\n\nSeparate comparisons by spaces (a<b as a < b)\nSplit chains longer than 30-40 characters over multiple lines\nSplit lines between delimiters (, )\nIncrease indentation for lines between delimiters\nFor chained operations, try to get each step in the chain shown on a separate line\nFor functions with multiple arguments, split lines so that each argument is on its own line\n\n\n\nQuestion 11\nWrite a chain with appropriate style to display the (first) most common boy and girl names in each of the years 2005-2015. Do this in two steps:\n\nFirst filter baby_names by year, then group by year and sex, and then find the indices of first occurence of the largest counts. Store these indices as ind.\nThen use .loc[] with your stored indices to slice baby_names so as to retrieve the rows corresponding to each most frequent name each year and for each sex; then pivot this table so that the columns are years, the rows are sexes, and the entries are names. Store this as pivot_names.\n\n\n\n...\nprint(ind)\npivot_names\n\n\ngrader.check(\"q11\")"
  },
  {
    "objectID": "miscellany.html",
    "href": "miscellany.html",
    "title": "Miscellany",
    "section": "",
    "text": "Automated tests in assignment notebooks are a guide, not a confirmation or refutation of your answer. Don’t rely too heavily on them, but do read the output message if they fail and think about what the message is telling you. On some occasions they will fail despite a correct answer; on others they will pass despite an incorrect answer. Furthermore, they will guide you to one particular strategy for obtaining the solution; most problems admit a few possible strategies.\nAll assignments are due on Mondays. You get two free late assignments. Late submissions are due Wednesdays.\nStart your homeworks and mini-projects early.\nTake your own notes during class; don’t simply rely on lecture slides."
  },
  {
    "objectID": "miscellany.html#troubleshooting",
    "href": "miscellany.html#troubleshooting",
    "title": "Miscellany",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nIf you need to recover the distribution copy of any assignment notebook, perhaps due to accidentally deleting cells or similar issues, simply rename the notebook containing your work on the LSIT server and then redeploy the notebook from the course website link.\nIf you try to open a notebook and the server fails at the ‘synchronizing git repository’ stage, open the LSIT server separately, rename the pstat100-content directory, and then try opening the notebook from the website link again. If successfull, you will need to migrate all of your previous work into the new pstat100-content directory."
  },
  {
    "objectID": "slides/week1-intro.html#attendance-form",
    "href": "slides/week1-intro.html#attendance-form",
    "title": "Course introduction",
    "section": "Attendance form",
    "text": "Attendance form"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-1-ace-and-health",
    "href": "slides/week1-intro.html#case-study-1-ace-and-health",
    "title": "Course introduction",
    "section": "Case study 1: ACE and health",
    "text": "Case study 1: ACE and health\n\nAssociation between adverse childhood experiences and general health, by sex."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-1-ace-and-health-1",
    "href": "slides/week1-intro.html#case-study-1-ace-and-health-1",
    "title": "Course introduction",
    "section": "Case study 1: ACE and health",
    "text": "Case study 1: ACE and health\nYou will:\n\nprocess and recode 10K survey responses from CDC’s 2019 behavior risk factor surveillance survey (BRFSS)\ncross-tabulate health-related measurements with frequency of adverse childhood experiences"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-2-seda",
    "href": "slides/week1-intro.html#case-study-2-seda",
    "title": "Course introduction",
    "section": "Case study 2: SEDA",
    "text": "Case study 2: SEDA\n\nEducation achievement gaps as functions of socioeconomic indicators, by gender."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-2-seda-1",
    "href": "slides/week1-intro.html#case-study-2-seda-1",
    "title": "Course introduction",
    "section": "Case study 2: SEDA",
    "text": "Case study 2: SEDA\nYou will:\n\nmerge test scores and socioeconomic indicators from the 2018 Standford Education Data Archive by school district\nvisually assess correlations between gender achievement gaps among grade schoolers and socioeconomic indicators across school districts in CA"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\n\nSea surface temperature reconstruction over the past 16,000 years."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology-1",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology-1",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\n\nClustering of diatom relative abundances in pleistocene (pre-11KyBP) vs. holocene (post-11KyBP) epochs."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology-2",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology-2",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\nYou will:\n\nexplore ecological community structure from relative abundances of diatoms measured in ocean sediment core samples spanning ~15,000 years\nuse dimension reduction techniques to obtain measures of community structure\nidentify shifts associated with the transition from pleistocene to holocene epochs"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nApparent disparity in allocation of DDS benefits across racial groups."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-1",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-1",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nExpenditure is strongly associated with age."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-2",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-2",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nCorrecting for age shows comparable expenditure across racial groups."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-3",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-3",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\nYou will:\n\nassess the case for discrimination in allocation of DDS benefits\nidentify confounding factors present in the sample\nmodel median expenditure by racial group after correcting for age"
  },
  {
    "objectID": "slides/week1-intro.html#scope",
    "href": "slides/week1-intro.html#scope",
    "title": "Course introduction",
    "section": "Scope",
    "text": "Scope\nThis course is about developing your data science toolkit with foundational skills:\n\nCore competency with Python data science libraries\nCritical thinking about data\nVisualization and exploratory analysis\nApplication of basic statistical concepts and methods in practice\nCommunication and interpretation of results"
  },
  {
    "objectID": "slides/week1-intro.html#whats-unique-about-pstat100",
    "href": "slides/week1-intro.html#whats-unique-about-pstat100",
    "title": "Course introduction",
    "section": "What’s unique about PSTAT100?",
    "text": "What’s unique about PSTAT100?\nThere are a few distinctive aspects:\n\nmultiple end-to-end case studies\nquestion-driven rather than method-driven\nemphasis on project workflow\ndata storytelling and communication"
  },
  {
    "objectID": "slides/week1-intro.html#limitations",
    "href": "slides/week1-intro.html#limitations",
    "title": "Course introduction",
    "section": "Limitations",
    "text": "Limitations\nThere are also some things we won’t cover:\n\nPredictive modeling or machine learning\nAlgorithm design and implementation\nTechniques and methods for big data\nTheoretical basis for methods"
  },
  {
    "objectID": "slides/week1-intro.html#weekly-pattern",
    "href": "slides/week1-intro.html#weekly-pattern",
    "title": "Course introduction",
    "section": "Weekly Pattern",
    "text": "Weekly Pattern\nWe’ll follow a simple weekly pattern:\n\nMondays\n\nLecture\nSections\nAssignments due 11:59pm PST\n\nWednesdays\n\nLecture\nLate work due 11:59pm PST"
  },
  {
    "objectID": "slides/week1-intro.html#course-pages-materials",
    "href": "slides/week1-intro.html#course-pages-materials",
    "title": "Course introduction",
    "section": "Course pages & materials",
    "text": "Course pages & materials\n\nMaterials via course website ruizt.github.io/pstat100\nComputing at pstat100.lsit.ucsb.edu\nAssignments/gradebook at Gradescope\nDiscussion board (TBA)"
  },
  {
    "objectID": "slides/week1-intro.html#tentative-schedule",
    "href": "slides/week1-intro.html#tentative-schedule",
    "title": "Course introduction",
    "section": "Tentative schedule",
    "text": "Tentative schedule\n\n\n\nWeek\nTopic\nLab\nHomework\nProject\n\n\n\n\n1\nData science life cycle\n\n\n\n\n\n2\nTidy data\nL0\n\n\n\n\n3\nSampling and bias\nL1\n\n\n\n\n4\nStatistical graphics\nL2\nH1\n\n\n\n5\nKernel density estimation\nL3\n\nMP1\n\n\n6\nPrincipal components\nL4\nH2\n\n\n\n7\nSimple regression\n\n\nMP2\n\n\n8\nMultiple regression\nL5\nH3\n\n\n\n9\nClassification and clustering\n\n\nCP1\n\n\n10\nCase study\n\nH4\n\n\n\n11\nFinals week\n\n\nCP2"
  },
  {
    "objectID": "slides/week1-intro.html#assessments",
    "href": "slides/week1-intro.html#assessments",
    "title": "Course introduction",
    "section": "Assessments",
    "text": "Assessments\n\nLabs introduce and develop core skills\nHomeworks apply core skills to case studies\nProjects practice creative problem-solving"
  },
  {
    "objectID": "slides/week1-intro.html#policies",
    "href": "slides/week1-intro.html#policies",
    "title": "Course introduction",
    "section": "Policies",
    "text": "Policies\n\nCommunication\n\nIf you have questions, please come to office hours\nAvoid email except for personal matters\n\nDeadlines and late work\n\nOne-hour grace period on all deadlines\n48-hour late submissions\nTwo free lates on any assignment (except last assignment)\n75% partial credit thereafter for late work"
  },
  {
    "objectID": "slides/week1-intro.html#policies-1",
    "href": "slides/week1-intro.html#policies-1",
    "title": "Course introduction",
    "section": "Policies",
    "text": "Policies\n\nGrades\n\nRoughly 10-20-30-40 attendance-labs-homeworks-projects\nFinal weighting and grade assignment at instructor’s discretion\nDo not expect 92+% = A, 90-92% = A-, 87-89.9 = B+, etc.\nA’s are awarded sparingly and indicate exceptional work"
  },
  {
    "objectID": "slides/week1-intro.html#other-info",
    "href": "slides/week1-intro.html#other-info",
    "title": "Course introduction",
    "section": "Other info",
    "text": "Other info\n\nInformal section swaps are allowed with TA permission\nAttendance required at all class meetings, but a few absences without notice are okay\nHonors contracts not available this quarter\nOffice hours start week 2, check website for schedule"
  },
  {
    "objectID": "slides/week1-intro.html#getting-started",
    "href": "slides/week1-intro.html#getting-started",
    "title": "Course introduction",
    "section": "Getting started",
    "text": "Getting started\n\nLab this week will introduce you to computing and course infrastructure\nPlease fill out intake survey ASAP\nCheck access to Gradescope, LSIT, course page\nReview syllabus"
  },
  {
    "objectID": "slides/week1-lifecycle.html#whats-data-science",
    "href": "slides/week1-lifecycle.html#whats-data-science",
    "title": "Data science lifecycle",
    "section": "What’s data science?",
    "text": "What’s data science?\nData science is a term of art encompassing a wide range of activities that involve uncovering insights from quantitative information.\n\nPeople that refer to themselves as data scientists typically combine specific interests (“domain knowledge”, e.g., biology) with computation, mathematics, and statistics and probability to contribute to knowledge in their communities.\n\nIntersectional in nature\nNo singular disciplinary background among practitioners"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-science-lifecycle",
    "href": "slides/week1-lifecycle.html#data-science-lifecycle",
    "title": "Data science lifecycle",
    "section": "Data science lifecycle",
    "text": "Data science lifecycle\n\nData science lifecycle: an end-to-end process resulting in a data analysis product\n\n\nQuestion formulation\nData collection and cleaning\nExploration\nAnalysis\n\n\nThese form a cycle in the sense that the steps are iterated for question refinement and futher discovery."
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-science-lifecylce",
    "href": "slides/week1-lifecycle.html#data-science-lifecylce",
    "title": "Data science lifecycle",
    "section": "Data science lifecylce",
    "text": "Data science lifecylce\n\n\nThe point isn’t really the exact steps, but rather the notion of an iterative process."
  },
  {
    "objectID": "slides/week1-lifecycle.html#starting-with-a-question",
    "href": "slides/week1-lifecycle.html#starting-with-a-question",
    "title": "Data science lifecycle",
    "section": "Starting with a question",
    "text": "Starting with a question\nThe scaling of brains with bodies is thought to contain clues about evolutionary patterns pertaining to intelligence.\n\nThere are lots of datasets out there with brain and body weight measurements, so let’s consider the question:\n\nWhat is the relationship between an animal’s brain and body weight?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-acquisition",
    "href": "slides/week1-lifecycle.html#data-acquisition",
    "title": "Data science lifecycle",
    "section": "Data acquisition",
    "text": "Data acquisition\nFrom Allison et al. 1976, average body and brain weights for 62 mammals.\n\n\n\n\n\n\n  \n    \n      \n      species\n      body_wt\n      brain_wt\n    \n  \n  \n    \n      0\n      Africanelephant\n      6654.000\n      5712.0\n    \n    \n      1\n      Africangiantpouchedrat\n      1.000\n      6.6\n    \n    \n      2\n      ArcticFox\n      3.385\n      44.5\n    \n  \n\n\n\n\nUnits of measurement\n\nbody weight in kilograms\nbrain weight in grams"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment",
    "href": "slides/week1-lifecycle.html#data-assessment",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nHow well-matched is the data to our question?\n\nMammals only (no birds, fish, reptiles, etc.)\nSpecies are those for which convenient specimens were available\nAverages across specimens are reported (‘aggregated’ data)\n\n\nWhat do you think? Take a moment to discuss with your neighbor."
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment-1",
    "href": "slides/week1-lifecycle.html#data-assessment-1",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nBased on the great points you just made, we really only stand to learn something about this particular sample of animals.\n\nIn other words, no inference is possible.\n\n\n\nDo you think the data are still useful?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#inpection",
    "href": "slides/week1-lifecycle.html#inpection",
    "title": "Data science lifecycle",
    "section": "Inpection",
    "text": "Inpection\nThis dataset is already impeccably neat: each row is an observation for some species of mammal, and the columns are the two variables (average weight).\nSo no tidying needed – we’ll just check the dimensions and see if any values are missing.\n\n# dimensions?\nbb_weights.shape\n\n(62, 3)\n\n\n\n# missing values?\nbb_weights.isna().sum(axis = 0)\n\nspecies     0\nbody_wt     0\nbrain_wt    0\ndtype: int64"
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration",
    "href": "slides/week1-lifecycle.html#exploration",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nVisualization is usually a good starting point for exploring data.\n\n\n\n\n\n\n\nNotice the apparent density of points near \\((0, 0)\\) – that suggests we shouldn’t look for a relationship on the scale of kg/g."
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration-1",
    "href": "slides/week1-lifecycle.html#exploration-1",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nA simple transformation of the axes reveals a clearer pattern."
  },
  {
    "objectID": "slides/week1-lifecycle.html#analysis",
    "href": "slides/week1-lifecycle.html#analysis",
    "title": "Data science lifecycle",
    "section": "Analysis",
    "text": "Analysis\nThe plot shows us that there’s a roughly linear relationship on the log scale:\n\\[\\log(\\text{brain}) = \\alpha \\log(\\text{body}) + c\\]\n\nSo what does that mean in terms of brain and body weights? A little algebra and we have a “power law”:\n\\[(\\text{brain}) \\propto (\\text{body})^\\alpha\\]\n\n\nCheck your understanding: what’s the proportionality constant?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#interpretation",
    "href": "slides/week1-lifecycle.html#interpretation",
    "title": "Data science lifecycle",
    "section": "Interpretation",
    "text": "Interpretation\nSo it appears that the brain-body scaling is well-described by a power law:\n\namong selected specimens of these 62 species of mammal, species average brain weight is approximately proportional to a power of species average body weight\n\n\nNotice that I did not say:\n\nanimals’ brains are proportional to a power of their bodies\namong these 62 mammals, average brain weight is approximately proportional to a power of average body weight"
  },
  {
    "objectID": "slides/week1-lifecycle.html#question-refinement",
    "href": "slides/week1-lifecycle.html#question-refinement",
    "title": "Data science lifecycle",
    "section": "Question refinement",
    "text": "Question refinement\nWe can now ask further, more specific questions:\n\nDo other types of animals exhibit the same power law relationship?\n\n\nTo investigate, we need richer data."
  },
  {
    "objectID": "slides/week1-lifecycle.html#more-data-acquisition",
    "href": "slides/week1-lifecycle.html#more-data-acquisition",
    "title": "Data science lifecycle",
    "section": "(More) data acquisition",
    "text": "(More) data acquisition\nA number of authors have compiled and published ‘meta-analysis’ datasets by combining the results of multiple studies.\nBelow we’ll import a few of these for three different animal classes.\n\n# import metaanalysis datasets\nreptiles = pd.read_csv('data/reptile_meta.csv')\nbirds = pd.read_csv('data/bird_meta.csv', encoding = 'latin1')\nmammals = pd.read_csv('data/mammal_meta.csv', encoding = 'latin1')"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment-2",
    "href": "slides/week1-lifecycle.html#data-assessment-2",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nWhere does this data come from? It’s kind of a convenience sample of scientific data:\n\nMultiple studies \\(\\rightarrow\\) possibly different sampling and measurement protocols\nCriteria for inclusion unknown \\(\\rightarrow\\) probably neither comprehensive nor representative of all such measurements taken\n\n\nSo these data, while richer, are still relatively narrow in terms of generalizability."
  },
  {
    "objectID": "slides/week1-lifecycle.html#a-comment-on-scope-of-inference",
    "href": "slides/week1-lifecycle.html#a-comment-on-scope-of-inference",
    "title": "Data science lifecycle",
    "section": "A comment on scope of inference",
    "text": "A comment on scope of inference\nThese data don’t support general inferences (e.g., to all animals, all mammals, etc.) because they weren’t collected for the purpose to which we’re putting them.\n\nUsually, if data are not collected for the explicit purpose of the question you’re trying to answer, they won’t constitute a representative sample."
  },
  {
    "objectID": "slides/week1-lifecycle.html#tidying",
    "href": "slides/week1-lifecycle.html#tidying",
    "title": "Data science lifecycle",
    "section": "Tidying",
    "text": "Tidying\nBack to the task at hand, in order to comine the datasets one must:\n\nSelect columns of interest;\nPut in consistent order;\nGive consistent names;\nConcatenate row-wise.\n\n\nWe’ll skip the details for now."
  },
  {
    "objectID": "slides/week1-lifecycle.html#inspection",
    "href": "slides/week1-lifecycle.html#inspection",
    "title": "Data science lifecycle",
    "section": "Inspection",
    "text": "Inspection\nThis dataset has quite a lot of missing brain weight measurements: many of the studies combined to form these datasets did not include that particular measurement.\n\n# missing values?\ndata.isna().mean(axis = 0)\n\nOrder      0.00000\nFamily     0.00000\nGenus      0.00000\nSpecies    0.00000\nSex        0.00000\nbody       0.00000\nbrain      0.57404\nclass      0.00000\ndtype: float64"
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration-2",
    "href": "slides/week1-lifecycle.html#exploration-2",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nFocusing on the nonmissing values, we see the same power law relationship but with different proportionality constants and exponents for the three classes of animals."
  },
  {
    "objectID": "slides/week1-lifecycle.html#analysis-1",
    "href": "slides/week1-lifecycle.html#analysis-1",
    "title": "Data science lifecycle",
    "section": "Analysis",
    "text": "Analysis\nSo we might hypothesize that:\n\\[\n(\\text{brain}) = \\beta_1(\\text{body})^{\\alpha_1} \\qquad \\text{(mammal)} \\\\\n(\\text{brain}) = \\beta_2(\\text{body})^{\\alpha_2} \\qquad \\text{(reptile)} \\\\\n(\\text{brain}) = \\beta_3(\\text{body})^{\\alpha_3} \\qquad \\text{(bird)} \\\\\n\\beta_i \\neq \\beta_j, \\alpha_i \\neq \\alpha_j \\quad \\text{for } i \\neq j\n\\]"
  },
  {
    "objectID": "slides/week1-lifecycle.html#interpretation-1",
    "href": "slides/week1-lifecycle.html#interpretation-1",
    "title": "Data science lifecycle",
    "section": "Interpretation",
    "text": "Interpretation\nIt seems that the average brain and body weights of the birds, mammals, and reptiles measured in these studies exhibit distinct power law relationships.\n\nWhat would you investigate next?\n\nCorrelates of body weight?\nAdjust for lifespan, habitat, predation, etc.?\nEstimate the \\(\\alpha_i\\)’s and \\(\\beta_i\\)’s?\nPredict brain weights for unobserved species?\nSomething else?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#a-comment",
    "href": "slides/week1-lifecycle.html#a-comment",
    "title": "Data science lifecycle",
    "section": "A comment",
    "text": "A comment\nNotice that I did not mention the word ‘model’ anywhere!\n\nThis was intentional – it is a common misconception that analyzing data always involves fitting models.\n\nModels are not not always necessary or appropriate\nYou can learn a lot from exploratory techniques\nModels approximate specific kinds of relationships in data\nExploratory analysis can reveal unexpected structure"
  },
  {
    "objectID": "slides/week1-lifecycle.html#but-if-we-did-want-to-fit-a-model",
    "href": "slides/week1-lifecycle.html#but-if-we-did-want-to-fit-a-model",
    "title": "Data science lifecycle",
    "section": "But if we did want to fit a model…",
    "text": "But if we did want to fit a model…\n\\((\\text{brain}) = \\beta_j(\\text{body})^{\\alpha_j} \\quad \\text{animal class } j = 1, 2, 3\\)\n\nFigureEstimates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                      coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Bird                -1.9574     0.040   -49.118  0.000    -2.036    -1.879\n\n\n  Mammal              -2.9391     0.029  -100.061  0.000    -2.997    -2.882\n\n\n  Reptile             -4.0335     0.083   -48.577  0.000    -4.196    -3.871\n\n\n  log.body.bird        0.5653     0.008    66.566  0.000     0.549     0.582\n\n\n  log.body.mammal      0.7651     0.004   191.544  0.000     0.757     0.773\n\n\n  log.body.reptile     0.5293     0.017    31.375  0.000     0.496     0.562"
  },
  {
    "objectID": "slides/week1-lifecycle.html#model-limitations",
    "href": "slides/week1-lifecycle.html#model-limitations",
    "title": "Data science lifecycle",
    "section": "Model limitations",
    "text": "Model limitations\nBack to the issue of representativeness:\n\nshouldn’t use this model for inferences\nmight not be reliable for prediction either\nbut does capture/convey some suggestive comparisons\n\n\nSo, just be careful with interpretation of results:\n\n“For this particular collection of specimens, we estimated…”"
  },
  {
    "objectID": "slides/week1-lifecycle.html#zooming-out",
    "href": "slides/week1-lifecycle.html#zooming-out",
    "title": "Data science lifecycle",
    "section": "Zooming out",
    "text": "Zooming out\nThis example illustrates the aspects of the lifecylce we’ll cover in this class:\n\ndata retrieval and import\ntidying and transformation\nvisualization\nexploratory analysis\nmodeling\n\n\nWe’ll address these topics in sequence."
  },
  {
    "objectID": "slides/week1-lifecycle.html#next-week",
    "href": "slides/week1-lifecycle.html#next-week",
    "title": "Data science lifecycle",
    "section": "Next week",
    "text": "Next week\n\nTabular data structure\nData semantics\nTidy data\nTransformations of tabular data\nAggregation and grouping"
  },
  {
    "objectID": "slides/week2-tidy.html#announcements",
    "href": "slides/week2-tidy.html#announcements",
    "title": "Tidy data",
    "section": "Announcements",
    "text": "Announcements\n\nPDF export fixed on JupyterHub\nLab 0 due today 11:59PM; late submissions allowed until Wednesday 11:59PM\nComplete Q1-Q4 (fruit_info section) of Lab 1 before section"
  },
  {
    "objectID": "slides/week2-tidy.html#this-week",
    "href": "slides/week2-tidy.html#this-week",
    "title": "Tidy data",
    "section": "This week",
    "text": "This week\n\nTabular data\n\nMany ways to structure a dataset\nFew organizational constraints ‘in the wild’\n\nPrinciples of tidy data: matching semantics with structure\n\nData semantics: observations and variables\nTabular structure: rows and columns\nThe tidy standard\nCommon messes\nTidying operations\n\nTransforming data frames\n\nSubsetting (slicing and filtering)\nDerived variables\nAggregation and summary statistics"
  },
  {
    "objectID": "slides/week2-tidy.html#tabular-data",
    "href": "slides/week2-tidy.html#tabular-data",
    "title": "Tidy data",
    "section": "Tabular data",
    "text": "Tabular data\n\nMany possible layouts for tabular data\n‘Real’ datasets have few organizational constraints\n\n\nMost data are stored in tables, but there are always multiple possible tabular layouts for the same underlying data.\n\n\nLet’s look at some examples."
  },
  {
    "objectID": "slides/week2-tidy.html#mammal-data-long-layouts",
    "href": "slides/week2-tidy.html#mammal-data-long-layouts",
    "title": "Tidy data",
    "section": "Mammal data: long layouts",
    "text": "Mammal data: long layouts\nBelow is the Allison 1976 mammal brain-body weight dataset from last time shown in two ‘long’ layouts:\n\n\n\n\n\n\n\n  \n    \n      \n      body_wt\n      brain_wt\n    \n    \n      species\n      \n      \n    \n  \n  \n    \n      Africanelephant\n      6654.0\n      5712.0\n    \n    \n      Africangiantpouchedrat\n      1.0\n      6.6\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      measurement\n      weight\n    \n    \n      species\n      \n      \n    \n  \n  \n    \n      Africanelephant\n      brain_wt\n      5712.0\n    \n    \n      Africanelephant\n      body_wt\n      6654.0\n    \n    \n      Africangiantpouchedrat\n      brain_wt\n      6.6\n    \n    \n      Africangiantpouchedrat\n      body_wt\n      1.0"
  },
  {
    "objectID": "slides/week2-tidy.html#mammal-data-wide-layout",
    "href": "slides/week2-tidy.html#mammal-data-wide-layout",
    "title": "Tidy data",
    "section": "Mammal data: wide layout",
    "text": "Mammal data: wide layout\nHere’s a third possible layout for the mammal brain-body weight data:\n\n\n\n\n\n\n\n  \n    \n      species\n      Africanelephant\n      Africangiantpouchedrat\n      ArcticFox\n      Arcticgroundsquirrel\n    \n    \n      measurement\n      \n      \n      \n      \n    \n  \n  \n    \n      body_wt\n      6654.0\n      1.0\n      3.385\n      0.92\n    \n    \n      brain_wt\n      5712.0\n      6.6\n      44.500\n      5.70"
  },
  {
    "objectID": "slides/week2-tidy.html#gdp-growth-data-wide-layout",
    "href": "slides/week2-tidy.html#gdp-growth-data-wide-layout",
    "title": "Tidy data",
    "section": "GDP growth data: wide layout",
    "text": "GDP growth data: wide layout\nHere’s another example: World Bank data on annual GDP growth for 264 countries from 1961 – 2019.\n\n\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Country Code\n      2009\n      2010\n      2011\n    \n  \n  \n    \n      0\n      Aruba\n      ABW\n      -10.519749\n      -3.685029\n      3.446055\n    \n    \n      1\n      Afghanistan\n      AFG\n      21.390528\n      14.362441\n      0.426355\n    \n    \n      2\n      Angola\n      AGO\n      0.858713\n      4.403933\n      3.471976\n    \n    \n      3\n      Albania\n      ALB\n      3.350067\n      3.706892\n      2.545322\n    \n    \n      4\n      Andorra\n      AND\n      -5.302847\n      -1.974958\n      -0.008070"
  },
  {
    "objectID": "slides/week2-tidy.html#gdp-growth-data-long-layout",
    "href": "slides/week2-tidy.html#gdp-growth-data-long-layout",
    "title": "Tidy data",
    "section": "GDP growth data: long layout",
    "text": "GDP growth data: long layout\nHere’s an alternative layout for the annual GDP growth data:\n\n\n\n\n\n\n  \n    \n      \n      year\n      growth_pct\n    \n    \n      Country Name\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      2009\n      21.390528\n    \n    \n      Aruba\n      2009\n      -10.519749\n    \n    \n      Afghanistan\n      2010\n      14.362441\n    \n    \n      Aruba\n      2010\n      -3.685029\n    \n    \n      Afghanistan\n      2011\n      0.426355\n    \n    \n      Aruba\n      2011\n      3.446055"
  },
  {
    "objectID": "slides/week2-tidy.html#sb-weather-data-long-layouts",
    "href": "slides/week2-tidy.html#sb-weather-data-long-layouts",
    "title": "Tidy data",
    "section": "SB weather data: long layouts",
    "text": "SB weather data: long layouts\nA third example: daily minimum and maximum temperatures recorded at Santa Barbara Municipal Airport from January 2021 through March 2021.\n\n\n\n\n\n\n\n  \n    \n      \n      STATION\n      TMAX\n      TMIN\n      MONTH\n      DAY\n      YEAR\n    \n  \n  \n    \n      0\n      USW00023190\n      65\n      37\n      1\n      1\n      2021\n    \n    \n      1\n      USW00023190\n      62\n      38\n      1\n      2\n      2021\n    \n    \n      2\n      USW00023190\n      60\n      42\n      1\n      3\n      2021"
  },
  {
    "objectID": "slides/week2-tidy.html#sb-weather-data-wide-layout",
    "href": "slides/week2-tidy.html#sb-weather-data-wide-layout",
    "title": "Tidy data",
    "section": "SB weather data: wide layout",
    "text": "SB weather data: wide layout\nHere’s a wide layout for the SB weather data:\n\n\n\n\n\n\n\n  \n    \n      \n      DAY\n      1\n      2\n      3\n      4\n    \n    \n      MONTH\n      type\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      TMAX\n      65.0\n      62.0\n      60.0\n      72.0\n    \n    \n      TMIN\n      37.0\n      38.0\n      42.0\n      43.0\n    \n    \n      2\n      TMAX\n      66.0\n      67.0\n      69.0\n      63.0\n    \n    \n      TMIN\n      45.0\n      40.0\n      44.0\n      37.0\n    \n    \n      3\n      TMAX\n      68.0\n      66.0\n      59.0\n      62.0"
  },
  {
    "objectID": "slides/week2-tidy.html#un-development-data-multiple-tables",
    "href": "slides/week2-tidy.html#un-development-data-multiple-tables",
    "title": "Tidy data",
    "section": "UN development data: multiple tables",
    "text": "UN development data: multiple tables\nA final example: United Nations country development data organized into different tables according to variable type.\nHere is a table of population measurements:\n\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n    \n    \n      country\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n    \n  \n\n\n\n\nAnd here is a table of a few gender-related variables:\n\n\n\n\n\n\n\n\n  \n    \n      \n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n    \n  \n  \n    \n      Norway\n      0.045\n      40.8\n      60.4\n      67.2\n    \n    \n      Ireland\n      0.093\n      24.3\n      56.0\n      68.4"
  },
  {
    "objectID": "slides/week2-tidy.html#un-development-data-one-table",
    "href": "slides/week2-tidy.html#un-development-data-one-table",
    "title": "Tidy data",
    "section": "UN development data: one table",
    "text": "UN development data: one table\nHere are both tables merged by country:\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n      0.655\n      27.2\n      21.6\n      74.7\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n      0.181\n      29.5\n      46.7\n      64.6\n    \n    \n      Algeria\n      43.1\n      73.2\n      5.0\n      27.1\n      2.8\n      0.429\n      21.5\n      14.6\n      67.4"
  },
  {
    "objectID": "slides/week2-tidy.html#un-development-data-one-longer-table",
    "href": "slides/week2-tidy.html#un-development-data-one-longer-table",
    "title": "Tidy data",
    "section": "UN development data: one (longer) table",
    "text": "UN development data: one (longer) table\nAnd here is another arrangement of the merged table:\n\n\n\n\n\n\n  \n    \n      \n      gender_variable\n      gender_value\n      population_variable\n      population_value\n    \n    \n      country\n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      gender_inequality\n      0.655\n      total_pop\n      38.0\n    \n    \n      Albania\n      gender_inequality\n      0.181\n      total_pop\n      2.9\n    \n    \n      Algeria\n      gender_inequality\n      0.429\n      total_pop\n      43.1\n    \n    \n      Andorra\n      gender_inequality\n      NaN\n      total_pop\n      0.1\n    \n    \n      Angola\n      gender_inequality\n      0.536\n      total_pop\n      31.8"
  },
  {
    "objectID": "slides/week2-tidy.html#what-are-the-differences",
    "href": "slides/week2-tidy.html#what-are-the-differences",
    "title": "Tidy data",
    "section": "What are the differences?",
    "text": "What are the differences?\nIn short, the alternate layouts differ in three respects:\n\nRows\nColumns\nNumber of tables"
  },
  {
    "objectID": "slides/week2-tidy.html#how-to-choose",
    "href": "slides/week2-tidy.html#how-to-choose",
    "title": "Tidy data",
    "section": "How to choose?",
    "text": "How to choose?\nReturn to one of the examples and review the different layouts with your neighbor.\n\nList a few advantages and disadvantages for each layout.\nWhich do you prefer and why?"
  },
  {
    "objectID": "slides/week2-tidy.html#few-organizational-constraints",
    "href": "slides/week2-tidy.html#few-organizational-constraints",
    "title": "Tidy data",
    "section": "Few organizational constraints",
    "text": "Few organizational constraints\nIt’s surprisingly difficult to articulate reasons why one layout might be preferable to another.\n\nUsually the choice of layout isn’t principled\nIdiosyncratic: two people are likely to make different choices\n\n\nAs a result:\n\nFew widely used conventions\nLots of variability ‘in the wild’\nDatasets are often organized in bizarre ways"
  },
  {
    "objectID": "slides/week2-tidy.html#form-and-representation",
    "href": "slides/week2-tidy.html#form-and-representation",
    "title": "Tidy data",
    "section": "Form and representation",
    "text": "Form and representation\nBecause of the wide range of possible layouts for a dataset, and the variety of choices that are made about how to store data, data scientists are constantly faced with determining how best to reorganize datasets in a way that facilitates exploration and analysis.\n\nBroadly, this involves two interdependent choices:\n\nChoice of representation: how to encode information.\n\nExample: parse dates as ‘MM/DD/YYYY’ (one variable) or ‘MM’, ‘DD’, ‘YYYY’ (three variables)?\nExample: use values 1, 2, 3 or ‘low’, ‘med’, ‘high’?\nExample: name variables ‘question1’, ‘question2’, …, or ‘age’, ‘income’, …?\n\nChoice of form: how to display information\n\nExample: wide table or long table?\nExample: one table or many?"
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-data",
    "href": "slides/week2-tidy.html#tidy-data",
    "title": "Tidy data",
    "section": "Tidy data",
    "text": "Tidy data\nThe tidy data standard is a principled way of organizing tabular data. It has two main advantages:\n\nFacilitates workflow by establishing a consistent dataset structure.\nPrinciples are designed to make transformation, exploration, visualization, and modeling easy."
  },
  {
    "objectID": "slides/week2-tidy.html#semantics-and-structure",
    "href": "slides/week2-tidy.html#semantics-and-structure",
    "title": "Tidy data",
    "section": "Semantics and structure",
    "text": "Semantics and structure\n\n“Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored.” Wickham and Grolemund, R for Data Science, 2017.\n\n\nA dataset is a collection of values.\n\nthe semantics of a dataset are the meanings of the values\nthe structure of a dataset is the arrangement of the values"
  },
  {
    "objectID": "slides/week2-tidy.html#data-semantics",
    "href": "slides/week2-tidy.html#data-semantics",
    "title": "Tidy data",
    "section": "Data semantics",
    "text": "Data semantics\nTo introduce some general vocabulary, each value in a dataset is\n\nan observation\nof a variable\ntaken on an observational unit."
  },
  {
    "objectID": "slides/week2-tidy.html#units-variables-and-observations",
    "href": "slides/week2-tidy.html#units-variables-and-observations",
    "title": "Tidy data",
    "section": "Units, variables, and observations",
    "text": "Units, variables, and observations\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n    \n    \n      country\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n    \n    \n      Albania\n      2.9\n      61.2\n    \n  \n\n\n\n\n\nAn observational unit is the entity measured.\n\nAbove, country\n\nA variable is an attribute measured on each unit.\n\nAbove, total population and urban percentage\n\nAn observation is a collection of measurements taken on one unit.\n\nAbove, 38.0 and 25.8"
  },
  {
    "objectID": "slides/week2-tidy.html#identifying-units-variables-and-observations",
    "href": "slides/week2-tidy.html#identifying-units-variables-and-observations",
    "title": "Tidy data",
    "section": "Identifying units, variables, and observations",
    "text": "Identifying units, variables, and observations\nLet’s do an example. Here’s one record from the GDP growth data:\n\n\n\n\n\n\n\n  \n    \n      \n      year\n      growth_pct\n    \n    \n      Country Name\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      2010\n      14.362441\n    \n  \n\n\n\n\n\n\nAbove, the values -13.605441 and 1961 are observations of the variables GDP growth and year recorded for the observational unit Algeria."
  },
  {
    "objectID": "slides/week2-tidy.html#your-turn",
    "href": "slides/week2-tidy.html#your-turn",
    "title": "Tidy data",
    "section": "Your turn",
    "text": "Your turn\nWhat are the units, variables and observations?\n\n\n\n\n\n\n\n  \n    \n      \n      DAY\n      1\n      2\n      3\n      4\n    \n    \n      MONTH\n      type\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      TMAX\n      65.0\n      62.0\n      60.0\n      72.0\n    \n    \n      TMIN\n      37.0\n      38.0\n      42.0\n      43.0\n    \n    \n      2\n      TMAX\n      66.0\n      67.0\n      69.0\n      63.0\n    \n    \n      TMIN\n      45.0\n      40.0\n      44.0\n      37.0\n    \n  \n\n\n\n\n\n\nThink about it, then confer with your neighbor."
  },
  {
    "objectID": "slides/week2-tidy.html#data-structure",
    "href": "slides/week2-tidy.html#data-structure",
    "title": "Tidy data",
    "section": "Data structure",
    "text": "Data structure\nData structure refers to the form in which it is stored.\n\nTabular data is arranged in rows and columns.\n\n\nAs we saw, there are multiple structures – arrangements of rows and columns – available to represent any dataset."
  },
  {
    "objectID": "slides/week2-tidy.html#the-tidy-standard",
    "href": "slides/week2-tidy.html#the-tidy-standard",
    "title": "Tidy data",
    "section": "The tidy standard",
    "text": "The tidy standard\nThe tidy standard consists in matching semantics and structure. A dataset is tidy if:\n\nEach variable is a column.\nEach observation is a row.\nEach table contains measurements on only one type of observational unit.\n\n\nTidy data."
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy",
    "href": "slides/week2-tidy.html#tidy-or-messy",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\nLet’s revisit some of our examples of multiple layouts.\n\n\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Country Code\n      2009\n      2010\n      2011\n    \n  \n  \n    \n      0\n      Aruba\n      ABW\n      -10.519749\n      -3.685029\n      3.446055\n    \n    \n      1\n      Afghanistan\n      AFG\n      21.390528\n      14.362441\n      0.426355\n    \n    \n      2\n      Angola\n      AGO\n      0.858713\n      4.403933\n      3.471976\n    \n  \n\n\n\n\n\n\nWe can compare the semantics and structure for alignment:\n\n\n\nSemantics\n\nStructure\n\n\n\n\n\nObservations\nAnnual records\nRows\nCountries\n\n\nVariables\nGDP growth and year\nColumns\nValue of year\n\n\nObservational units\nCountries\nTables\nJust one\n\n\n\n\n\nRules 1 and 2 are violated, since column names are values (of year), not variables. Not tidy."
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy-1",
    "href": "slides/week2-tidy.html#tidy-or-messy-1",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\n\n\n\n\n\n\n\n  \n    \n      \n      year\n      growth_pct\n    \n    \n      Country Name\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      2009\n      21.390528\n    \n    \n      Aruba\n      2009\n      -10.519749\n    \n    \n      Afghanistan\n      2010\n      14.362441\n    \n    \n      Aruba\n      2010\n      -3.685029\n    \n  \n\n\n\n\n\n\nComparison of semantics and structure:\n\n\n\nSemantics\n\nStructure\n\n\n\n\n\nObservations\nAnnual records\nRows\nAnnual records\n\n\nVariables\nGDP growth and year\nColumns\nGDP growth and year\n\n\nObservational units\nCountries\nTables\nJust one\n\n\n\n\n\nAll three rules are met: rows are observations, columns are variables, and there’s one unit type and one table. Tidy."
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy-2",
    "href": "slides/week2-tidy.html#tidy-or-messy-2",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\n\n\n\n\n\n\n\n  \n    \n      \n      STATION\n      TMAX\n      TMIN\n      MONTH\n      DAY\n      YEAR\n    \n  \n  \n    \n      0\n      USW00023190\n      65\n      37\n      1\n      1\n      2021\n    \n    \n      1\n      USW00023190\n      62\n      38\n      1\n      2\n      2021\n    \n    \n      2\n      USW00023190\n      60\n      42\n      1\n      3\n      2021\n    \n  \n\n\n\n\nTry this one on your own. Then compare with your neighbor.\n\nIdentify the observations and variables\nWhat are the observational units?"
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy-3",
    "href": "slides/week2-tidy.html#tidy-or-messy-3",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\nIn undev1 and undev2:\n\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n    \n    \n      country\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n    \n  \n  \n    \n      Norway\n      0.045\n      40.8\n      60.4\n      67.2\n    \n    \n      Ireland\n      0.093\n      24.3\n      56.0\n      68.4\n    \n  \n\n\n\n\n\n\nHere there are multiple tables. To discuss:\n\nAre the observational units the same or different?\nBased on your answer above, is the data tidy or not?"
  },
  {
    "objectID": "slides/week2-tidy.html#common-messes",
    "href": "slides/week2-tidy.html#common-messes",
    "title": "Tidy data",
    "section": "Common messes",
    "text": "Common messes\n\n“Well, here’s another nice mess you’ve gotten me into” – Oliver Hardy\n\nThese examples illustrate some common messes:\n\nColumns are values, not variables\n\nGDP data: columns are 1961, 1962, …\n\nMultiple variables are stored in one column\n\nMammal data: weight column contains both body and brain weights\n\nVariables or values are stored in rows and columns\n\nWeather data: date values are stored in rows and columns, each column contains both min and max temperatures\n\nMeasurements on one type of observational unit are divided into multiple tables.\n\nUN development data: one table for population statistics and a separate table for gender statistics."
  },
  {
    "objectID": "slides/week2-tidy.html#tidying-operations",
    "href": "slides/week2-tidy.html#tidying-operations",
    "title": "Tidy data",
    "section": "Tidying operations",
    "text": "Tidying operations\nThese common messes can be cleaned up by some simple operations:\n\nmelt\n\nreshape a dataframe from wide to long format\n\npivot\n\nreshape a dataframe from long to wide format\n\nmerge\n\ncombine two dataframes row-wise by matching the values of certain columns"
  },
  {
    "objectID": "slides/week2-tidy.html#melt",
    "href": "slides/week2-tidy.html#melt",
    "title": "Tidy data",
    "section": "Melt",
    "text": "Melt\nMelting resolves the problem of having values stored as columns (common mess 1)."
  },
  {
    "objectID": "slides/week2-tidy.html#melt-1",
    "href": "slides/week2-tidy.html#melt-1",
    "title": "Tidy data",
    "section": "Melt",
    "text": "Melt\n\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Country Code\n      1961\n      1962\n      1963\n      1964\n      1965\n      1966\n      1967\n      1968\n      ...\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n  \n  \n    \n      0\n      Aruba\n      ABW\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      -3.685029\n      3.446055\n      -1.369863\n      4.198232\n      0.300000\n      5.700001\n      2.100000\n      1.999999\n      NaN\n      NaN\n    \n    \n      1\n      Afghanistan\n      AFG\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      14.362441\n      0.426355\n      12.752287\n      5.600745\n      2.724543\n      1.451315\n      2.260314\n      2.647003\n      1.189228\n      3.911603\n    \n  \n\n2 rows × 61 columns\n\n\n\n\n\n# in pandas\ngdp1.melt(\n    id_vars = ['Country Name', 'Country Code'], # which variables do you want to retain for each row? .\n    var_name = 'Year', # what do you want to name the variable that will contain the column names?\n    value_name = 'GDP Growth', # what do you want to name the variable that will contain the values?\n).head(2)\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Country Code\n      Year\n      GDP Growth\n    \n  \n  \n    \n      0\n      Aruba\n      ABW\n      1961\n      NaN\n    \n    \n      1\n      Afghanistan\n      AFG\n      1961\n      NaN"
  },
  {
    "objectID": "slides/week2-tidy.html#pivot",
    "href": "slides/week2-tidy.html#pivot",
    "title": "Tidy data",
    "section": "Pivot",
    "text": "Pivot\nPivoting resolves the issue of having multiple variables stored in one column (common mess 2). It’s the inverse operation of melting."
  },
  {
    "objectID": "slides/week2-tidy.html#pivot-1",
    "href": "slides/week2-tidy.html#pivot-1",
    "title": "Tidy data",
    "section": "Pivot",
    "text": "Pivot\n\n\n\n\n\n\n  \n    \n      \n      measurement\n      weight\n    \n    \n      species\n      \n      \n    \n  \n  \n    \n      Africanelephant\n      brain_wt\n      5712.0\n    \n    \n      Africanelephant\n      body_wt\n      6654.0\n    \n    \n      Africangiantpouchedrat\n      brain_wt\n      6.6\n    \n    \n      Africangiantpouchedrat\n      body_wt\n      1.0\n    \n  \n\n\n\n\n\n# in pandas\nmammal2.pivot(\n    columns = 'measurement', # which variable(s) do you want to send to new column names?\n    values = 'weight' # which variable(s) do you want to use to populate the new columns?\n).head(2)\n\n\n\n\n\n  \n    \n      measurement\n      body_wt\n      brain_wt\n    \n    \n      species\n      \n      \n    \n  \n  \n    \n      Africanelephant\n      6654.0\n      5712.0\n    \n    \n      Africangiantpouchedrat\n      1.0\n      6.6"
  },
  {
    "objectID": "slides/week2-tidy.html#pivot-and-melt",
    "href": "slides/week2-tidy.html#pivot-and-melt",
    "title": "Tidy data",
    "section": "Pivot and melt",
    "text": "Pivot and melt\nCommon mess 3 is a combination of messes 1 and 2: values or variables are stored in both rows and columns. Pivoting and melting in sequence can usually fix this.\n\n\n\n\n\n\n\n  \n    \n      \n      DAY\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      22\n      23\n      24\n      25\n      26\n      27\n      28\n      29\n      30\n      31\n    \n    \n      MONTH\n      type\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      TMAX\n      65.0\n      62.0\n      60.0\n      72.0\n      61.0\n      71.0\n      73.0\n      79.0\n      71.0\n      67.0\n      ...\n      61.0\n      59.0\n      65.0\n      55.0\n      57.0\n      54.0\n      55.0\n      55.0\n      58.0\n      63.0\n    \n    \n      TMIN\n      37.0\n      38.0\n      42.0\n      43.0\n      40.0\n      39.0\n      38.0\n      36.0\n      39.0\n      37.0\n      ...\n      41.0\n      40.0\n      38.0\n      44.0\n      40.0\n      48.0\n      49.0\n      42.0\n      37.0\n      37.0\n    \n    \n      2\n      TMAX\n      66.0\n      67.0\n      69.0\n      63.0\n      66.0\n      68.0\n      60.0\n      57.0\n      59.0\n      61.0\n      ...\n      75.0\n      75.0\n      70.0\n      66.0\n      69.0\n      76.0\n      68.0\n      NaN\n      NaN\n      NaN\n    \n    \n      TMIN\n      45.0\n      40.0\n      44.0\n      37.0\n      38.0\n      38.0\n      38.0\n      49.0\n      49.0\n      41.0\n      ...\n      37.0\n      39.0\n      41.0\n      39.0\n      36.0\n      43.0\n      38.0\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      TMAX\n      68.0\n      66.0\n      59.0\n      62.0\n      67.0\n      69.0\n      60.0\n      69.0\n      65.0\n      58.0\n      ...\n      71.0\n      72.0\n      67.0\n      65.0\n      63.0\n      72.0\n      73.0\n      77.0\n      NaN\n      NaN\n    \n    \n      TMIN\n      37.0\n      36.0\n      36.0\n      37.0\n      39.0\n      43.0\n      47.0\n      47.0\n      47.0\n      43.0\n      ...\n      50.0\n      49.0\n      41.0\n      44.0\n      40.0\n      41.0\n      41.0\n      42.0\n      NaN\n      NaN\n    \n  \n\n6 rows × 31 columns"
  },
  {
    "objectID": "slides/week2-tidy.html#pivot-and-melt-1",
    "href": "slides/week2-tidy.html#pivot-and-melt-1",
    "title": "Tidy data",
    "section": "Pivot and melt",
    "text": "Pivot and melt\n\nFirst, meltThen, pivot\n\n\n\nweather3.melt(\n    ignore_index = False,\n    var_name = 'day',\n    value_name = 'temp'\n).head()\n\n\n\n\n\n  \n    \n      \n      \n      day\n      temp\n    \n    \n      MONTH\n      type\n      \n      \n    \n  \n  \n    \n      1\n      TMAX\n      1\n      65.0\n    \n    \n      TMIN\n      1\n      37.0\n    \n    \n      2\n      TMAX\n      1\n      66.0\n    \n    \n      TMIN\n      1\n      45.0\n    \n    \n      3\n      TMAX\n      1\n      68.0\n    \n  \n\n\n\n\n\n\n\nweather3.melt(\n    ignore_index = False,\n    var_name = 'day',\n    value_name = 'temp'\n).reset_index().pivot(\n    index = ['MONTH', 'day'],\n    columns = 'type',\n    values = 'temp'\n).reset_index().rename_axis(columns = {'type': ''}).head()\n\n\n\n\n\n  \n    \n      \n      MONTH\n      day\n      TMAX\n      TMIN\n    \n  \n  \n    \n      0\n      1\n      1\n      65.0\n      37.0\n    \n    \n      1\n      1\n      2\n      62.0\n      38.0\n    \n    \n      2\n      1\n      3\n      60.0\n      42.0\n    \n    \n      3\n      1\n      4\n      72.0\n      43.0\n    \n    \n      4\n      1\n      5\n      61.0\n      40.0"
  },
  {
    "objectID": "slides/week2-tidy.html#merge",
    "href": "slides/week2-tidy.html#merge",
    "title": "Tidy data",
    "section": "Merge",
    "text": "Merge\nMerging resolves the issue of storing observations or variables on one unit type in multiple tables (mess 4). The basic idea is to combine by matching rows."
  },
  {
    "objectID": "slides/week2-tidy.html#merge-1",
    "href": "slides/week2-tidy.html#merge-1",
    "title": "Tidy data",
    "section": "Merge",
    "text": "Merge\nThe code below combines columns in each table by matching rows based on country.\n\npd.merge(undev1, undev2, on = 'country').head(4)\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n      0.655\n      27.2\n      21.6\n      74.7\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n      0.181\n      29.5\n      46.7\n      64.6\n    \n    \n      Algeria\n      43.1\n      73.2\n      5.0\n      27.1\n      2.8\n      0.429\n      21.5\n      14.6\n      67.4\n    \n    \n      Andorra\n      0.1\n      88.0\n      NaN\n      NaN\n      NaN\n      NaN\n      46.4\n      NaN\n      NaN"
  },
  {
    "objectID": "slides/week2-tidy.html#merge-2",
    "href": "slides/week2-tidy.html#merge-2",
    "title": "Tidy data",
    "section": "Merge",
    "text": "Merge\nThere are various rules for exactly how to merge, but the general syntactical procedure to merge dataframes df1 and df2 is this.\n\nSpecify an order: merge(df1, df2) or merge(df2, df1).\nSpecify keys: the shared columns to use for matching rows of df1 with rows of df2.\n\nfor example, merging on date will align rows in df2 with rows of df1 that have the same value for date\n\nSpecify a rule for which rows to return after merging\n\nkeep all rows with key entries in df1, drop non-matching rows in df2 (‘left’ join)\nkeep all rows with key entries in df2 drop non-matching rows in df1 (‘right’ join)\nkeep all rows with key entries in either df1 or df2, inducing missing values (‘outer’ join)\nkeep all rows with key entries in both df1 and df2 (‘inner’ join)"
  },
  {
    "objectID": "slides/week2-tidy.html#next-time",
    "href": "slides/week2-tidy.html#next-time",
    "title": "Tidy data",
    "section": "Next time",
    "text": "Next time\nTransformations of tabular data\n\nSlicing and filtering\nDefining new variables\nVectorized operatioons\nAggregation and grouping"
  },
  {
    "objectID": "slides/week2-transform.html#recap-tidy-data",
    "href": "slides/week2-transform.html#recap-tidy-data",
    "title": "Dataframe Transformations",
    "section": "Recap: tidy data",
    "text": "Recap: tidy data\nThe tidy standard consists in matching semantics and structure.\n\nA dataset is tidy if:\n\nEach variable is a column.\nEach observation is a row.\nEach table contains measurements on only one type of observational unit."
  },
  {
    "objectID": "slides/week2-transform.html#why-tidy",
    "href": "slides/week2-transform.html#why-tidy",
    "title": "Dataframe Transformations",
    "section": "Why tidy?",
    "text": "Why tidy?\n\nWhy use the tidy standard? Wouldn’t any system of organization do just as well?\n\n\nThe tidy standard has three main advantages:\n\nHaving a consistent system of organization makes it easier to focus on analysis and exploration. (True of any system)\nMany software tools are designed to work with tidy data inputs. (Tidy only)\nTransformation of tidy data is especially natural in most computing environments due to vectorized operations. (Tidy only)"
  },
  {
    "objectID": "slides/week2-transform.html#transformations",
    "href": "slides/week2-transform.html#transformations",
    "title": "Dataframe Transformations",
    "section": "Transformations",
    "text": "Transformations\nTransformations of data frames are operations that modify the shape or values of a data frame. These include:\n\nSlicing rows and columns by index\nFiltering rows by logical conditions\nDefining new variables from scratch or by operations on existing variables\nAggregations (min, mean, max, etc.)"
  },
  {
    "objectID": "slides/week2-transform.html#slicing",
    "href": "slides/week2-transform.html#slicing",
    "title": "Dataframe Transformations",
    "section": "Slicing",
    "text": "Slicing\nSlicing refers to retrieving a (usually contiguous) subset (a ‘slice’) of rows/columns from a data frame.\n\nUses:\n\ndata inspection/retrieval\nsubsetting for further analysis/manipulation\ndata display"
  },
  {
    "objectID": "slides/week2-transform.html#data-display",
    "href": "slides/week2-transform.html#data-display",
    "title": "Dataframe Transformations",
    "section": "Data display",
    "text": "Data display\nRecall the UN Development data:\n\n# preview UN data -- note indexed by country\nundev.head(3)\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n      0.655\n      27.2\n      21.6\n      74.7\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n      0.181\n      29.5\n      46.7\n      64.6\n    \n    \n      Algeria\n      43.1\n      73.2\n      5.0\n      27.1\n      2.8\n      0.429\n      21.5\n      14.6\n      67.4\n    \n  \n\n\n\n\n\nAside: .head() is a slicing operation – it returns the ‘top’ slice of rows."
  },
  {
    "objectID": "slides/week2-transform.html#data-inspectionretrieval",
    "href": "slides/week2-transform.html#data-inspectionretrieval",
    "title": "Dataframe Transformations",
    "section": "Data inspection/retrieval",
    "text": "Data inspection/retrieval\nTo inspect the percentage of women in parliament in Mexico, slice accordingly:\n\n\nundev.loc[['Mexico'], ['parliament_pct_women']]\n\n\n\n\n\n  \n    \n      \n      parliament_pct_women\n    \n    \n      country\n      \n    \n  \n  \n    \n      Mexico\n      48.4"
  },
  {
    "objectID": "slides/week2-transform.html#review-.loc-and-.iloc",
    "href": "slides/week2-transform.html#review-.loc-and-.iloc",
    "title": "Dataframe Transformations",
    "section": "Review: .loc and .iloc",
    "text": "Review: .loc and .iloc\nThe primary slicing functions in pandas are\n\n.loc (location) to slice by index\n.iloc (integer location) to slice by position\n\n\n\n# .iloc equivalent of previous slice\nundev.iloc[[111], [6]]\n\n\n\n\n\n  \n    \n      \n      parliament_pct_women\n    \n    \n      country\n      \n    \n  \n  \n    \n      Mexico\n      48.4\n    \n  \n\n\n\n\n\n\nCheck your understanding: which row in the dataframe is the observation for Mexico?\n\n\nIf a single index rather than a list is provided – e.g., Mexico rather than [Mexico], – these functions will return the raw value as a float rather than a dataframe.\n\nundev.loc['Mexico', 'parliament_pct_women']\n\n48.4"
  },
  {
    "objectID": "slides/week2-transform.html#larger-slices",
    "href": "slides/week2-transform.html#larger-slices",
    "title": "Dataframe Transformations",
    "section": "Larger slices",
    "text": "Larger slices\nMore typically, a slice will be a contiguous chunk of rows and columns.\n\nSlicing operations can interpret start:end as shorthand for a range of indices.\n\nundev.loc['Mexico':'Mongolia', ['parliament_pct_women']]\n\n\n\n\n\n  \n    \n      \n      parliament_pct_women\n    \n    \n      country\n      \n    \n  \n  \n    \n      Mexico\n      48.4\n    \n    \n      Micronesia (Federated States of)\n      0.0\n    \n    \n      Moldova (Republic of)\n      25.7\n    \n    \n      Mongolia\n      17.3\n    \n  \n\n\n\n\n\n\nNote: start:end is inclusive of both endpoints with .loc, but not inclusive of the right endpoint with .iloc. Get in the habit of double-checking results."
  },
  {
    "objectID": "slides/week2-transform.html#defining-new-variables",
    "href": "slides/week2-transform.html#defining-new-variables",
    "title": "Dataframe Transformations",
    "section": "Defining new variables",
    "text": "Defining new variables\nVectorization of operations in pandas and numpy make tidy data especially nice to manipulate mathematically. For example:\n\n\nweather2['TRANGE'] = weather2.TMAX - weather2.TMIN\nweather2.loc[0:3, ['TMAX', 'TMIN', 'TRANGE']]\n\n\n\n\n\n  \n    \n      \n      TMAX\n      TMIN\n      TRANGE\n    \n  \n  \n    \n      0\n      65\n      37\n      28\n    \n    \n      1\n      62\n      38\n      24\n    \n    \n      2\n      60\n      42\n      18\n    \n    \n      3\n      72\n      43\n      29\n    \n  \n\n\n\n\n\n\nThis computes \\(t_{min, i} - t_{max, i}\\) for all observations \\(i = 1, \\dots, n\\).\n\n\nCheck your understanding: express this calculation as a linear algebra arithmetic operation."
  },
  {
    "objectID": "slides/week2-transform.html#your-turn",
    "href": "slides/week2-transform.html#your-turn",
    "title": "Dataframe Transformations",
    "section": "Your turn",
    "text": "Your turn\nLet’s take another example – consider this slice of the undev data:\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n    \n    \n      country\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n    \n    \n      Albania\n      2.9\n      61.2\n    \n    \n      Algeria\n      43.1\n      73.2\n    \n  \n\n\n\n\n\nWith your neighbor, write a line of code that calculates the percentage of the population living in rural areas."
  },
  {
    "objectID": "slides/week2-transform.html#filtering",
    "href": "slides/week2-transform.html#filtering",
    "title": "Dataframe Transformations",
    "section": "Filtering",
    "text": "Filtering\nFiltering refers to removing a subset of rows based on one or more conditions. (Think of “filtering out” certain rows.)\n\nFor example, suppose we wanted to retrieve only the countries with populations exceeding 1Bn people:\n\nundev[undev.total_pop > 1000]\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      China\n      1433.8\n      60.3\n      85.0\n      1014.0\n      164.5\n      0.168\n      24.9\n      60.5\n      75.3\n    \n    \n      India\n      1366.4\n      34.5\n      116.8\n      915.6\n      87.1\n      0.488\n      13.5\n      20.5\n      76.1"
  },
  {
    "objectID": "slides/week2-transform.html#filtering-1",
    "href": "slides/week2-transform.html#filtering-1",
    "title": "Dataframe Transformations",
    "section": "Filtering",
    "text": "Filtering\nTechnically, filtering works by slicing according to a long logical vector with one entry per row specifying whether to retain (True) or drop (False).\n\nundev.total_pop > 1000\n\ncountry\nAfghanistan                           False\nAlbania                               False\nAlgeria                               False\nAndorra                               False\nAngola                                False\n                                      ...  \nVenezuela (Bolivarian Republic of)    False\nViet Nam                              False\nYemen                                 False\nZambia                                False\nZimbabwe                              False\nName: total_pop, Length: 189, dtype: bool"
  },
  {
    "objectID": "slides/week2-transform.html#a-small-puzzle",
    "href": "slides/week2-transform.html#a-small-puzzle",
    "title": "Dataframe Transformations",
    "section": "A small puzzle",
    "text": "A small puzzle\nConsider a random filter:\n\nrandom_filter = np.random.binomial(n = 1, p = 0.03, size = undev.shape[0]).astype('bool')\n\nrandom_filter\n\narray([False, False, False, False,  True, False, False, False, False,\n       False, False,  True, False, False, False, False,  True, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False,  True, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False,  True, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False,  True,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False,  True,\n       False, False, False, False, False, False, False, False, False,\n        True,  True, False, False, False, False, False, False, False])\n\n\n\n\nHow many rows will undev[random_filter] have?\nHow many rows should this random filtering produce on average?"
  },
  {
    "objectID": "slides/week2-transform.html#logical-comparisons",
    "href": "slides/week2-transform.html#logical-comparisons",
    "title": "Dataframe Transformations",
    "section": "Logical comparisons",
    "text": "Logical comparisons\nAny of the following relations can be used to define filtering conditions\n\n\n\nSymbol\nUsage\nMeaning\n\n\n\n\n==\na == b\nDoes a equal b?\n\n\n<=\na <= b\nIs a less than or equal to b?\n\n\n>=\na >= b\nIs a greater than or equal to b?\n\n\n<\na < b\nIs a less than b?\n\n\n>\na > b\nIs a greater than b?\n\n\n~\n~p\nReturns negation of p\n\n\n|\np | q\np OR q\n\n\n&\np & q\np AND q\n\n\n^\np ^ q\np XOR q (exclusive or)"
  },
  {
    "objectID": "slides/week2-transform.html#aggregation",
    "href": "slides/week2-transform.html#aggregation",
    "title": "Dataframe Transformations",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation refers to any operation that combines many values into fewer values.\n\nCommon aggregation operations include:\n\nsummation \\(\\sum_{i} x_i\\)\naveraging \\(n^{-1} \\sum_i x_i\\)\nextrema \\(\\text{min}_i x_i\\) and \\(\\text{max}_i x_i\\)\nstatistics: median, variance, standard deviation, mean absolute deviation, order statistics, quantiles"
  },
  {
    "objectID": "slides/week2-transform.html#aggregation-vs.-other-transformations",
    "href": "slides/week2-transform.html#aggregation-vs.-other-transformations",
    "title": "Dataframe Transformations",
    "section": "Aggregation vs. other transformations",
    "text": "Aggregation vs. other transformations\nAggregations reduce the number of values, whereas other transformations do not.\n\nA bit more formally:\n\naggregations map larger sets of values to smaller sets of values\ntransformations map sets of values to sets of the same size\n\n\n\nCheck your understanding:\n\nis \\((f*g)(x_i) = \\int f(h)g(x_i - h)dh\\) an aggregation?\nis \\(f(x_1, x_2, \\dots, x_n) = \\left(\\prod_i x_i\\right)^{\\frac{1}{n}}\\) an aggregation?"
  },
  {
    "objectID": "slides/week2-transform.html#aggregation-1",
    "href": "slides/week2-transform.html#aggregation-1",
    "title": "Dataframe Transformations",
    "section": "Aggregation?",
    "text": "Aggregation?\n\nGaussian blur."
  },
  {
    "objectID": "slides/week2-transform.html#example-aggregations",
    "href": "slides/week2-transform.html#example-aggregations",
    "title": "Dataframe Transformations",
    "section": "Example aggregations",
    "text": "Example aggregations\nIn numpy, the most common aggregations are implemented as functions:\n\n\n\nnumpy\nfunction\n\n\n\n\nnp.sum()\n\\(\\sum_i x_i\\)\n\n\nnp.max()\n\\(\\text{max}(x_1, \\dots, x_n)\\)\n\n\nnp.min()\n\\(\\text{min}(x_1, \\dots, x_n)\\)\n\n\nnp.median()\n\\(\\text{median}(x_1, \\dots, x_n)\\)\n\n\nnp.mean()\n\\(n^{-1}\\sum_{i = 1}^n x_i\\)\n\n\nnp.var()\n\\((n - 1)^{-1}\\sum_{i = 1}^n (x_i - \\bar{x})^2\\)\n\n\nnp.std()\n\\(\\sqrt{(n - 1)^{-1}\\sum_{i = 1}^n (x_i - \\bar{x})^2}\\)\n\n\nnp.prod()\n\\(\\prod_i x_i\\)\n\n\nnp.percentile()\n\\(\\hat{F}^{-1}(q)\\)"
  },
  {
    "objectID": "slides/week2-transform.html#argmin-and-argmax",
    "href": "slides/week2-transform.html#argmin-and-argmax",
    "title": "Dataframe Transformations",
    "section": "Argmin and argmax",
    "text": "Argmin and argmax\n\\(\\text{argmax}_D f(x)\\) refers to the value or values in the domain \\(D\\) of \\(f\\) at which the function attains its maximum – the argument in \\(D\\) maximizing \\(f\\).\n\nSimilarly, \\(\\text{argmax}_i x_i\\) refers to the index (or indices, if ties) of the largest value in the set \\(\\{x_i\\}\\).\n\n\nCheck your understanding: what does the following return?\n\nnp.array([1, 5, 10, 2]).argmin()"
  },
  {
    "objectID": "slides/week2-transform.html#argmin-and-argmax-1",
    "href": "slides/week2-transform.html#argmin-and-argmax-1",
    "title": "Dataframe Transformations",
    "section": "Argmin and argmax",
    "text": "Argmin and argmax\nThese index retrieval functions can be handy for slicing rows of interest.\n\nFor example, which country had the largest percentage of women in parliament in the year the UN development data was collected?\n\n\n\nundev.index[undev.parliament_pct_women.argmax()]\n\n'Rwanda'\n\n\n\n\nAnd what were the observations?\n\n\n\nundev.iloc[undev.parliament_pct_women.argmax(), :]\n\ntotal_pop                    12.600\nurban_pct_pop                17.300\npop_under5                    1.800\npop_15to64                    7.200\npop_over65                    0.400\ngender_inequality             0.402\nparliament_pct_women         55.700\nlabor_participation_women    83.900\nlabor_participation_men      83.400\nName: Rwanda, dtype: float64"
  },
  {
    "objectID": "slides/week2-transform.html#dataframe-aggregations",
    "href": "slides/week2-transform.html#dataframe-aggregations",
    "title": "Dataframe Transformations",
    "section": "Dataframe aggregations",
    "text": "Dataframe aggregations\nIn pandas, the numpy aggregation operations are available as dataframe methods that apply the corresponding operation over each column:\n\n# mean of every column\nundev.mean()\n\ntotal_pop                    40.423810\nurban_pct_pop                58.660847\npop_under5                    3.666120\npop_15to64                   27.250820\npop_over65                    3.797814\ngender_inequality             0.344154\nparliament_pct_women         23.093048\nlabor_participation_women    52.139888\nlabor_participation_men      72.470787\ndtype: float64"
  },
  {
    "objectID": "slides/week2-transform.html#row-wise-aggregation",
    "href": "slides/week2-transform.html#row-wise-aggregation",
    "title": "Dataframe Transformations",
    "section": "Row-wise aggregation",
    "text": "Row-wise aggregation\nIn general, supplying the argument axis = 1 will compute rowwise aggregations. For example:\n\n# sum `pop_under5`, `pop_15to64`, and `pop_over65`\nundev.iloc[:, 2:5].sum(axis = 1).head(3)\n\ncountry\nAfghanistan    27.5\nAlbania         2.6\nAlgeria        34.9\ndtype: float64\n\n\n\nThis facilitates, for example:\n\nundev['pop_5to14'] = undev.total_pop - undev.iloc[:, 2:5].sum(axis = 1)"
  },
  {
    "objectID": "slides/week2-transform.html#argminidxmin-and-argmaxidxmax",
    "href": "slides/week2-transform.html#argminidxmin-and-argmaxidxmax",
    "title": "Dataframe Transformations",
    "section": "Argmin/idxmin and argmax/idxmax",
    "text": "Argmin/idxmin and argmax/idxmax\nIn pandas, np.argmin() and np.argmax() are implemented as pd.df.idxmin() and pd.df.idxmax().\n\nundev.idxmax()\n\ntotal_pop                                     China\nurban_pct_pop                Hong Kong, China (SAR)\npop_under5                                    India\npop_15to64                                    China\npop_over65                                    China\ngender_inequality                             Yemen\nparliament_pct_women                         Rwanda\nlabor_participation_women                    Rwanda\nlabor_participation_men                       Qatar\npop_5to14                                     India\ndtype: object"
  },
  {
    "objectID": "slides/week2-transform.html#other-functions",
    "href": "slides/week2-transform.html#other-functions",
    "title": "Dataframe Transformations",
    "section": "Other functions",
    "text": "Other functions\nPandas has a wide array of other aggregation and transformation functions. To show just one example:\n\n## slice weather data\nweather4 = weather1.set_index('DATE').iloc[:, 2:4]\nweather4.head(2)\n\n\n\n\n\n  \n    \n      \n      TMAX\n      TMIN\n    \n    \n      DATE\n      \n      \n    \n  \n  \n    \n      1/1/2021\n      65\n      37\n    \n    \n      1/2/2021\n      62\n      38\n    \n  \n\n\n\n\n\n\n# rolling average\nweather4.rolling(window = 7).mean().head(10)\n\n\n\n\n\n  \n    \n      \n      TMAX\n      TMIN\n    \n    \n      DATE\n      \n      \n    \n  \n  \n    \n      1/1/2021\n      NaN\n      NaN\n    \n    \n      1/2/2021\n      NaN\n      NaN\n    \n    \n      1/3/2021\n      NaN\n      NaN\n    \n    \n      1/4/2021\n      NaN\n      NaN\n    \n    \n      1/5/2021\n      NaN\n      NaN\n    \n    \n      1/6/2021\n      NaN\n      NaN\n    \n    \n      1/7/2021\n      66.285714\n      39.571429\n    \n    \n      1/8/2021\n      68.285714\n      39.428571\n    \n    \n      1/9/2021\n      69.571429\n      39.571429\n    \n    \n      1/10/2021\n      70.571429\n      38.857143"
  },
  {
    "objectID": "slides/week2-transform.html#check-your-understanding",
    "href": "slides/week2-transform.html#check-your-understanding",
    "title": "Dataframe Transformations",
    "section": "Check your understanding",
    "text": "Check your understanding\nInterpret this result:\n\nweather4.rolling(window = 7).mean().idxmax()\n\nTMAX    1/20/2021\nTMIN    3/24/2021\ndtype: object\n\n\n(The weather data is January through March.)"
  },
  {
    "objectID": "slides/week2-transform.html#custom-functions",
    "href": "slides/week2-transform.html#custom-functions",
    "title": "Dataframe Transformations",
    "section": "Custom functions",
    "text": "Custom functions\nSee the documentation for a comprehensive list of transformations and aggregations.\n\nIf pandas doesn’t have a method for an operation you’re wanting to perform, you can implement custom transformations/aggregations with:\n\npd.df.apply() or pd.df.transform() apply a function row-wise or column-wise\npd.df.agg() or pd.df.aggregate()"
  },
  {
    "objectID": "slides/week2-transform.html#custom-functions-1",
    "href": "slides/week2-transform.html#custom-functions-1",
    "title": "Dataframe Transformations",
    "section": "Custom functions",
    "text": "Custom functions\nHere’s an example:\n\n\n\n\n\n\n  \n    \n      \n      1961\n      1962\n      1963\n      1964\n      1965\n      1966\n      1967\n      1968\n      1969\n      1970\n      ...\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Country Name\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Argentina\n      5.427843\n      -0.852022\n      -5.308197\n      10.130298\n      10.569433\n      -0.659726\n      3.191997\n      4.822501\n      9.679526\n      3.045643\n      ...\n      10.125398\n      6.003952\n      -1.026420\n      2.405324\n      -2.512615\n      2.731160\n      -2.080328\n      2.818503\n      -2.565352\n      -2.088015\n    \n    \n      Australia\n      2.485769\n      1.296087\n      6.214630\n      6.978522\n      5.983506\n      2.382458\n      6.302620\n      5.095814\n      7.044329\n      7.172187\n      ...\n      2.067417\n      2.462756\n      3.918163\n      2.584898\n      2.533115\n      2.192647\n      2.770652\n      2.300611\n      2.949286\n      2.160956\n    \n  \n\n2 rows × 59 columns\n\n\n\n\n\n# convert percentages to proportions\ngdp_prop = gdp.transform(lambda x: x/100 + 1)\n\n# compute geometric mean\ngdp_prop.aggregate(\n    lambda x: np.prod(x)**(1/len(x)), \n    axis = 1).head(4)\n\nCountry Name\nArgentina    1.022831\nAustralia    1.034228\nAustria      1.027254\nBurundi      1.023854\ndtype: float64"
  },
  {
    "objectID": "slides/week2-transform.html#your-turn-1",
    "href": "slides/week2-transform.html#your-turn-1",
    "title": "Dataframe Transformations",
    "section": "Your turn",
    "text": "Your turn\nHere’s the country with the highest annualized GDP growth for the period 1961-2019:\n\n\nCountry Name\nBotswana    1.079442\ndtype: float64\n\n\n\nHow did I find this? Suppose that the result on the previous slide were stored as gdp_annualized. Write a line of code that generates the result shown above."
  },
  {
    "objectID": "slides/week2-transform.html#grouped-aggregations",
    "href": "slides/week2-transform.html#grouped-aggregations",
    "title": "Dataframe Transformations",
    "section": "Grouped aggregations",
    "text": "Grouped aggregations\nSuppose we wanted to compute annualized growth by decade for each country.\n\nTo do so, we’d compute the same aggregation (geometric mean) repeatedly for subsets of data values. This is called a grouped aggregation.\n\n\nUsually, one defines a grouping of dataframe rows using columns in the dataset. For example:\n\ngdp_decades.head(4)\n\n\n\n\n\n  \n    \n      \n      Country Name\n      growth\n      decade\n    \n  \n  \n    \n      0\n      Argentina\n      1.054278\n      1960\n    \n    \n      1\n      Australia\n      1.024858\n      1960\n    \n    \n      2\n      Austria\n      1.055380\n      1960\n    \n    \n      3\n      Burundi\n      0.862539\n      1960\n    \n  \n\n\n\n\n\n\nHow should the rows be grouped?"
  },
  {
    "objectID": "slides/week2-transform.html#groupby",
    "href": "slides/week2-transform.html#groupby",
    "title": "Dataframe Transformations",
    "section": ".groupby",
    "text": ".groupby\nIn pandas, df.groupby('COLUMN') defines a grouping of dataframe rows in which each group is a set of rows with the same value of 'COLUMN'.\n\nThere will be exactly as many groups as the number of unique values in 'COLUMN'.\nMultiple columns may be specified to define a grouping, e.g., df.groupby(['COL1', 'COL2'])\nSubsequent operations will be performed group-wise"
  },
  {
    "objectID": "slides/week2-transform.html#annualized-gdp-growth-by-decade",
    "href": "slides/week2-transform.html#annualized-gdp-growth-by-decade",
    "title": "Dataframe Transformations",
    "section": "Annualized GDP growth by decade",
    "text": "Annualized GDP growth by decade\nReturning to our example:\n\ngdp_anngrowth = gdp_decades.groupby(\n    ['Country Name', 'decade']\n    ).aggregate(\n    lambda x: np.prod(x)**(1/len(x))\n    )\n\ngdp_anngrowth\n\n\n\n\n\n  \n    \n      \n      \n      growth\n    \n    \n      Country Name\n      decade\n      \n    \n  \n  \n    \n      Algeria\n      1960\n      1.030579\n    \n    \n      1970\n      1.068009\n    \n    \n      1980\n      1.027661\n    \n    \n      1990\n      1.015431\n    \n    \n      2000\n      1.038750\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      Zimbabwe\n      1970\n      1.038505\n    \n    \n      1980\n      1.051066\n    \n    \n      1990\n      1.027630\n    \n    \n      2000\n      0.944765\n    \n    \n      2010\n      1.055855\n    \n  \n\n714 rows × 1 columns"
  },
  {
    "objectID": "slides/week2-transform.html#your-turn-2",
    "href": "slides/week2-transform.html#your-turn-2",
    "title": "Dataframe Transformations",
    "section": "Your turn",
    "text": "Your turn\nHow do you find the country with the highest annualized GDP growth for each decade?\n\nWrite a line of code that would perform this calculation.\n\ngdp_anngrowth...\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      growth\n    \n    \n      decade\n      \n    \n  \n  \n    \n      1960\n      (Iran, Islamic Rep., 1960)\n    \n    \n      1970\n      (Botswana, 1970)\n    \n    \n      1980\n      (Botswana, 1980)\n    \n    \n      1990\n      (China, 1990)\n    \n    \n      2000\n      (Myanmar, 2000)\n    \n    \n      2010\n      (China, 2010)"
  },
  {
    "objectID": "slides/week2-transform.html#recap",
    "href": "slides/week2-transform.html#recap",
    "title": "Dataframe Transformations",
    "section": "Recap",
    "text": "Recap\n\nIn tidy data, rows and columns correspond to observations and variables.\n\nThis provides a standard dataset structure that facilitates exploration and analysis.\nMany datasets are not stored in this format.\nTransformation operations are a lot easier with tidy data, due in part to the way tools in pandas are designed.\n\nTransformations are operations that modify the shape or values of dataframes. We discussed\n\nslicing\nfiltering\ncreating new variables\naggregations (mean, min, max, argmin, etc.)\ngrouped aggregations\n\nDataframe manipulations will be used throughout the course to tidy up data and perform various inspections and summaries."
  },
  {
    "objectID": "slides/week2-transform.html#up-next",
    "href": "slides/week2-transform.html#up-next",
    "title": "Dataframe Transformations",
    "section": "Up next",
    "text": "Up next\nWe started en media res at this stage of the lifecyle (tidy) so that you could start developing skills that would enable you to jump right into playing with datasets.\n\nNext week, we’ll backtrack to the data collection and assessment stages of a project and discuss:\n\nsampling\nscope of inference\ndata assessment\nmissing data"
  },
  {
    "objectID": "hw/hw1-brfss/hw1-brfss-soln.html#recoding-and-renaming-categorical-variables",
    "href": "hw/hw1-brfss/hw1-brfss-soln.html#recoding-and-renaming-categorical-variables",
    "title": "PSTAT100",
    "section": "Recoding and renaming categorical variables",
    "text": "Recoding and renaming categorical variables\nIn the following series of questions you’ll tidy up the subsample by performing these steps:\n\nselecting columns of interest;\nreplacing coded values of question responses with responses;\ndefining new variables based on existing ones;\nrenaming columns.\n\nThe goal of this is to produce a clean version of the dataset that is well-organized, intuitive to navigate, and ready for analysis.\nNotice the missing values. How many entries are missing in each column? The cell below computes the proportion of missing values for each of the selected variables.\n\n# proportions of missingness -- uncomment after resolving q1a\nsamp.isna().mean()\n\nGENHLTH     0.0000\nADDEPEV3    0.0000\nACEDEPRS    0.8086\nACEDRINK    0.8088\nACEDRUGS    0.8088\nACEPRISN    0.8088\n_LLCPWT     0.0000\n_SEX        0.0000\n_AGEG5YR    0.0000\n_SMOKER3    0.0000\ndtype: float64"
  },
  {
    "objectID": "hw/hw1-brfss/hw1-brfss.html#recoding-and-renaming-categorical-variables",
    "href": "hw/hw1-brfss/hw1-brfss.html#recoding-and-renaming-categorical-variables",
    "title": "PSTAT100",
    "section": "Recoding and renaming categorical variables",
    "text": "Recoding and renaming categorical variables\nIn the following series of questions you’ll tidy up the subsample by performing these steps:\n\nselecting columns of interest;\nreplacing coded values of question responses with responses;\ndefining new variables based on existing ones;\nrenaming columns.\n\nThe goal of this is to produce a clean version of the dataset that is well-organized, intuitive to navigate, and ready for analysis.\nNotice the missing values. How many entries are missing in each column? The cell below computes the proportion of missing values for each of the selected variables.\n\n# proportions of missingness -- uncomment after resolving q1a\nsamp.isna().mean()"
  }
]