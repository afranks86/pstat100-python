[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Instructor: Trevor Ruiz\nTeaching assistants: Mengye Liu, Harry Yu, Gabrielle Salo\nClass meetings: M-W 12:30pm – 1:45pm Buchanan 1920\nSection meetings:\n\nM 2:00pm – 2:50pm Phelps 1525 (Mengye)\nM 3:00pm – 3:50pm Phelps 1525 (Mengye)\nM 4:00pm – 4:50pm Phelps 1513 (Harry)\nM 5:00pm – 5:50pm Phelps 1525 (Harry)\nM 6:00pm – 6:50pm Phelps 1525 (Gabrielle)\n\nOffice hours:\n\nMengye M 9:00am – 11:00am on Zoom\nGabrielle Tu 7:00pm – 8:00pm on Zoom\nHarry W 2:00pm – 4:00pm Building 434 Room 113\nTrevor W 2:00pm – 3:00pm ILP 2207"
  },
  {
    "objectID": "about.html#content-and-materials",
    "href": "about.html#content-and-materials",
    "title": "Course syllabus",
    "section": "Content and materials",
    "text": "Content and materials\nData Science Concepts and Analysis (PSTAT100) is a hands-on introduction to data science intended for intermediate-level students from any discipline with some exposure to probability and basic computing skills, but few or no upper-division courses in statistics or computer science. The course introduces central concepts in statistics – such as sampling variation, uncertainty, and inference – in an applied setting together with techniques for data exploration and analysis. Applications emphasize end-to-end data analyses. Course activities model standard data science workflow practices by example, and successful students acquire programming skills, project management skills, and subject exposure that will serve them well in upper-division courses as well as in independent research or projects.\n\nCatalog description\nOverview of data science key concepts and the use of tools for data retrieval, analysis, visualization, and reproducible research. Topics include an introduction to inference and prediction, principles of measurement, missing data, and notions of causality, statistical “traps”, and concepts in data ethics and privacy. Case studies will illustrate the importance of domain knowledge. Credit units: 4.\n\n\nPrerequisites\n\nProbability and Statistics I (PSTAT 120A)\nLinear Algebra (MATH 4A)\nPrior experience with Python or another programming language (CMPSC 9 or CMPSC 16).\n\n\n\nLearning outcomes\nSuccessful students will establish foundational data science skills:\n\ncritical assessment of data quality and sampling design\nretrieval, inspection, and cleaning of raw data\nexploratory, descriptive, visual, and inferential techniques\ninterpretation and communication of results in context\n\nThese skills will be discussed in depth during course lectures; students will practice them through lab activities, homework assignments, and project work.\n\n\nAssessments\nAttainment of course learning outcomes will be measured by assessment of submitted work. Submitted work falls into four categories:\n\nLabs will be assigned weekly in most weeks. These are structured coding assignments with small exercises throughout that introduce the programming skills needed to complete homework assignments.\nHomeworks will be assigned biweekly. These are fairly involved assignments which apply concepts and techniques from the lectures and programming skills from the labs to real data sets in order to reproduce an analysis and answer substantive questions. Collaboration is encouraged, but students must write up and submit their own work individually.\nMini projects will be assigned biweekly in alternation with homeworks. These assignments prompt students to use skills from the course in an unstructured setting to answer high-level questions pertaining to one or more datasets. Mini projects should be completed collaboratively.\nA course project will be assigned requiring students to carry out an open-ended data analysis. This will be completed in teams. Each team will prepare a project plan for initial feedback a few weeks before the end of the quarter, and submit a final report of work and findings by the end of the quarter.\n\nOverall scores in the course will be calculated for each student as the weighted average of scores on all submitted work; the relative weighting and letter grade assignments will depend entirely on the score distribution of the class as a whole and as such reflect each student’s performance relative to their peers.\n\n\nSchedule\nThe tentative topic and assignment schedule is given below. Assignments are indicated by due date: all assignments are due by Monday 11:59pm in the week indicated. Late submissions are allowed, with a possible penalty, for up to 48 hours.\nThe schedule is subject to change based on the progress of the class.\n\n\n\nWeek\nTopic\nLab\nHomework\nProject\n\n\n\n\n1\nData science life cycle\n\n\n\n\n\n2\nTidy data\nL0\n\n\n\n\n3\nSampling and bias\nL1\n\n\n\n\n4\nStatistical graphics\nL2\nH1\n\n\n\n5\nKernel density estimation\nL3\n\nMP1\n\n\n6\nPrincipal components\nL4\nH2\n\n\n\n7\nSimple regression\n\n\nMP2\n\n\n8\nMultiple regression\nL5\nH3\n\n\n\n9\nClassification and clustering\n\n\nCP1\n\n\n10\nCase study\n\nH4\n\n\n\n11\nFinals\n\n\nCP2\n\n\n\n\nL: lab\nH: homework\nMP: mini project\nCP: course project\n\n\n\nMaterials\nThe course website ruizt.github.io/pstat100 will link to all course content and resources. Readings for the course will draw on multiple sources, including:\n\nPython Data Science Handbook (PDSH);\nLearning Data Science (LDS);\ncollected articles distributed as assigned.\n\n\n\nSoftware\nComputing will be hosted via the course LSIT server pstat100.lsit.ucsb.edu. Students need only a web browser and stable internet connection to complete all course work. It is strongly recommended that students download backup copies of their assignments from the LSIT server.\nInterested students are encouraged to install the software needed to open, edit, and execute notebooks on their own machine, in particular:\n\na Python install;\n(recommended) Miniconda\nJupyter;\npackages utilized in course materials (primarily numpy, pandas, altair, and scikit-learn).\n\nManaging package installations will require some (straightforward) use of the package installer pip or pip3 in the terminal to retrieve/install packages from the Python Package Index repository. Documentation for specific packages (or a Google search) will indicate the appropriate pip command."
  },
  {
    "objectID": "about.html#policies",
    "href": "about.html#policies",
    "title": "Course syllabus",
    "section": "Policies",
    "text": "Policies\n\nCommunication\nThere are two primary means of communication outside of scheduled class meetings: office hours and a discussion board.\nCourse staff have limited availability via email. Course staff will make every effort to respond to individual communication within 48 weekday-hours on the following (or similar) matters:\n\naccommodations/extensions due to personal circumstances;\nlogistical issues such as access to materials or missing scores;\ngeneral advising.\n\nEmail should not be used to ask content questions or submit assignments (unless specifically requested). Emails related to the following (or similar) matters may not receive replies and should be redirected:\n\n\n\nTopic\nRedirect to…\n\n\n\n\nTroubleshooting codes\nDiscussion board\n\n\nChecking answers\nOffice hours or discussion board\n\n\nClarifying assignment content\nOffice hours or discussion board\n\n\nAssignment submission\nGradescope\n\n\nRe-evaluation request\nGradescope\n\n\n\n\n\nExpected time commitment\nThe course is 4 credit units; each credit unit corresponds to an approximate time commitment of 3 hours. So, students should expect to allocate 12 hours per week to the course on average. Course staff are available to help any students spending considerably more time on the class balance the workload.\n\n\nScores and grades\nScores on submitted work can be monitored on Gradescope to ensure fair assignment of course grades. On any individual assignment, re-evaluation can be requested within one week of receiving a score. Requests for re-evaluation made beyond one week after publication of scores may or may not be considered on a discretionary basis.\nDetermination of letter grade assignments is made entirely at the discretion of the instructor based on the assessments outlined above and consistent with university policy. Students are not permitted to negotiate their grades, and are discouraged from requesting audits, recalculations, or verification of self-calculations after the course has concluded. The instructor is under no obligation to share the details of grade calculations with students or to respond to such requests.\nIf at the end of the course a student believes their grade was unfairly assigned, either due to discrimination or without basis in coursework, they are entitled to contest it according to the procedure outlined here.\n\n\nConduct\nStudents are expected to uphold the student code of conduct and to maintain integrity. All individually-submitted work must be an honest reflection of individual effort. Evidence of dishonest conduct will be discussed with the student(s) involved and reported to the Office of Student Conduct (OSC). Depending on the nature of the evidence and the violation, penalty in the course may range from a warning to loss of credit to automatic failure. For a definition and examples of dishonesty, a discussion of what constitutes an appropriate response from faculty, and an explanation of the reporting and investigation process, see the OSC page on academic integrity.\n\n\nDeadlines and late work\nThere is a one-hour grace period on all submission deadlines. After that, work may be submitted within 48 hours of the original deadline (not the deadline plus grace period) and will be considered late. Every student can submit two late assignments without penalty. Subsequent late submissions will be evaluated for 75% credit.\n\n\nAccommodations\nReasonable accommodations will be made for any student with a qualifying disability. Such requests should be made through the Disabled Students Program (DSP). More information, instructions on how to access accommodations, and information on related resources can be found on the DSP website.\n\n\nFeedback\nToward the end of the term students will be given an opportunity to provide feedback about the course via ESCI. This feedback is valuable for improvement of the course in future terms, and students are strongly encouraged to provide thoughtful course evaluations. The identities of student respondents to ESCI surveys are not disclosed to instructors."
  },
  {
    "objectID": "activities/week3-activity-miller.html",
    "href": "activities/week3-activity-miller.html",
    "title": "PSTAT100",
    "section": "",
    "text": "This notebook details the voter survey simulation presented in lecture as part of the Miller case study. Recall that the point of the simulation is to show that, under some assumptions about the sampling design and missing data mechanism, a strongly biased result is expected even when the actual rate of erroneous/fraudulent mail ballot requests is very low.\nFor this activity you’ll get to tinker with the simulation settings to better understand the example and the factors that might impact bias in the results.\n\nimport numpy as np\nimport pandas as pd\n\nThe cell below simulates a hypothetical population of 150,000 voters who were issued mail ballots according to state records.\nThe quantity true_prop is the population parameter we will ultimately estimate; this parameter is the proportion of the voters who were issued mail ballots according to state records but who did not request mail ballots. For these voters, either the mail ballots were issued erroneously or they were fraudulently requested. In context, a large estimate for this quantity is suggestive of some irregularities pertaining to the mail-in vote.\nBelow this parameter of interest is set at \\(0.5\\%\\).\nBased on true_prop, an indicator is assigned to each voter that is a 1 if the voter requested the mail ballot and a 0 otherwise; for simplicity, all zeroes are added to the top \\(N\\times\\)true_prop rows and the remaining rows are assigned ones.\n\n# for reproducibility\nnp.random.seed(41021)\n\n# proportion of fraud/error\ntrue_prop = 0.005\n\n# generate population of voters\nN = 150000\npopulation = pd.DataFrame(data = {'requested': np.ones(N)})\n\n# add a label indicating whether the voter requested a mail ballot\nnum_nrequest = round(N*true_prop) - 1\npopulation.iloc[0:num_nrequest, 0] = 0"
  },
  {
    "objectID": "activities/week3-activity-miller.html#simulating-sampling-mechanisms",
    "href": "activities/week3-activity-miller.html#simulating-sampling-mechanisms",
    "title": "PSTAT100",
    "section": "Simulating sampling mechanisms",
    "text": "Simulating sampling mechanisms\nThe cell below assigns sampling weights that represent the probability a voter in the population answers the phone and agrees to an interview.\nThe weights can be thought of as expected conditional response rates – the probabilities that (a) a voter is interviewed given they did request a mail ballot and (b) a voter is interviewed given that they did not request a mail ballot.\nThe assumption figuring in the weight calculation is that voters who did not request mail ballots are more likely to agree to an interview. This would naturally occur if the interviewer is not careful about the interview request and discloses immediately that they are investigating irregularities in mail ballot requests – those who didn’t experience any irregularities are much more likely to hang up or decline.\nCurrently, it is assumed that voters who did not request ballots are 10 times more likely to talk than those who did. This factor is stored as talk_factor. The overall response rate is set at \\(5\\%\\) and stored as p_talk. The weight calculation proceeds using the law of total probability:\n\\[\nP(T) = P(T|R)\\left(P(R) + \\underbrace{\\frac{P(T|NR)}{P(T|R)}}_{\\text{talk factor}}P(NR)\\right)\n\\]\nRearrangement yields an expression for \\(P(T|R)\\) in terms of the request rates, overall response rate, and talking factor.\n\n# probability that a randomly chosen voter requested a mail ballot\np_request = 1 - true_prop\n\n# probability that a randomly chosen voter did not request a mail ballot\np_nrequest = true_prop\n\n# assume respondents who did not request are more likely to talk by this factor\ntalk_factor = 10\n\n# overall response rate\np_talk = 0.05\n\n# conditional response rates \np_talk_request = p_talk/(p_request + talk_factor*p_nrequest) \np_talk_nrequest = talk_factor*p_talk_request\n\n# print\nprint('rate for requesters: ', p_talk_request)\nprint('rate for non-requesters: ', p_talk_nrequest)\n\nrate for requesters:  0.04784688995215312\nrate for non-requesters:  0.47846889952153115\n\n\nIf you like, feel free to adjust the overall response rate and talking factor to values that interest you empirically. The question to ask to determine these values is:\n\nIf I assume the response rate is \\(x\\) and that those who did not request mail ballots are \\(y\\) times more likely to talk, how much bias will that induce for the estimated proportion of erroneous/fraudulent requests?\n\nChoose values for \\(x\\) and \\(y\\) for which you’d like to know the answer. Make sure the conditional rates are valid probabilities. The cell below will draw a sample for your specifications.\n\n# sample size\nn = 2500\n\n# draw sample weighted by conditional probabilities\nnp.random.seed(41923)\npopulation.loc[population.requested == 1, 'sample_weight'] = p_talk_request\npopulation.loc[population.requested == 0, 'sample_weight'] = p_talk_nrequest\nsamp = population.sample(n = n, replace = False, weights = 'sample_weight')\n\nThe cell below returns the estimated proportion of erroneous/fraudulent requests and the error associated with this estimate.\n\n\nprint('estimated fraudulent/erroneous requests: ', 1 - samp.requested.mean())\nprint('true value: ', true_prop)\nprint('estimation error: ', 1 - samp.requested.mean() - true_prop)\n\nestimated fraudulent/erroneous requests:  0.04400000000000004\ntrue value:  0.005\nestimation error:  0.03900000000000004\n\n\nExtrapolating this estimate to a raw vote count among the population yields the following:\n\n\nprint('estimated fraudulent/erroneous requests: ', \n      np.round(N*(1 - samp.requested.mean())))\nprint('true value: ', N*true_prop)\nprint('estimation error: ', \n      np.round(N*(1 - samp.requested.mean() - true_prop)))\n\nestimated fraudulent/erroneous requests:  6600.0\ntrue value:  750.0\nestimation error:  5850.0\n\n\nThe bias – average error across samples – can be estimated by repeating this sampling scheme many times. The cell below computes estimates for nsim simulated samples.\n\n# for reproducibility\nnp.random.seed(41923)\n\n# number of simulated samples\nnsim = 1000\n\n# storage for the estimates from each sample\nestimates = np.zeros(nsim)\n\n# for each simulation ...\nfor i in range(0, nsim):\n    # draw a sample and compute the estimated proportion\n    estimates[i] = population.sample(n = n, \n                                 replace = False, \n                                 weights = 'sample_weight'\n                                 ).requested.mean()\n\nThe average error for this sampling design is given below.\n\nprint('average estimate: ', np.mean((1 - estimates)))\nprint('standard deviation of estimates: ', np.std(estimates))\nprint('truth: ', true_prop)\nprint('bias (proportion): ', np.mean((1 - estimates) - true_prop))\nprint('bias (count): ', np.mean(N*((1 - estimates) - true_prop)))\n\naverage estimate:  0.04465079999999999\nstandard deviation of estimates:  0.0037278169697558933\ntruth:  0.005\nbias (proportion):  0.0396508\nbias (count):  5947.62\n\n\n\nActivity 1: tinker with the sampling mechanism\nTake note of these results or make a duplicate of the cell and re-run it so that you have a copy for later reference. Now go back and adjust the settings. Repeat the simulation and compare changes.\nSome questions you could explore are:\n\nhow does increasing the overall response rate impact the bias?\ndoes sample size matter?\nwhat response rate(s) and talking factor(s) would produce estimates of 10% or more?"
  },
  {
    "objectID": "activities/week3-activity-miller.html#simulating-missingness",
    "href": "activities/week3-activity-miller.html#simulating-missingness",
    "title": "PSTAT100",
    "section": "Simulating missingness",
    "text": "Simulating missingness\nSome interviews were terminated early because the respondent hung up or declined to proceed. We can think of these instances as missing values.\nThe cell below creates missingness probabilities under the assumption that those who did request mail ballots are more likely to terminate interviews than those who did not. The calculation is exactly the same as that used to figure sampling weights.\n\n# assume requesters are more likely to terminate early by this factor\nmissing_factor = 12\n\n# overall observed missing rate\np_missing = 0.5\n\n# proportions of requesters/nonrequesters in sample\np_request_samp = samp.requested.mean()\np_nrequest_samp = 1 - p_request_samp\n\n# conditional probabilities of missing given request status\np_missing_nrequest = p_missing/(p_nrequest + missing_factor*p_request) \np_missing_request = missing_factor*p_missing_nrequest\n\nprint('missing rate for requesters: ', p_missing_request)\nprint('missing rate for nonrequesters: ', p_missing_nrequest)\n\nmissing rate for requesters:  0.502302218501465\nmissing rate for nonrequesters:  0.04185851820845542\n\n\nThe following cell inputs missing values at random according to the missingness mechanism specified above.\n\n# append missingness probabilities\nsamp.loc[samp.requested == 1, 'missing_weight'] = p_missing_request\nsamp.loc[samp.requested == 0, 'missing_weight'] = p_missing_nrequest\n\n# make a copy of the sample\nsamp_incomplete = samp.copy()\n\n# input missing values at random\nnp.random.seed(41923)\nsamp_incomplete['missing'] = np.random.binomial(n = 1, p = samp_incomplete.missing_weight.values)\nsamp_incomplete.loc[samp_incomplete.missing == 1, 'requested'] = float('nan')\n\nFinally, ignoring these missing responses yields the estimate below of the proportion of erroneous/fraudulent ballot requests.\n\n\nprint('estimated fraudulent/erroneous requests: ', \n      1 - samp_incomplete.requested.mean())\nprint('true value: ', true_prop)\nprint('estimation error: ', \n      1 - samp_incomplete.requested.mean() - true_prop)\n\nestimated fraudulent/erroneous requests:  0.08307453416149069\ntrue value:  0.005\nestimation error:  0.07807453416149068\n\n\nExtrapolating this estimate to raw vote counts among the population yields the following:\n\n\nprint('estimated fraudulent/erroneous requests: ', \n      np.round(N*(1 - samp_incomplete.requested.mean())))\nprint('true value: ', N*true_prop)\nprint('estimation error: ', \n      np.round(N*(1 - samp_incomplete.requested.mean() - true_prop)))\n\nestimated fraudulent/erroneous requests:  12461.0\ntrue value:  750.0\nestimation error:  11711.0\n\n\nRepeating the entire experiment – sampling from the population and then introducing missing values – many times will allow for an assessment of the additional bias due to missingness.\n\n# for reproducibility\nnp.random.seed(41923)\n\n# number of simulated samples\nnsim = 1000\n\n# storage for estimates\nestimates = np.zeros([1000, 2])\n\n# for each simulation\nfor i in range(0, nsim):\n    # draw sample from population\n    samp_complete = population.sample(n = 2500, \n                                 replace = False, \n                                 weights = 'sample_weight'\n                                 )\n    \n    # compute mean from complete data\n    estimates[i, 0] = samp_complete.requested.mean()\n    \n    # introduce missing values\n    samp_complete.loc[samp_complete.requested == 1, 'missing_weight'] = p_missing_request\n    samp_complete.loc[samp_complete.requested == 0, 'missing_weight'] = p_missing_nrequest\n\n    # make a copy of the sample\n    samp_incomplete = samp.copy()\n\n    # input missing values at random\n    samp_incomplete['missing'] = np.random.binomial(n = 1, p = samp_incomplete.missing_weight.values)\n    samp_incomplete.loc[samp_incomplete.missing == 1, 'requested'] = float('nan')\n    estimates[i, 1] = samp_incomplete.requested.mean()\n\nNote that both an estimate with complete data (no missing values) and with incomplete data (with missing values that are dropped) are computed. This allows us to compute average errors with and without missingness, and thus, average excess error due to missingness.\n\navg_estimates = 1 - np.mean(estimates, axis = 0)\n\nprint('average estimate without missingness: ', avg_estimates[0])\nprint('average estimate with missingness: ', avg_estimates[1])\nprint('total bias: ', avg_estimates[1] - true_prop)\nprint('bias due to sampling: ', avg_estimates[0] - true_prop)\nprint('excess bias due to missingness: ', avg_estimates[1] - avg_estimates[0])\n\n\naverage estimate without missingness:  0.04469400000000001\naverage estimate with missingness:  0.08155802934699397\ntotal bias:  0.07655802934699396\nbias due to sampling:  0.039694000000000014\nexcess bias due to missingness:  0.036864029346993954\n\n\nIn terms of raw vote counts, these same quantities are:\n\nprint('average estimate without missingness: ', N*avg_estimates[0])\nprint('average estimate with missingness: ', N*avg_estimates[1])\nprint('total bias: ', N*(avg_estimates[1] - true_prop))\nprint('bias due to sampling: ', N*(avg_estimates[0] - true_prop))\nprint('excess bias due to missingness: ', N*(avg_estimates[1] - avg_estimates[0]))\n\n\naverage estimate without missingness:  6704.100000000001\naverage estimate with missingness:  12233.704402049094\ntotal bias:  11483.704402049094\nbias due to sampling:  5954.100000000002\nexcess bias due to missingness:  5529.604402049093\n\n\n\nActivity 2: tinker with missingness mechanism\nGo back and adjust the settings for inputting missing values. Choose a missingness factor and overall nonresponse rate that interest you. Some questions you could explore are:\n\nwhat is the effect of a very high nonresponse rate with little differentiation between requesters and nonrequesters?\nare there any missing data mechanisms that would actually reduce bias?\nif the missing mechanism is similar to the sampling mechanism in how it favors nonrequesters, which has the larger effect?"
  },
  {
    "objectID": "activities/week3-activity-miller.html#extra-credit-assignment",
    "href": "activities/week3-activity-miller.html#extra-credit-assignment",
    "title": "PSTAT100",
    "section": "Extra credit assignment",
    "text": "Extra credit assignment\nDesign and carry out a simulation to further explore how bias due to sampling changes as a function of the factor by which respondents who did not request ballots are more likely to be interviewed. Ignore the potential impact of missing values and focus just on the sampling design.\nFix an evenly-spaced grid of values for the talking factor between 1 and 25. For each value, simulate 1000 samples and calculate the estimate of the proportion of fraudulent/erroneous ballot requests for each sample. For each set of 1000 samples, store: (1) the average estimate; (2) the standard deviation of estimates. Plot the estimated bias (average estimate - true proportion) as a function of talking factor, and add uncertainty bands at \\(\\pm 2SD\\). Repeat the entire procedure for overall response rates of \\(10\\%\\), \\(20\\%\\), and \\(30\\%\\).\nPrepare and submit a notebook detailing the simulation study and briefly explaining the results."
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Materials",
    "section": "",
    "text": "Attendance form (fill out once per class meeting, including sections)\nGradescope (for assignment submissions)\nLSIT server (for course computing)"
  },
  {
    "objectID": "content.html#getting-started-checklist",
    "href": "content.html#getting-started-checklist",
    "title": "Materials",
    "section": "Getting started checklist",
    "text": "Getting started checklist\n\nConfirm access to all course pages\nRead syllabus\nFill out intake survey"
  },
  {
    "objectID": "content.html#week-1",
    "href": "content.html#week-1",
    "title": "Materials",
    "section": "Week 1",
    "text": "Week 1\nReadings:\n\nLDS1 The Data Science Lifecycle\nLDS5 Case Study: Why is my Bus Always Late?\nPDSH2.1 Understanding data types in python\nPDSH2.2 The basics of numpy arrays\nPDSH2.4 Aggregations: min, max, and everything in between\n\nMonday: Course introduction [slides]\nLab sections: Orientation to Jupyter notebooks [html] [notebook] [solutions]\nWednesday: Data science lifecycle [slides]"
  },
  {
    "objectID": "content.html#week-2",
    "href": "content.html#week-2",
    "title": "Materials",
    "section": "Week 2",
    "text": "Week 2\nReadings:\n\nWickham (2014). Tidy data. Journal of statistical software 59(10). [link to paper]\nPDSH3.1 Introducing pandas objects\nPDSH3.2 Data indexing and selection\nPDSH3.7 Merge and join\nPDSH3.8 Aggregation and grouping\n\nAssignments:\n\nHW1, BRFSS case study, due Monday, April 24 [html] [notebook] [solutions]\n\nMonday: Tidy data [slides]\nLab sections: Pandas [html] [notebook] [solutions]\nWednesday: Dataframe transformations [slides]"
  },
  {
    "objectID": "content.html#week-3",
    "href": "content.html#week-3",
    "title": "Materials",
    "section": "Week 3",
    "text": "Week 3\nReadings:\n\nLDS2.2 Population, frame, sample\nVan Buuren, Flexible Imputation of Missing Data, section 2.2 Concepts in incomplete data\nPDSH3.4 Handling missing data\n\nAssignments:\n\nMini project 1, due Monday, May 1 [html] [notebook]\n\nMonday: Sampling, bias, and missingness [slides]\nLab sections: Exploring sampling bias through simulation [html] [notebook] [solutions]\nWednesday: Voter fraud case study [slides] [activity html] [activity notebook]"
  },
  {
    "objectID": "content.html#week-4",
    "href": "content.html#week-4",
    "title": "Materials",
    "section": "Week 4",
    "text": "Week 4\nReadings:\n\nWilke, Fundamentals of Data Visualization Ch. 2-5\nLDS11.1 Choosing scale to reveal structure\n(Recommended) Cook, D., Lee, E. K., & Majumder, M. (2016). Data visualization and statistical graphics in big data analysis. Annual Review of Statistics and Its Application, 3, 133-159. [link to paper]\n(Recommended) Gelman, A., & Unwin, A. (2013). Infovis and statistical graphics: different goals, different looks. Journal of Computational and Graphical Statistics, 22(1), 2-28. [link to paper]\n(Recommended) Iliinsky, N. (2010). On beauty. Beautiful visualization: Looking at data through the eyes of experts, 1-13. [link to chapter]\n\nAssignments:\n\nHW2, SEDA case study, due Monday, May 8 [html] [notebook]\n\nMonday: Statistical graphics [slides]\nLab sections: Data visualization [html] [notebook] [solutions]\nWednesday: Principles of figure design [slides]"
  },
  {
    "objectID": "hw/hw1-brfss/hw1-brfss-soln.html",
    "href": "hw/hw1-brfss/hw1-brfss-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport altair as alt\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\n\n\nBackground\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a long-term effort administered by the CDC to collect data on behaviors affecting physical and mental health, past and present health conditions, and access to healthcare among U.S. residents. The BRFSS comprises telephone surveys of U.S. residents conducted annually since 1984; in the last decade, over half a million interviews have been conducted each year. This is the largest such data collection effort in the world, and many countries have developed similar programs. The objective of the program is to support monitoring and analysis of factors influencing public health in the United States.\nEach year, a standard survey questionnaire is developed that includes a core component comprising questions about: demographic and household information; health-related perceptions, conditions, and behaviors; substance use; and diet. Trained interviewers in each state call randomly selected telephone (landline and cell) numbers and administer the questionnaire; the phone numbers are chosen so as to obtain a representative sample of all households with telephone numbers. Take a moment to read about the 2019 survey here.\nIn this assignment you’ll import and subsample the BRFSS 2019 data and perform a simple descriptive analysis exploring associations between adverse childhood experiences, health perceptions, tobacco use, and depressive disorders. This is an opportunity to practice:\n\nreview of data documentation\ndata assessment and critical thinking about data collection\ndataframe transformations in pandas\ncommunicating and interpreting grouped summaries\n\n\n\nData import and assessment\nThe cell below imports select columns from the 2019 dataset as a pandas DataFrame. The file is big, so this may take a few moments. Run the cell and then have a quick look at the first few rows and columns.\n\n# store variable names of interest\nselected_vars = ['_SEX', '_AGEG5YR', \n                 'GENHLTH', 'ACEPRISN', \n                 'ACEDRUGS', 'ACEDRINK', \n                 'ACEDEPRS', 'ADDEPEV3', \n                 '_SMOKER3', '_LLCPWT']\n\n# import full 2019 BRFSS dataset\nbrfss = pd.read_csv('data/brfss2019.zip', compression = 'zip', usecols = selected_vars)\n\n# invert sampling weights\nbrfss['_LLCPWT'] = 1/brfss._LLCPWT\n\n# print first few rows\nbrfss.head()\n\n\n\n\n\n  \n    \n      \n      GENHLTH\n      ADDEPEV3\n      ACEDEPRS\n      ACEDRINK\n      ACEDRUGS\n      ACEPRISN\n      _LLCPWT\n      _SEX\n      _AGEG5YR\n      _SMOKER3\n    \n  \n  \n    \n      0\n      3.0\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.007391\n      2.0\n      13.0\n      3.0\n    \n    \n      1\n      4.0\n      2.0\n      2.0\n      1.0\n      2.0\n      2.0\n      0.000687\n      2.0\n      11.0\n      4.0\n    \n    \n      2\n      3.0\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.004639\n      2.0\n      10.0\n      4.0\n    \n    \n      3\n      4.0\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.003827\n      2.0\n      13.0\n      9.0\n    \n    \n      4\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.001868\n      2.0\n      13.0\n      3.0\n    \n  \n\n\n\n\n\nQuestion 1: Data dimensions\nCheck the dimensions of the dataset. Store the dimensions as nrows and ncolumns.\n\nnrows, ncolumns = brfss.shape # SOLUTION\n\nprint(nrows, ncolumns)\n\n418268 10\n\n\n\ngrader.check(\"q1\")\n\n\n\n\nQuestion 2: Row and column information\nNow that you’ve imported the data, you should verify that the dimensions conform to the format you expect based on data documentation and ensure you understand what each row and each column represents.\nCheck the number of records (interviews conducted) reported and variables measured for 2019 by reviewing the surveillance summaries by year, and then answer the following questions in a few sentences:\n\nDoes the number of rows match the number of reported records?\nHow many columns were imported, and how many columns are reported in the full dataset?\nWhat does each row in the brfss dataframe represent?\nWhat does each column in the brfss dataframe represent\n\nType your answer here, replacing this text.\nSOLUTION:\nYes, there are exactly as many rows as reported records. The documentation reports 342 variables measured; we only imported 10 of those variables. Each row corresponds to a respondent, and each column corresponds to a respondent attribute or answer to one of the survey questions.\n\n\n\n\nQuestion 3: Sampling design and data collection\nSkim the overview documentation for the 2019 BRFSS data. Focus specifically the ‘Background’ and ‘Data Collection’ sections, read selectively for relevant details, and answer the following questions in a few sentences:\n\nWho conducts the interviews and how long does a typical interview last?\nWho does an interviewer speak to in each household?\nWhat criteria must a person meet to be interviewed?\nWho can’t appear in the survey? Give two examples.\nWhat is the study population (i.e., all individuals who could possibly be sampled)?\nDoes the data contain any identifying information?\n\nType your answer here, replacing this text.\nSOLUTION: State healthcare personell or trained contractors conduct interveiews, and they generally last 17-27 minutes. After speaking with whoever answers the phone, the interviewer determines a randomly selected adult in the household to survey. The respondent must be over 18, live in a private residence or college housing, and have a working phone. Anyone not meeting these criteria cannot participate, such as: anyone under 18; anyone living in residential care facilities or prisons; anyone without a permanent home. The study population is all adult U.S. residents with working phones and living in private or college housing. The data are de-identified and none of the variables allow for easy reconstruction of a respondent’s identity.\n\n\n\n\nQuestion 4: Variable descriptions\nYou’ll work with the small subset variables imported above: sex, age, general health self-assessment, smoking status, depressive disorder, and adverse childhood experiences (ACEs). The names of these variables as they appear in the raw dataset are defined in the cell in which you imported the data as selected_vars. It is often useful, and therefore good practice, to include a brief description of each variable at the outset of any reported analyses, both for your own clarity and for that of any potential readers. Open the 2019 BRFSS codebook in your browser and use text searching to locate each of the variable names of interest. Read the codebook entries and fill in the second column in the table below with a one-sentence description of each variable identified in selected_vars. Rephrase the descriptions in your own words – do not copy the codebook descriptions verbatim.\n\n\n\nVariable name\nDescription\n\n\n\n\nGENHLTH\n\n\n\n_SEX\n\n\n\n_AGEG5YR\n\n\n\nACEPRISN\n\n\n\nACEDRUGS\n\n\n\nACEDRINK\n\n\n\nACEDEPRS\n\n\n\nADDEPEV3\n\n\n\n_SMOKER3\n\n\n\n\nSOLUTION\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\nGENHLTH\nSelf-rated general health\n\n\n_SEX\nRespondent’s sex\n\n\n_AGEG5YR\nAge bracket in 5-year intervals\n\n\nACEPRISN\nLived with anyone who served prison time or was in prison?\n\n\nACEDRUGS\nLived with anyone abusing substances?\n\n\nACEDRINK\nLived with a problem drinker or alcoholic?\n\n\nACEDEPRS\nLived with anyone depressed, mentally ill, or suicidal?\n\n\nADDEPEV3\nEver diagnosed with a depressive disorder?\n\n\n_SMOKER3\nSmoking status\n\n\n\n\n\n\n\nSubsampling\nTo simplify life a little, we’ll draw a large random sample of the rows and work with that in place of the full dataset. This is known as subsampling.\nThe cell below draws a random subsample of 10k records. Because the subsample is randomly drawn, we should not expect it to vary in any systematic way from the overall dataset, and distinct subsamples should have similar properties – therefore, results downstream should be similar to an analysis of the full dataset, and should also be possible to replicate using distinct subsamples.\n\n# for reproducibility\nnp.random.seed(32221) \n\n# randomly sample 10k records\nsamp = brfss.sample(n = 10000, \n                    replace = False, \n                    weights = '_LLCPWT')\n\nAsides:\n\nNotice that the random number generator seed is set before carrying out this task – this ensures that every time the cell is run, the same subsample is drawn. As a result, the computations in this notebook are reproducible: when I run the notebook on my computer, I get the same results as you get when you run the notebook on your computer.\nNotice also that sampling weights provided with the dataset are used to draw a weighted sample. Some respondents are more likely to be selected than others from the general population of U.S. adults with phone numbers, so the BRFSS calculates derived weights that are inversely proportional to estimates of the probability that the respondent is included in the survey. This is a somewhat sophisticated calculation, however if you’re interested, you can read about how these weights are calculated and why in the overview documentation you used to answer the questions above. We use the sampling weights in drawing the subsample so that we get a representative sample of U.S. adults with phone numbers.\nNotice the missing values. How many entries are missing in each column? The cell below computes the proportion of missing values for each of the selected variables. We’ll return to this issue later on.\n\n\n# proportions of missingness \nsamp.isna().mean()\n\nGENHLTH     0.0000\nADDEPEV3    0.0000\nACEDEPRS    0.8086\nACEDRINK    0.8088\nACEDRUGS    0.8088\nACEPRISN    0.8088\n_LLCPWT     0.0000\n_SEX        0.0000\n_AGEG5YR    0.0000\n_SMOKER3    0.0000\ndtype: float64\n\n\n\n\nTidying\nIn the following series of questions you’ll tidy up the subsample by performing these steps:\n\nselecting columns of interest;\nreplacing coded values of question responses with responses;\ndefining new variables based on existing ones;\nrenaming columns.\n\nThe goal of this is to produce a clean version of the dataset that is well-organized, intuitive to navigate, and ready for analysis.\nThe variable entries are coded numerically to represent certain responses. These should be replaced by more informative entries. We can use the codebook to determine which number means what, and replace the values accordingly.\nThe cell below replaces the numeric values for _AGEG5YR by their meanings, illustrating how to use .replace() with a dictionary to convert the numeric coding to interpretable values. The basic strategy is:\n\nStore the variable coding for VAR as a dictionary var_codes.\nUse .replace({'VAR': var_codes}) to modify values.\n\nIf you need additional examples, check the pandas documentation for .replace().\n\n# dictionary representing variable coding\nage_codes = {\n    1: '18-24', 2: '25-29', 3: '30-34',\n    4: '35-39', 5: '40-44', 6: '45-49',\n    7: '50-54', 8: '55-59', 9: '60-64',\n    10: '65-69', 11: '70-74', 12: '75-79',\n    13: '80+', 14: 'Unsure/refused/missing'\n}\n\n# recode age categories\nsamp_mod1 = samp.replace({'_AGEG5YR': age_codes})\n\n# check result\nsamp_mod1.head()\n\n\n\n\n\n  \n    \n      \n      GENHLTH\n      ADDEPEV3\n      ACEDEPRS\n      ACEDRINK\n      ACEDRUGS\n      ACEPRISN\n      _LLCPWT\n      _SEX\n      _AGEG5YR\n      _SMOKER3\n    \n  \n  \n    \n      237125\n      5.0\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.057004\n      2.0\n      25-29\n      3.0\n    \n    \n      329116\n      5.0\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.108336\n      2.0\n      80+\n      3.0\n    \n    \n      178937\n      3.0\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.000998\n      1.0\n      18-24\n      4.0\n    \n    \n      410081\n      4.0\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.021973\n      2.0\n      45-49\n      2.0\n    \n    \n      184555\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.027175\n      2.0\n      80+\n      3.0\n    \n  \n\n\n\n\n\nQuestion 5: Recoding variables\nFollowing the example immediately above and referring to the 2019 BRFSS codebook, replace the numeric codings with response categories for each of the following variables:\n\n_SEX\nGENHLTH\n_SMOKER3\n\nNotice that above, the first modification (slicing) was stored as samp_mod1, and was a function of samp. You’ll follow this pattern, creating samp_mod2, samp_mod3, and so on so that each step (modification) of your data manipulations is stored separately, for easy troubleshooting.\n\nRecode _SEX: define a new dataframe samp_mod2 that is the same as samp_mod1 but with the _SEX variable recoded as M and F.\nRecode GENHLTH: define a new dataframe samp_mod3 that is the same as samp_mod2 but with the GENHLTH variable recoded as Excellent, Very good, Good, Fair, Poor, Unsure, and Refused.\nRecode _SMOKER3: define a new dataframe samp_mod4 that is the same as samp_mod3 but with _SMOKER3 recoded as Daily, Some days, Former, Never, and Unsure/refused/missing.\nPrint the first few rows of samp_mod4.\n\n\n# define dictionary for sex\nsex_codes = {1: 'M', 2: 'F'} # SOLUTION\n\n# recode sex\nsamp_mod2 = samp_mod1.replace({'_SEX': sex_codes}) # SOLUTION\n\n# define dictionary for health\nhealth_codes = { 1: 'Excellent', 2: 'Very good', 3: 'Good', 4: 'Fair', 5: 'Poor', 7: 'Unsure', 9: 'Refused'} # SOLUTION\n\n# recode health\nsamp_mod3 = samp_mod2.replace({'GENHLTH': health_codes}) # SOLUTION\n\n# define dictionary for smoking\nsmoke_codes = { 1: 'Daily', 2: 'Some days', 3: 'Former', 4: 'Never', 9: 'Unsure/refused/missing'} # SOLUTION\n\n# recode smoking\nsamp_mod4 = samp_mod3.replace({'_SMOKER3': smoke_codes}) # SOLUTION\n\n# print a few rows\nsamp_mod4.head() # SOLUTION\n\n\n\n\n\n  \n    \n      \n      GENHLTH\n      ADDEPEV3\n      ACEDEPRS\n      ACEDRINK\n      ACEDRUGS\n      ACEPRISN\n      _LLCPWT\n      _SEX\n      _AGEG5YR\n      _SMOKER3\n    \n  \n  \n    \n      237125\n      Poor\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.057004\n      F\n      25-29\n      Former\n    \n    \n      329116\n      Poor\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.108336\n      F\n      80+\n      Former\n    \n    \n      178937\n      Good\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.000998\n      M\n      18-24\n      Never\n    \n    \n      410081\n      Fair\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.021973\n      F\n      45-49\n      Some days\n    \n    \n      184555\n      Very good\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.027175\n      F\n      80+\n      Former\n    \n  \n\n\n\n\n\ngrader.check(\"q5\")\n\n\n\nQuestion 6: Value replacement\nNow all the variables except the adverse childhood experience and depressive disorder question responses are represented interpretably. In the codebook that the answer key is identical for these remaining variables.\nThe numeric codings can be replaced all at once by applying .replace() to the dataframe with an argument of the form\n\ndf.replace({'var1': varcodes1, 'var2': varcodes1, ..., 'varp': varcodesp})\n\nDefine a new dataframe samp_mod5 that is the same as samp_mod4 but with the remaining variables recoded according to the answer key Yes, No, Unsure, Refused. Print the first few rows of the result using .head().\n\n# define dictionary\nanswer_codes = {1: 'Yes', 2: 'No', 7: 'Unsure', 9: 'Refused'} #SOLUTION\n\n# recode\nsamp_mod5 =  samp_mod4.replace({'ACEPRISN': answer_codes, 'ACEDRUGS': answer_codes, 'ACEDRINK': answer_codes, 'ACEDEPRS': answer_codes, 'ADDEPEV3': answer_codes}) #SOLUTION\n\n# check using head()\nsamp_mod5.head() #SOLUTION\n\n\n\n\n\n  \n    \n      \n      GENHLTH\n      ADDEPEV3\n      ACEDEPRS\n      ACEDRINK\n      ACEDRUGS\n      ACEPRISN\n      _LLCPWT\n      _SEX\n      _AGEG5YR\n      _SMOKER3\n    \n  \n  \n    \n      237125\n      Poor\n      No\n      NaN\n      NaN\n      NaN\n      NaN\n      0.057004\n      F\n      25-29\n      Former\n    \n    \n      329116\n      Poor\n      No\n      NaN\n      NaN\n      NaN\n      NaN\n      0.108336\n      F\n      80+\n      Former\n    \n    \n      178937\n      Good\n      No\n      NaN\n      NaN\n      NaN\n      NaN\n      0.000998\n      M\n      18-24\n      Never\n    \n    \n      410081\n      Fair\n      Yes\n      NaN\n      NaN\n      NaN\n      NaN\n      0.021973\n      F\n      45-49\n      Some days\n    \n    \n      184555\n      Very good\n      No\n      No\n      No\n      No\n      No\n      0.027175\n      F\n      80+\n      Former\n    \n  \n\n\n\n\n\ngrader.check(\"q6\")\n\nFinally, all the variables in the dataset are categorical. Notice that the current data types do not reflect this.\n\nsamp_mod5.dtypes\n\nGENHLTH      object\nADDEPEV3     object\nACEDEPRS     object\nACEDRINK     object\nACEDRUGS     object\nACEPRISN     object\n_LLCPWT     float64\n_SEX         object\n_AGEG5YR     object\n_SMOKER3     object\ndtype: object\n\n\nLet’s coerce the variables to category data types using .astype().\n\n# coerce to categorical\nsamp_mod6 = samp_mod5.astype('category')\n\n# check new data types\nsamp_mod6.dtypes\n\nGENHLTH     category\nADDEPEV3    category\nACEDEPRS    category\nACEDRINK    category\nACEDRUGS    category\nACEPRISN    category\n_LLCPWT     category\n_SEX        category\n_AGEG5YR    category\n_SMOKER3    category\ndtype: object\n\n\n\n\nQuestion 7: Define ACE indicator variable\nDownstream analysis of ACEs will be facilitated by having an indicator variable that is a 1 if the respondent answered ‘Yes’ to any ACE question, and a 0 otherwise – that way, you can easily count the number of respondents reporting ACEs by summing up the indicator or compute the proportion by taking an average.\nTo this end, define a new logical variable:\n\nadverse_conditions: did the respondent answer yes to any of the adverse childhood condition questions?\n\nYou can accomplish this task in several steps:\n\nObtain a logical array indicating the positions of the ACE variables (hint: use .columns to obtain the column index and operate on the result with .str.startswith(...).). Store this as ace_positions.\nUse the logical array ace_positions to select the ACE columns via .loc[]. Store this as ace_data.\nObtain a dataframe that indicates whether each entry is a ‘Yes’ (hint: use the boolean operator ==, which is a vectorized operation). Store this as ace_yes.\nCompute the row sums using .sum(). Store this as ace_numyes.\nDefine the new variable as ace_numyes > 0.\n\nStore the result as samp_mod7, and print the first few rows using .head().\n\n# BEGIN SOLUTION NO PROMPT\n# copy samp_mod6\nsamp_mod7 = samp_mod6.copy()\n\n# ace column positions\nace_positions = samp_mod7.columns.str.startswith('ACE')\n\n# ace data\nace_data = samp_mod7.loc[:, ace_positions]\n\n# ace yes indicators\nace_yes = (ace_data == 'Yes')\n\n# number of yesses\nace_numyes = ace_yes.sum(axis = 1)\n\n# assign new variable\nsamp_mod7['adverse_conditions'] = (ace_numyes > 0)\n\n# check result\nsamp_mod7.head()\n# END SOLUTION\n\n\"\"\" # BEGIN PROMPT\n# copy samp_mod6\nsamp_mod7 = samp_mod6.copy()\n\n# ace column positions\nace_positions = ...\n\n# ace data\nace_data = ...\n\n# ace yes indicators\nace_yes = ...\n\n# number of yesses\nace_numyes = ...\n\n# assign new variable\nsamp_mod7['adverse_conditions'] = ...\n\n# check result using .head()\n...\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q7\")\n\n\n\nQuestion 8: Define missingness indicator variable\nAs you saw earlier, there are some missing values for the ACE questions. These arise whenever a respondent is not asked these questions. In fact, answers are missing for nearly 80% of the respondents in our subsample. We should keep track of this information. Define a missing indicator:\n\nadverse_missing: is a response missing for at least one of the ACE questions?\n\n\n# BEGIN SOLUTION NO PROMPT\n# copy modification 7\nsamp_mod8 = samp_mod7.copy()\n\n# define missing indicator using loc\nsamp_mod8.loc[:, 'adverse_missing'] = samp_mod8.loc[:, samp_mod8.columns.str.startswith('ACE').tolist()].isna().sum(axis = 1) > 0\n\n# check\nsamp_mod8.head()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# copy modification 7\nsamp_mod8 = samp_mod7.copy()\n\n# define missing indicator using loc\n...\n\n# check using head()\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q8\")\n\n\n\nQuestion 9: Filter respondents who did not answer ACE questions\nSince values are missing for the ACE question if a respondent was not asked, we can remove these observations and do any analysis conditional on respondents having been asked the ACE questions. Use your indicator variable adverse_missing to filter out respondents who were not asked the ACE questions.\nNote that this dramatically limits the scope of inference for subsequent analyses to only those locations where the ACE module was included in the survey.\n\nsamp_mod9 = samp_mod8[~samp_mod8.adverse_missing] #SOLUTION\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10: Define depression indicator variable\nIt will prove similarly helpful to define an indicator for reported depression:\n\ndepression: did the respondent report having been diagnosed with a depressive disorder?\n\nFollow the same strategy as above for the ACE variables, and store the result as samp_mod10. See if you can perform the calculation of the new variable in a single line of code. Print the first few rows using .head().\n\n# BEGIN SOLUTION NO PROMPT\n# copy samp_mod9\nsamp_mod10 = samp_mod9.copy()\n\n# define missing indicator using loc\nsamp_mod10['depression'] = ((samp_mod10.loc[:, 'ADDEPEV3'] == 'Yes') > 0)\n\n# check\nsamp_mod10.head()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# copy samp_mod9\nsamp_mod10 = samp_mod9.copy()\n\n# define new variable using loc\n...\n\n# check using .head()\n...\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q10\")\n\n\n\nQuestion 11: Final dataset\nFor the final dataset, drop the respondent answers to individual questions, the missingness indicator, and select just the derived indicator variables along with general health, sex, age, and smoking status. Check the pandas documentation for .rename() and follow the examples to rename the latter variables:\n\ngeneral_health\nsex\nage\nsmoking\n\nSee if you can perform both operations (slicing and renaming) in a single chain. Store the result as data.\n\nsamp_mod10.columns\n\nIndex(['GENHLTH', 'ADDEPEV3', 'ACEDEPRS', 'ACEDRINK', 'ACEDRUGS', 'ACEPRISN',\n       '_LLCPWT', '_SEX', '_AGEG5YR', '_SMOKER3', 'adverse_conditions',\n       'adverse_missing', 'depression'],\n      dtype='object')\n\n\n\n# BEGIN SOLUTION NO PROMPT\n# slice and rename\ndata = samp_mod10.iloc[:, [0, 7, 8, 9, 10, 12]].rename( #dropping some variables is the same as only selecting the remaining variables\n    columns = {'GENHLTH': 'general_health',\n               '_SEX': 'sex',\n               '_AGEG5YR': 'age',\n               '_SMOKER3': 'smoking'}\n)\n\n# preview\ndata.head()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# slice and rename\ndata = ...\n\n# check using .head()\n\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q11\")\n\n\n\n\nDescriptive analysis\nNow that you have a clean dataset, you’ll use grouping and aggregation to compute several summary statistics that will help you explore whether there is an apparent association between experiencing adverse childhood conditions and self-reported health, smoking status, and depressive disorders in areas where the ACE module was administered.\nThe basic strategy will be to calculate the proportions of respondents who answered yes to one of the adverse experience questions when respondents are grouped by the other variables.\n\nQuestion 12: Proportion of respondents reporting ACEs\nCalculate the overall proportion of respondents in the subsample that reported experiencing at least one adverse condition (given that they answered the ACE questions). Use .mean(); store the result as mean_ace and print.\n\n# proportion of respondents reporting at least one adverse condition\nmean_ace = data.adverse_conditions.mean() #SOLUTION\n\n# print\nmean_ace\n\n0.3070083682008368\n\n\n\ngrader.check(\"q12\")\n\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by general health?\nThe cell below computes the porportion separately by general health self-rating. Notice that the depression variable is dropped so that the result doesn’t also report the proportion of respondents reporting having been diagnosed with a depressive disorder. Notice also that the proportion of missing values for respondents indicating each general health rating is shown.\n\n# proportions grouped by general health\ndata.drop(\n    columns = 'depression'\n).groupby(\n    'general_health'\n).mean(numeric_only = True)\n\n\n\n\n\n  \n    \n      \n      adverse_conditions\n    \n    \n      general_health\n      \n    \n  \n  \n    \n      Excellent\n      0.300000\n    \n    \n      Fair\n      0.355491\n    \n    \n      Good\n      0.299174\n    \n    \n      Poor\n      0.441667\n    \n    \n      Refused\n      0.000000\n    \n    \n      Unsure\n      0.000000\n    \n    \n      Very good\n      0.264957\n    \n  \n\n\n\n\nNotice that the row index lists the general health rating out of order. This can be fixed using a .loc[] call and the dictionary that was defined for the variable coding.\n\n# same as above, rearranging index\nace_health = data.drop(\n    columns = 'depression'\n).groupby(\n    'general_health'\n).mean(\n    numeric_only = True\n).loc[list(health_codes.values()), :]\n\n# print\nace_health\n\n\n\n\n\n  \n    \n      \n      adverse_conditions\n    \n    \n      general_health\n      \n    \n  \n  \n    \n      Excellent\n      0.300000\n    \n    \n      Very good\n      0.264957\n    \n    \n      Good\n      0.299174\n    \n    \n      Fair\n      0.355491\n    \n    \n      Poor\n      0.441667\n    \n    \n      Unsure\n      0.000000\n    \n    \n      Refused\n      0.000000\n    \n  \n\n\n\n\n\n\nQuestion 13: Association between smoking status and ACEs\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by smoking status?\nFollowing the example above for computing the proportion of respondents reporting ACEs by general health rating, calculate the proportion of respondents reporting ACEs by smoking status (be sure to arrange the rows in appropriate order of smoking status) and store as ace_smoking.\n\n# proportions grouped by smoking status\nace_smoking = data.drop(\n    columns = 'depression'\n).groupby(\n    'smoking'\n).mean(\n    numeric_only = True\n).loc[list(smoke_codes.values()), :] #SOLUTION\n\n# print\nace_smoking\n\n\n\n\n\n  \n    \n      \n      adverse_conditions\n    \n    \n      smoking\n      \n    \n  \n  \n    \n      Daily\n      0.453125\n    \n    \n      Some days\n      0.527778\n    \n    \n      Former\n      0.334459\n    \n    \n      Never\n      0.251434\n    \n    \n      Unsure/refused/missing\n      0.100000\n    \n  \n\n\n\n\n\ngrader.check(\"q13\")\n\n\n\nQuestion 14: Association between depression and ACEs\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by smoking status?\nCalculate the proportion of respondents reporting ACEs by whether respondents had been diagnosed with a depressive disorder and store as ace_depr.\n\n# proportions grouped by having experienced depression\nace_depr = data.groupby(\n    'depression'\n).mean(\n    numeric_only = True\n) #SOLUTION\n\n# print\nace_depr\n\n\n\n\n\n  \n    \n      \n      adverse_conditions\n    \n    \n      depression\n      \n    \n  \n  \n    \n      False\n      0.250975\n    \n    \n      True\n      0.537433\n    \n  \n\n\n\n\n\ngrader.check(\"q14\")\n\n\n\nQuestion 15: Exploring subgroupings\nDoes the apparent association between general health and ACEs persist after accounting for sex?\nRepeat the calculation of the proportion of respondents reporting ACEs by general health rating, but also group by sex. Store the result as ace_health_sex.\n\n# group by general health and sex\nace_health_sex = data.drop(\n    columns = 'depression'\n).groupby(\n    ['general_health', 'sex']\n).mean(numeric_only = True) #SOLUTION\n\n\ngrader.check(\"q15\")\n\nThe cell below rearranges the table a little for better readability.\n\n# pivot table for better display\nace_health_sex.reset_index().pivot(columns = 'sex', index = 'general_health', values = 'adverse_conditions').loc[list(health_codes.values()), :]\n\n\n\n\n\n  \n    \n      sex\n      F\n      M\n    \n    \n      general_health\n      \n      \n    \n  \n  \n    \n      Excellent\n      0.328671\n      0.261682\n    \n    \n      Very good\n      0.282123\n      0.237885\n    \n    \n      Good\n      0.308108\n      0.285106\n    \n    \n      Fair\n      0.367150\n      0.338129\n    \n    \n      Poor\n      0.549296\n      0.285714\n    \n    \n      Unsure\n      NaN\n      0.000000\n    \n    \n      Refused\n      0.000000\n      NaN\n    \n  \n\n\n\n\nEven after rearrangement, the table in the last question is a little tricky to read (few people like visually scanning tables). This information would be better displayed in a plot. The example below generates a bar chart showing the summaries you calculated in Q2(d), with the proportion on the y axis, the health rating on the x axis, and separate bars for the two sexes.\n\n# coerce indices to columns for plotting\nplot_df = ace_health_sex.reset_index()\n\n# specify order of general health categories\ngenhealth_order = list(health_codes.values())\nplot_df.general_health.cat.set_categories(genhealth_order, inplace=True)\nplot_df.sort_values([\"general_health\"], inplace=True)\n\n# plot\nalt.Chart(plot_df).mark_bar().encode(\n    x = alt.X('general_health', \n              sort = ['general_health'],\n              title = 'Self-rated general health'),\n    y = alt.Y('adverse_conditions',\n              title = 'Prop. of respondents reporting ACEs'),\n    color = 'sex',\n    column = 'sex'\n).properties(\n    width = 200, \n    height = 200\n)\n\nC:\\Users\\trdea\\AppData\\Local\\Temp\\ipykernel_10156\\2150558614.py:6: FutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object.\n  plot_df.general_health.cat.set_categories(genhealth_order, inplace=True)\n\n\n\n\n\n\n\n\n\n\nQuestion 16: Visualization\nUse the example above to plot the proportion of respondents reporting ACEs against smoking status for men and women.\nHint: you only need to modify the example by substituting smoking status for general health.\n\n# BEGIN SOLUTION NO PROMPT\n# proportions grouped by smoking status\nace_smoking_sex = data.drop(\n    columns = 'depression'\n).groupby(\n    ['smoking', 'sex']\n).mean(numeric_only = True).loc[list(smoke_codes.values()), :]\n\n# coerce indices to columns for plotting\nplot_df = ace_smoking_sex.reset_index()\n\n# specify order of general health categories\nsmoke_order = pd.CategoricalDtype(list(smoke_codes.values()), ordered = True)\nplot_df['smoking'] = plot_df.smoking.astype(smoke_order)\n\n# plot\nalt.Chart(plot_df).mark_bar().encode(\n    x = alt.X('smoking', \n              sort = list(health_codes.values()),\n              title = 'Smoking status'),\n    y = alt.Y('adverse_conditions',\n              title = 'Prop. of respondents reporting ACEs'),\n    color = 'sex',\n    column = 'sex'\n).properties(\n    width = 200, \n    height = 200\n)\n# END SOLUTION\n\n\n\n\n\n\n\n# BEGIN PROMPT\n# dataframe of proportions grouped by smoking status\nace_smoking_sex = ...\n\n# coerce indices to columns for plotting\n...\n\n# specify order of general health categories\n...\n\n# plot\n...\n# END PROMPT\n\nEllipsis\n\n\n\n\n\n\nCommunicating results\nHere you’ll be asked to reflect briefly on your findings.\n\n\nQuestion 17: Summary\nIs there an observed association between reporting ACEs and general health, smoking status, and depression among survey respondents who answered the ACE questions?\nWrite a two to three sentence answer to the above question summarizing your findings. State an answer to the question in your first sentence, and then in your second/third sentences describe exactly what you observed in the foregoing descriptive analysis of the BRFSS data. Be precise, but also concise. There is no need to describe any of the data manipulations, survey design, or the like.\nType your answer here, replacing this text.\nSOLUTION\nYes, there are observed associations between reported adverse childhood experiences and general health, smoking status, and depression. The proportion of respondents reporting ACEs generally increases with smoking frequency for both men and women; there are higher observed rates of ACE reports among respondents in poorer health for both men and women; and there are higher observed rates of ACE reports among respondents with a diagnosed depressive disorder.\n\n\n\n\nQuestion 18: Scope of inference\nRecall from the overview documentation all the care that the BRFSS dedicates to collecting a representative sample of the U.S. adult population with phone numbers. Do you think that your findings provide evidence of an association among the general public (not just the individuals survey)? Why or why not? Answer in two sentences.\nType your answer here, replacing this text.\nSOLUTION\nThe sample is a probability sample of the study population, so results are in principle generalizable; however, many ACE responses were missing because certain states did not ask those questions. As a result, the observed proportions are likely underestimates of the rates among the general public (U.S. adults with phone numbers in private or college housing) and may misrepresent the overall pattern of association. More narrowly, the findings do provide evidence of associations between adverse childhood experiences and health, depression, and smoking among a subset of states.\n\n\n\n\nQuestion 19: Bias\nWhat is a potential source of bias in the survey results, and how might this affect the proportions you’ve calculated?\nAnswer in one or two sentences.\nType your answer here, replacing this text.\nSOLUTION\nAdverse childhood experience is a sensitive matter; respondents may not be comfortable responding truthfully to some of these questions. This would likely produce negative bias – the sample proportions may be underestimates if this is common.\n\n\n\nComment\nNotice that the language ‘association’ is non-causual: we don’t say that ACEs cause (or don’t cause) poorer health outcomes. This is intentional, because the BRFSS data are what are known as ‘observational’ data, i.e. not originating from a controlled experiment. There could be unobserved factors that explain the association.\nTo take a simple example, dog owners live longer, but the reason is simply that dog owners walk more – so it’s the exercise, not the dogs, that cause an increase in longevity. An observational study that doesn’t measure exercise would show a positive association between dog ownership and lifespan, but it’s a non-causal relationship.\n(As an interesting/amusing aside, there is a well known study that established an association between birdkeeping and lung cancer; obviously this is non-causal, yet the study authors recommended that individuals at high risk for cancer avoid ‘avian exposure’, as they were unsure of the mechanism.)\nSo there could easily be unobserved factors that account for the observed association in the BRFSS data. We guard against over-interpreting the results by using causally-neutral language.\n\n\n\nSubmission\n\nSave the notebook.\nRestart the kernel and run all cells. (CAUTION: if your notebook is not saved, you will lose your work.)\nCarefully look through your notebook and verify that all computations execute correctly and all graphics are displayed clearly. You should see no errors; if there are any errors, make sure to correct them before you submit the notebook.\nDownload the notebook as an .ipynb file. This is your backup copy.\nExport the notebook as PDF and upload to Gradescope.\n\n\nTo double-check your work, the cell below will rerun all of the autograder tests.\n\ngrader.check_all()"
  },
  {
    "objectID": "hw/hw1-brfss/hw1-brfss.html",
    "href": "hw/hw1-brfss/hw1-brfss.html",
    "title": "PSTAT100",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport altair as alt\n# disable row limit for plotting\nalt.data_transformers.disable_max_rows()\n# uncomment to ensure graphics display with pdf export\n# alt.renderers.enable('mimetype')\n\n\nBackground\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a long-term effort administered by the CDC to collect data on behaviors affecting physical and mental health, past and present health conditions, and access to healthcare among U.S. residents. The BRFSS comprises telephone surveys of U.S. residents conducted annually since 1984; in the last decade, over half a million interviews have been conducted each year. This is the largest such data collection effort in the world, and many countries have developed similar programs. The objective of the program is to support monitoring and analysis of factors influencing public health in the United States.\nEach year, a standard survey questionnaire is developed that includes a core component comprising questions about: demographic and household information; health-related perceptions, conditions, and behaviors; substance use; and diet. Trained interviewers in each state call randomly selected telephone (landline and cell) numbers and administer the questionnaire; the phone numbers are chosen so as to obtain a representative sample of all households with telephone numbers. Take a moment to read about the 2019 survey here.\nIn this assignment you’ll import and subsample the BRFSS 2019 data and perform a simple descriptive analysis exploring associations between adverse childhood experiences, health perceptions, tobacco use, and depressive disorders. This is an opportunity to practice:\n\nreview of data documentation\ndata assessment and critical thinking about data collection\ndataframe transformations in pandas\ncommunicating and interpreting grouped summaries\n\n\n\nData import and assessment\nThe cell below imports select columns from the 2019 dataset as a pandas DataFrame. The file is big, so this may take a few moments. Run the cell and then have a quick look at the first few rows and columns.\n\n# store variable names of interest\nselected_vars = ['_SEX', '_AGEG5YR', \n                 'GENHLTH', 'ACEPRISN', \n                 'ACEDRUGS', 'ACEDRINK', \n                 'ACEDEPRS', 'ADDEPEV3', \n                 '_SMOKER3', '_LLCPWT']\n\n# import full 2019 BRFSS dataset\nbrfss = pd.read_csv('data/brfss2019.zip', compression = 'zip', usecols = selected_vars)\n\n# invert sampling weights\nbrfss['_LLCPWT'] = 1/brfss._LLCPWT\n\n# print first few rows\nbrfss.head()\n\n\nQuestion 1: Data dimensions\nCheck the dimensions of the dataset. Store the dimensions as nrows and ncolumns.\n\nnrows, ncolumns = ...\n\nprint(nrows, ncolumns)\n\n\ngrader.check(\"q1\")\n\n\n\n\nQuestion 2: Row and column information\nNow that you’ve imported the data, you should verify that the dimensions conform to the format you expect based on data documentation and ensure you understand what each row and each column represents.\nCheck the number of records (interviews conducted) reported and variables measured for 2019 by reviewing the surveillance summaries by year, and then answer the following questions in a few sentences:\n\nDoes the number of rows match the number of reported records?\nHow many columns were imported, and how many columns are reported in the full dataset?\nWhat does each row in the brfss dataframe represent?\nWhat does each column in the brfss dataframe represent\n\nType your answer here, replacing this text.\n\n\n\n\nQuestion 3: Sampling design and data collection\nSkim the overview documentation for the 2019 BRFSS data. Focus specifically the ‘Background’ and ‘Data Collection’ sections, read selectively for relevant details, and answer the following questions in a few sentences:\n\nWho conducts the interviews and how long does a typical interview last?\nWho does an interviewer speak to in each household?\nWhat criteria must a person meet to be interviewed?\nWho can’t appear in the survey? Give two examples.\nWhat is the study population (i.e., all individuals who could possibly be sampled)?\nDoes the data contain any identifying information?\n\nType your answer here, replacing this text.\n\n\n\n\nQuestion 4: Variable descriptions\nYou’ll work with the small subset variables imported above: sex, age, general health self-assessment, smoking status, depressive disorder, and adverse childhood experiences (ACEs). The names of these variables as they appear in the raw dataset are defined in the cell in which you imported the data as selected_vars. It is often useful, and therefore good practice, to include a brief description of each variable at the outset of any reported analyses, both for your own clarity and for that of any potential readers. Open the 2019 BRFSS codebook in your browser and use text searching to locate each of the variable names of interest. Read the codebook entries and fill in the second column in the table below with a one-sentence description of each variable identified in selected_vars. Rephrase the descriptions in your own words – do not copy the codebook descriptions verbatim.\n\n\n\nVariable name\nDescription\n\n\n\n\nGENHLTH\n\n\n\n_SEX\n\n\n\n_AGEG5YR\n\n\n\nACEPRISN\n\n\n\nACEDRUGS\n\n\n\nACEDRINK\n\n\n\nACEDEPRS\n\n\n\nADDEPEV3\n\n\n\n_SMOKER3\n\n\n\n\n\n\n\n\nSubsampling\nTo simplify life a little, we’ll draw a large random sample of the rows and work with that in place of the full dataset. This is known as subsampling.\nThe cell below draws a random subsample of 10k records. Because the subsample is randomly drawn, we should not expect it to vary in any systematic way from the overall dataset, and distinct subsamples should have similar properties – therefore, results downstream should be similar to an analysis of the full dataset, and should also be possible to replicate using distinct subsamples.\n\n# for reproducibility\nnp.random.seed(32221) \n\n# randomly sample 10k records\nsamp = brfss.sample(n = 10000, \n                    replace = False, \n                    weights = '_LLCPWT')\n\nAsides:\n\nNotice that the random number generator seed is set before carrying out this task – this ensures that every time the cell is run, the same subsample is drawn. As a result, the computations in this notebook are reproducible: when I run the notebook on my computer, I get the same results as you get when you run the notebook on your computer.\nNotice also that sampling weights provided with the dataset are used to draw a weighted sample. Some respondents are more likely to be selected than others from the general population of U.S. adults with phone numbers, so the BRFSS calculates derived weights that are inversely proportional to estimates of the probability that the respondent is included in the survey. This is a somewhat sophisticated calculation, however if you’re interested, you can read about how these weights are calculated and why in the overview documentation you used to answer the questions above. We use the sampling weights in drawing the subsample so that we get a representative sample of U.S. adults with phone numbers.\nNotice the missing values. How many entries are missing in each column? The cell below computes the proportion of missing values for each of the selected variables. We’ll return to this issue later on.\n\n\n# proportions of missingness \nsamp.isna().mean()\n\n\n\nTidying\nIn the following series of questions you’ll tidy up the subsample by performing these steps:\n\nselecting columns of interest;\nreplacing coded values of question responses with responses;\ndefining new variables based on existing ones;\nrenaming columns.\n\nThe goal of this is to produce a clean version of the dataset that is well-organized, intuitive to navigate, and ready for analysis.\nThe variable entries are coded numerically to represent certain responses. These should be replaced by more informative entries. We can use the codebook to determine which number means what, and replace the values accordingly.\nThe cell below replaces the numeric values for _AGEG5YR by their meanings, illustrating how to use .replace() with a dictionary to convert the numeric coding to interpretable values. The basic strategy is:\n\nStore the variable coding for VAR as a dictionary var_codes.\nUse .replace({'VAR': var_codes}) to modify values.\n\nIf you need additional examples, check the pandas documentation for .replace().\n\n# dictionary representing variable coding\nage_codes = {\n    1: '18-24', 2: '25-29', 3: '30-34',\n    4: '35-39', 5: '40-44', 6: '45-49',\n    7: '50-54', 8: '55-59', 9: '60-64',\n    10: '65-69', 11: '70-74', 12: '75-79',\n    13: '80+', 14: 'Unsure/refused/missing'\n}\n\n# recode age categories\nsamp_mod1 = samp.replace({'_AGEG5YR': age_codes})\n\n# check result\nsamp_mod1.head()\n\n\nQuestion 5: Recoding variables\nFollowing the example immediately above and referring to the 2019 BRFSS codebook, replace the numeric codings with response categories for each of the following variables:\n\n_SEX\nGENHLTH\n_SMOKER3\n\nNotice that above, the first modification (slicing) was stored as samp_mod1, and was a function of samp. You’ll follow this pattern, creating samp_mod2, samp_mod3, and so on so that each step (modification) of your data manipulations is stored separately, for easy troubleshooting.\n\nRecode _SEX: define a new dataframe samp_mod2 that is the same as samp_mod1 but with the _SEX variable recoded as M and F.\nRecode GENHLTH: define a new dataframe samp_mod3 that is the same as samp_mod2 but with the GENHLTH variable recoded as Excellent, Very good, Good, Fair, Poor, Unsure, and Refused.\nRecode _SMOKER3: define a new dataframe samp_mod4 that is the same as samp_mod3 but with _SMOKER3 recoded as Daily, Some days, Former, Never, and Unsure/refused/missing.\nPrint the first few rows of samp_mod4.\n\n\n# define dictionary for sex\nsex_codes = ...\n\n# recode sex\nsamp_mod2 = ...\n\n# define dictionary for health\nhealth_codes = ...\n\n# recode health\nsamp_mod3 = ...\n\n# define dictionary for smoking\nsmoke_codes = ...\n\n# recode smoking\nsamp_mod4 = ...\n\n# print a few rows\n...\n\n\ngrader.check(\"q5\")\n\n\n\nQuestion 6: Value replacement\nNow all the variables except the adverse childhood experience and depressive disorder question responses are represented interpretably. In the codebook that the answer key is identical for these remaining variables.\nThe numeric codings can be replaced all at once by applying .replace() to the dataframe with an argument of the form\n\ndf.replace({'var1': varcodes1, 'var2': varcodes1, ..., 'varp': varcodesp})\n\nDefine a new dataframe samp_mod5 that is the same as samp_mod4 but with the remaining variables recoded according to the answer key Yes, No, Unsure, Refused. Print the first few rows of the result using .head().\n\n# define dictionary\nanswer_codes = ...\n\n# recode\nsamp_mod5 = ...\n\n# check using head()\n...\n\n\ngrader.check(\"q6\")\n\nFinally, all the variables in the dataset are categorical. Notice that the current data types do not reflect this.\n\nsamp_mod5.dtypes\n\nLet’s coerce the variables to category data types using .astype().\n\n# coerce to categorical\nsamp_mod6 = samp_mod5.astype('category')\n\n# check new data types\nsamp_mod6.dtypes\n\n\n\nQuestion 7: Define ACE indicator variable\nDownstream analysis of ACEs will be facilitated by having an indicator variable that is a 1 if the respondent answered ‘Yes’ to any ACE question, and a 0 otherwise – that way, you can easily count the number of respondents reporting ACEs by summing up the indicator or compute the proportion by taking an average.\nTo this end, define a new logical variable:\n\nadverse_conditions: did the respondent answer yes to any of the adverse childhood condition questions?\n\nYou can accomplish this task in several steps:\n\nObtain a logical array indicating the positions of the ACE variables (hint: use .columns to obtain the column index and operate on the result with .str.startswith(...).). Store this as ace_positions.\nUse the logical array ace_positions to select the ACE columns via .loc[]. Store this as ace_data.\nObtain a dataframe that indicates whether each entry is a ‘Yes’ (hint: use the boolean operator ==, which is a vectorized operation). Store this as ace_yes.\nCompute the row sums using .sum(). Store this as ace_numyes.\nDefine the new variable as ace_numyes > 0.\n\nStore the result as samp_mod7, and print the first few rows using .head().\n\n\n# copy samp_mod6\nsamp_mod7 = samp_mod6.copy()\n\n# ace column positions\nace_positions = ...\n\n# ace data\nace_data = ...\n\n# ace yes indicators\nace_yes = ...\n\n# number of yesses\nace_numyes = ...\n\n# assign new variable\nsamp_mod7['adverse_conditions'] = ...\n\n# check result using .head()\n...\n\n\ngrader.check(\"q7\")\n\n\n\nQuestion 8: Define missingness indicator variable\nAs you saw earlier, there are some missing values for the ACE questions. These arise whenever a respondent is not asked these questions. In fact, answers are missing for nearly 80% of the respondents in our subsample. We should keep track of this information. Define a missing indicator:\n\nadverse_missing: is a response missing for at least one of the ACE questions?\n\n\n# copy modification 7\nsamp_mod8 = samp_mod7.copy()\n\n# define missing indicator using loc\n...\n\n# check using head()\n\n\ngrader.check(\"q8\")\n\n\n\nQuestion 9: Filter respondents who did not answer ACE questions\nSince values are missing for the ACE question if a respondent was not asked, we can remove these observations and do any analysis conditional on respondents having been asked the ACE questions. Use your indicator variable adverse_missing to filter out respondents who were not asked the ACE questions.\nNote that this dramatically limits the scope of inference for subsequent analyses to only those locations where the ACE module was included in the survey.\n\nsamp_mod9 = ...\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10: Define depression indicator variable\nIt will prove similarly helpful to define an indicator for reported depression:\n\ndepression: did the respondent report having been diagnosed with a depressive disorder?\n\nFollow the same strategy as above for the ACE variables, and store the result as samp_mod10. See if you can perform the calculation of the new variable in a single line of code. Print the first few rows using .head().\n\n# copy samp_mod9\nsamp_mod10 = samp_mod9.copy()\n\n# define new variable using loc\n...\n\n# check using .head()\n...\n\n\ngrader.check(\"q10\")\n\n\n\nQuestion 11: Final dataset\nFor the final dataset, drop the respondent answers to individual questions, the missingness indicator, and select just the derived indicator variables along with general health, sex, age, and smoking status. Check the pandas documentation for .rename() and follow the examples to rename the latter variables:\n\ngeneral_health\nsex\nage\nsmoking\n\nSee if you can perform both operations (slicing and renaming) in a single chain. Store the result as data.\n\nsamp_mod10.columns\n\n\n# slice and rename\ndata = ...\n\n# check using .head()\n\n\ngrader.check(\"q11\")\n\n\n\n\nDescriptive analysis\nNow that you have a clean dataset, you’ll use grouping and aggregation to compute several summary statistics that will help you explore whether there is an apparent association between experiencing adverse childhood conditions and self-reported health, smoking status, and depressive disorders in areas where the ACE module was administered.\nThe basic strategy will be to calculate the proportions of respondents who answered yes to one of the adverse experience questions when respondents are grouped by the other variables.\n\nQuestion 12: Proportion of respondents reporting ACEs\nCalculate the overall proportion of respondents in the subsample that reported experiencing at least one adverse condition (given that they answered the ACE questions). Use .mean(); store the result as mean_ace and print.\n\n# proportion of respondents reporting at least one adverse condition\nmean_ace = ...\n\n# print\nmean_ace\n\n\ngrader.check(\"q12\")\n\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by general health?\nThe cell below computes the porportion separately by general health self-rating. Notice that the depression variable is dropped so that the result doesn’t also report the proportion of respondents reporting having been diagnosed with a depressive disorder. Notice also that the proportion of missing values for respondents indicating each general health rating is shown.\n\n# proportions grouped by general health\ndata.drop(\n    columns = 'depression'\n).groupby(\n    'general_health'\n).mean(numeric_only = True)\n\nNotice that the row index lists the general health rating out of order. This can be fixed using a .loc[] call and the dictionary that was defined for the variable coding.\n\n# same as above, rearranging index\nace_health = data.drop(\n    columns = 'depression'\n).groupby(\n    'general_health'\n).mean(\n    numeric_only = True\n).loc[list(health_codes.values()), :]\n\n# print\nace_health\n\n\n\nQuestion 13: Association between smoking status and ACEs\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by smoking status?\nFollowing the example above for computing the proportion of respondents reporting ACEs by general health rating, calculate the proportion of respondents reporting ACEs by smoking status (be sure to arrange the rows in appropriate order of smoking status) and store as ace_smoking.\n\n# proportions grouped by smoking status\nace_smoking = data.drop(\n    columns = 'depression'\n).groupby(\n    'smoking'\n).mean(\n    numeric_only = True\n...\n\n# print\nace_smoking\n\n\ngrader.check(\"q13\")\n\n\n\nQuestion 14: Association between depression and ACEs\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by smoking status?\nCalculate the proportion of respondents reporting ACEs by whether respondents had been diagnosed with a depressive disorder and store as ace_depr.\n\n# proportions grouped by having experienced depression\nace_depr = data.groupby(\n    'depression'\n).mean(\n    numeric_only = True\n...\n\n# print\nace_depr\n\n\ngrader.check(\"q14\")\n\n\n\nQuestion 15: Exploring subgroupings\nDoes the apparent association between general health and ACEs persist after accounting for sex?\nRepeat the calculation of the proportion of respondents reporting ACEs by general health rating, but also group by sex. Store the result as ace_health_sex.\n\n# group by general health and sex\nace_health_sex = data.drop(\n    columns = 'depression'\n).groupby(\n    ['general_health', 'sex']\n...\n\n\ngrader.check(\"q15\")\n\nThe cell below rearranges the table a little for better readability.\n\n# pivot table for better display\nace_health_sex.reset_index().pivot(columns = 'sex', index = 'general_health', values = 'adverse_conditions').loc[list(health_codes.values()), :]\n\nEven after rearrangement, the table in the last question is a little tricky to read (few people like visually scanning tables). This information would be better displayed in a plot. The example below generates a bar chart showing the summaries you calculated in Q2(d), with the proportion on the y axis, the health rating on the x axis, and separate bars for the two sexes.\n\n# coerce indices to columns for plotting\nplot_df = ace_health_sex.reset_index()\n\n# specify order of general health categories\ngenhealth_order = list(health_codes.values())\nplot_df.general_health.cat.set_categories(genhealth_order, inplace=True)\nplot_df.sort_values([\"general_health\"], inplace=True)\n\n# plot\nalt.Chart(plot_df).mark_bar().encode(\n    x = alt.X('general_health', \n              sort = ['general_health'],\n              title = 'Self-rated general health'),\n    y = alt.Y('adverse_conditions',\n              title = 'Prop. of respondents reporting ACEs'),\n    color = 'sex',\n    column = 'sex'\n).properties(\n    width = 200, \n    height = 200\n)\n\n\n\n\nQuestion 16: Visualization\nUse the example above to plot the proportion of respondents reporting ACEs against smoking status for men and women.\nHint: you only need to modify the example by substituting smoking status for general health.\n\n# dataframe of proportions grouped by smoking status\nace_smoking_sex = ...\n\n# coerce indices to columns for plotting\n...\n\n# specify order of general health categories\n...\n\n# plot\n...\n\n\n\n\n\nCommunicating results\nHere you’ll be asked to reflect briefly on your findings.\n\n\nQuestion 17: Summary\nIs there an observed association between reporting ACEs and general health, smoking status, and depression among survey respondents who answered the ACE questions?\nWrite a two to three sentence answer to the above question summarizing your findings. State an answer to the question in your first sentence, and then in your second/third sentences describe exactly what you observed in the foregoing descriptive analysis of the BRFSS data. Be precise, but also concise. There is no need to describe any of the data manipulations, survey design, or the like.\nType your answer here, replacing this text.\n\n\n\n\nQuestion 18: Scope of inference\nRecall from the overview documentation all the care that the BRFSS dedicates to collecting a representative sample of the U.S. adult population with phone numbers. Do you think that your findings provide evidence of an association among the general public (not just the individuals survey)? Why or why not? Answer in two sentences.\nType your answer here, replacing this text.\n\n\n\n\nQuestion 19: Bias\nWhat is a potential source of bias in the survey results, and how might this affect the proportions you’ve calculated?\nAnswer in one or two sentences.\nType your answer here, replacing this text.\n\n\n\nComment\nNotice that the language ‘association’ is non-causual: we don’t say that ACEs cause (or don’t cause) poorer health outcomes. This is intentional, because the BRFSS data are what are known as ‘observational’ data, i.e. not originating from a controlled experiment. There could be unobserved factors that explain the association.\nTo take a simple example, dog owners live longer, but the reason is simply that dog owners walk more – so it’s the exercise, not the dogs, that cause an increase in longevity. An observational study that doesn’t measure exercise would show a positive association between dog ownership and lifespan, but it’s a non-causal relationship.\n(As an interesting/amusing aside, there is a well known study that established an association between birdkeeping and lung cancer; obviously this is non-causal, yet the study authors recommended that individuals at high risk for cancer avoid ‘avian exposure’, as they were unsure of the mechanism.)\nSo there could easily be unobserved factors that account for the observed association in the BRFSS data. We guard against over-interpreting the results by using causally-neutral language.\n\n\n\nSubmission\n\nSave the notebook.\nRestart the kernel and run all cells. (CAUTION: if your notebook is not saved, you will lose your work.)\nCarefully look through your notebook and verify that all computations execute correctly and all graphics are displayed clearly. You should see no errors; if there are any errors, make sure to correct them before you submit the notebook.\nDownload the notebook as an .ipynb file. This is your backup copy.\nExport the notebook as PDF and upload to Gradescope.\n\n\nTo double-check your work, the cell below will rerun all of the autograder tests.\n\ngrader.check_all()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Concepts and Analysis",
    "section": "",
    "text": "This is the course website for UCSB’s Data Science Concepts and Analysis class (PSTAT100). Content is directed towards currently enrolled students. Please ask permission before using course materials in any other capacity."
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab0-gettingstarted.ipynb\")"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#jupyter-notebooks",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#jupyter-notebooks",
    "title": "PSTAT100",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nJupyter notebooks are organized into ‘cells’ that can contain either text or codes. For example, this is a text cell.\nTechnically, Jupyter is an application/interface that runs atop a kernel – a programming-language-specific independent environment in which code cells are executed. This basic organization allows for interactive computing with text integration.\nSelecting a cell and pressing Enter will enter edit mode and allow you to edit the cell. From edit mode, pressing Esc will revert to command mode and allow you to navigate the notebook’s cells.\nIn edit mode, most of the keyboard is dedicated to typing into the cell’s editor. Thus, in edit mode there are relatively few shortcuts. In command mode, the entire keyboard is available for shortcuts, so there are many more. Here are a few useful ones:\n\nCtrl + Return : Evaluate the current cell\nShift + Return: Evaluate the current cell and move to the next\nSaving the notebook: s\nBasic navigation: up one cell k, down one cell j\na : create a cell above\nb : create a cell below\ndd : delete a cell\nz : undo the last cell operation\nm : convert a cell to markdown\ny : convert a cell to code\n\nTake a moment to find out what the following commands do:\n\nCell editing: x, c, v, z\nKernel operations: i, 0 (press twice)\n\n\n# Practice the above commands on this cell\n\n\nRunning Cells and Displaying Output\nRun the following cell.\n\nprint(\"Hello, World!\")\n\nHello, World!\n\n\nIn Jupyter notebooks, all print statements are displayed below the cell. Furthermore, the output of only the last line is displayed following the cell upon execution.\n\n\"Will this line be displayed?\"\n\nprint(\"Hello\" + \",\", \"world!\")\n\n5 + 3\n\nHello, world!\n\n\n8\n\n\n\n\nViewing Documentation\nTo output the documentation for a function, use the help() function.\n\nhelp(print)\n\nHelp on built-in function print in module builtins:\n\nprint(...)\n    print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\n    \n    Prints the values to a stream, or to sys.stdout by default.\n    Optional keyword arguments:\n    file:  a file-like object (stream); defaults to the current sys.stdout.\n    sep:   string inserted between values, default a space.\n    end:   string appended after the last value, default a newline.\n    flush: whether to forcibly flush the stream.\n\n\n\nYou can also use Jupyter to view function documentation inside your notebook. The function must already be defined in the kernel for this to work.\nBelow, click your mouse anywhere on print() and use Shift + Tab to view the function’s documentation.\n\nprint('Welcome to this course!')\n\nWelcome to this course!\n\n\n\n\nImporting Libraries\nIn this course, we will be using common Python libraries to help us retrieve, manipulate, and perform operations on data. By convention, we import all libraries at the very top of the notebook. There are also a set of standard aliases that are used to shorten the library names. Below are some of the libraries that you may encounter throughout the course, along with their respective aliases.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#practice-questions-and-numpy-review",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#practice-questions-and-numpy-review",
    "title": "PSTAT100",
    "section": "Practice questions and numpy review",
    "text": "Practice questions and numpy review\nMost assignments for this class will be given as notebooks organized into explanation and prompts followed by response cells; you will complete assignments by filling in all of the response cells.\nMany response cells are followed by a test cell that performs a few checks on your work. Please be aware that test cells don’t always confirm that your response is correct or incorrect. They are meant to give you some useful feedback, but it’s your responsibility to interpret the feedback – please be sure to read and think about test output if tests fail, and make your own assessment of whether you need to revise your response.\nBelow are a few practice questions for you to familiarize yourself with the process. These assume familiarity with basic python syntax and the numpy package.\n\nQuestion 1\nWrite a function summation that evaluates the following summation for \\(n \\geq 1\\):\n\\[\\sum_{i=1}^{n} \\left(i^3 + 5 i^3\\right)\\]\nHint: np.arange(5).sum() will generate an array comprising \\(1, 2, \\dots, 5\\) and then add up the elements of the array.\n\ndef summation(n):\n    \"\"\"Compute the summation i^3 + 5 * i^3 for 1 <= i <= n.\"\"\"\n    # BEGIN SOLUTION\n    out = (6*(np.arange(n + 1)**3)).sum()\n    return out\n    # END SOLUTION\n\n\ngrader.check(\"q1\")\n\nUse your function to compute the sum for…\n\n# n = 2\n...\n\n\n# n = 20\n...\n\n\n\nQuestion 2\nThe core of numpy is the array. Let’s use np.array to create an array. It takes a sequence, such as a list or range (remember that list elements are included between the square brackets [ and ], such as [1, 5, 3]).\nBelow, create an array containing the values 1, 2, 3, 4, and 5 (in that order) and assign it the name my_array.\n\nmy_array = np.array([1, 2, 3, 4, 5]) #SOLUTION\n\n\ngrader.check(\"q2\")\n\nNumpy arrays are integer-indexed by position, with the first element indexed as position 0. Elements can be retrieved by enclosing the desired positions in brackets [].\n\nmy_array[3]\n\n4\n\n\nTo retrieve consecutive positions, specify the starting index and the ending index separated by :, for instance, arr[from:to]. This syntax is non-inclusive of the left endpoint, meaning that the starting index is not included in the output.\n\nmy_array[2:4]\n\narray([3, 4])\n\n\nIn addition to values in the array, we can access attributes such as array’s shape and data type that can be retrieved by name using syntax of the form array.attr. Some useful attributes are:\n\n.shape, a tuple with the length of each array dimension\n.size, the length of the first array dimension\n.dtype, the data type of the entries (float, integer, etc.)\n\nA full list of attributes is here.\n\nmy_array.shape\n\n(5,)\n\n\n\nmy_array.size\n\n5\n\n\n\nmy_array.dtype\n\ndtype('int32')\n\n\nArrays, unlike Python lists, cannot store items of different data types.\n\n# A regular Python list can store items of different data types\n[1, '3']\n\n[1, '3']\n\n\n\n# Arrays will convert everything to the same data type\nnp.array([1, '3'])\n\narray(['1', '3'], dtype='<U11')\n\n\n\n# Another example of array type conversion\nnp.array([5, 8.3])\n\narray([5. , 8.3])\n\n\nArrays are also useful in performing vectorized operations. Given two or more arrays of equal length, arithmetic will perform element-wise computations across the arrays.\nFor example, observe the following:\n\n# Python list addition will concatenate the two lists\n[1, 2, 3] + [4, 5, 6]\n\n[1, 2, 3, 4, 5, 6]\n\n\n\n# NumPy array addition will add them element-wise\nnp.array([1, 2, 3]) + np.array([4, 5, 6])\n\narray([5, 7, 9])\n\n\nArrays can be subsetted by index position, as shown above, or by a logical vector of the same length. For example:\n\nexample_arr = np.arange(4, 10)\nexample_arr\n\narray([4, 5, 6, 7, 8, 9])\n\n\nSuppose we want the last three elements. One option is to use index position:\n\nexample_arr[3:6]\n\narray([7, 8, 9])\n\n\nOr a logical vector:\n\nexample_arr[np.array([False, False, False, True, True, True])]\n\narray([7, 8, 9])\n\n\nThe latter approach allows one to subset based on a condition defined by the values of the vector. For example, we can use the condition \\(x \\geq 7\\) to obtain the logical vector used above.\n\nexample_arr >= 7\n\narray([False, False, False,  True,  True,  True])\n\n\nAnd then we can subset just as before:\n\nexample_arr[example_arr >= 7]\n\narray([7, 8, 9])\n\n\nYou’ll see this done frequently, and it’s sometimes referred to as filtering, because we’re selectively removing values.\n\n\nQuestion 3\nGiven the array random_arr, create an array containing all values \\(x\\) such that \\(2x^4 > 1\\). Name the array valid_values.\n\n# for reproducibility - setting the seed will result in the same random draw each time\nnp.random.seed(42)\n\n# draw 60 uniformly random integers between 0 and 1\nrandom_arr = np.random.rand(60)\n\n# solution here\nvalid_values = random_arr[2*(random_arr**4) > 1] # SOLUTION\n\n\ngrader.check(\"q3\")\n\n\n\nA note on np.arange and np.linspace\nUsually we use np.arange to return an array that steps from a to b with a fixed step size s. While this is fine in some cases, we sometimes prefer to use np.linspace(a, b, N), which divides the interval [a, b] into N equally spaced points.\nnp.arange(start, stop, step) produces an array with all the numbers starting at start, incremendted up by step, stopping before stop is reached. For example, the value of np.arange(1, 6, 2) is an array with elements 1, 3, and 5 – it starts at 1 and counts up by 2, then stops before 6. np.arange(4, 9, 1) is an array with elements 4, 5, 6, 7, and 8. (It doesn’t contain 9 because np.arange stops before the stop value is reached.)\nnp.linspace always includes both end points while np.arange will not include the second end point b. For this reason, especially when we are plotting ranges of values we tend to prefer np.linspace.\nNotice how the following two statements have different parameters but return the same result.\n\nnp.arange(-5, 6, 1.0)\n\narray([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])\n\n\n\nnp.linspace(-5, 5, 11)\n\narray([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])\n\n\nCheck your understanding. Will np.arange(1, 10) produce an array that contains 10? Add a cell below and check to confirm your answer."
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab0-gettingstarted.ipynb\")"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html#jupyter-notebooks",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html#jupyter-notebooks",
    "title": "PSTAT100",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nJupyter notebooks are organized into ‘cells’ that can contain either text or codes. For example, this is a text cell.\nTechnically, Jupyter is an application/interface that runs atop a kernel – a programming-language-specific independent environment in which code cells are executed. This basic organization allows for interactive computing with text integration.\nSelecting a cell and pressing Enter will enter edit mode and allow you to edit the cell. From edit mode, pressing Esc will revert to command mode and allow you to navigate the notebook’s cells.\nIn edit mode, most of the keyboard is dedicated to typing into the cell’s editor. Thus, in edit mode there are relatively few shortcuts. In command mode, the entire keyboard is available for shortcuts, so there are many more. Here are a few useful ones:\n\nCtrl + Return : Evaluate the current cell\nShift + Return: Evaluate the current cell and move to the next\nSaving the notebook: s\nBasic navigation: up one cell k, down one cell j\na : create a cell above\nb : create a cell below\ndd : delete a cell\nz : undo the last cell operation\nm : convert a cell to markdown\ny : convert a cell to code\n\nTake a moment to find out what the following commands do:\n\nCell editing: x, c, v, z\nKernel operations: i, 0 (press twice)\n\n\n# Practice the above commands on this cell\n\n\nRunning Cells and Displaying Output\nRun the following cell.\n\nprint(\"Hello, World!\")\n\nIn Jupyter notebooks, all print statements are displayed below the cell. Furthermore, the output of only the last line is displayed following the cell upon execution.\n\n\"Will this line be displayed?\"\n\nprint(\"Hello\" + \",\", \"world!\")\n\n5 + 3\n\n\n\nViewing Documentation\nTo output the documentation for a function, use the help() function.\n\nhelp(print)\n\nYou can also use Jupyter to view function documentation inside your notebook. The function must already be defined in the kernel for this to work.\nBelow, click your mouse anywhere on print() and use Shift + Tab to view the function’s documentation.\n\nprint('Welcome to this course!')\n\n\n\nImporting Libraries\nIn this course, we will be using common Python libraries to help us retrieve, manipulate, and perform operations on data. By convention, we import all libraries at the very top of the notebook. There are also a set of standard aliases that are used to shorten the library names. Below are some of the libraries that you may encounter throughout the course, along with their respective aliases.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html#practice-questions-and-numpy-review",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html#practice-questions-and-numpy-review",
    "title": "PSTAT100",
    "section": "Practice questions and numpy review",
    "text": "Practice questions and numpy review\nMost assignments for this class will be given as notebooks organized into explanation and prompts followed by response cells; you will complete assignments by filling in all of the response cells.\nMany response cells are followed by a test cell that performs a few checks on your work. Please be aware that test cells don’t always confirm that your response is correct or incorrect. They are meant to give you some useful feedback, but it’s your responsibility to interpret the feedback – please be sure to read and think about test output if tests fail, and make your own assessment of whether you need to revise your response.\nBelow are a few practice questions for you to familiarize yourself with the process. These assume familiarity with basic python syntax and the numpy package.\n\nQuestion 1\nWrite a function summation that evaluates the following summation for \\(n \\geq 1\\):\n\\[\\sum_{i=1}^{n} \\left(i^3 + 5 i^3\\right)\\]\nHint: np.arange(5).sum() will generate an array comprising \\(1, 2, \\dots, 5\\) and then add up the elements of the array.\n\ndef summation(n):\n    \"\"\"Compute the summation i^3 + 5 * i^3 for 1 <= i <= n.\"\"\"\n    ...\n\n\ngrader.check(\"q1\")\n\nUse your function to compute the sum for…\n\n# n = 2\n...\n\n\n# n = 20\n...\n\n\n\nQuestion 2\nThe core of numpy is the array. Let’s use np.array to create an array. It takes a sequence, such as a list or range (remember that list elements are included between the square brackets [ and ], such as [1, 5, 3]).\nBelow, create an array containing the values 1, 2, 3, 4, and 5 (in that order) and assign it the name my_array.\n\nmy_array = ...\n\n\ngrader.check(\"q2\")\n\nNumpy arrays are integer-indexed by position, with the first element indexed as position 0. Elements can be retrieved by enclosing the desired positions in brackets [].\n\nmy_array[3]\n\nTo retrieve consecutive positions, specify the starting index and the ending index separated by :, for instance, arr[from:to]. This syntax is non-inclusive of the left endpoint, meaning that the starting index is not included in the output.\n\nmy_array[2:4]\n\nIn addition to values in the array, we can access attributes such as array’s shape and data type that can be retrieved by name using syntax of the form array.attr. Some useful attributes are:\n\n.shape, a tuple with the length of each array dimension\n.size, the length of the first array dimension\n.dtype, the data type of the entries (float, integer, etc.)\n\nA full list of attributes is here.\n\nmy_array.shape\n\n\nmy_array.size\n\n\nmy_array.dtype\n\nArrays, unlike Python lists, cannot store items of different data types.\n\n# A regular Python list can store items of different data types\n[1, '3']\n\n\n# Arrays will convert everything to the same data type\nnp.array([1, '3'])\n\n\n# Another example of array type conversion\nnp.array([5, 8.3])\n\nArrays are also useful in performing vectorized operations. Given two or more arrays of equal length, arithmetic will perform element-wise computations across the arrays.\nFor example, observe the following:\n\n# Python list addition will concatenate the two lists\n[1, 2, 3] + [4, 5, 6]\n\n\n# NumPy array addition will add them element-wise\nnp.array([1, 2, 3]) + np.array([4, 5, 6])\n\nArrays can be subsetted by index position, as shown above, or by a logical vector of the same length. For example:\n\nexample_arr = np.arange(4, 10)\nexample_arr\n\nSuppose we want the last three elements. One option is to use index position:\n\nexample_arr[3:6]\n\nOr a logical vector:\n\nexample_arr[np.array([False, False, False, True, True, True])]\n\nThe latter approach allows one to subset based on a condition defined by the values of the vector. For example, we can use the condition \\(x \\geq 7\\) to obtain the logical vector used above.\n\nexample_arr >= 7\n\nAnd then we can subset just as before:\n\nexample_arr[example_arr >= 7]\n\nYou’ll see this done frequently, and it’s sometimes referred to as filtering, because we’re selectively removing values.\n\n\nQuestion 3\nGiven the array random_arr, create an array containing all values \\(x\\) such that \\(2x^4 > 1\\). Name the array valid_values.\n\n# for reproducibility - setting the seed will result in the same random draw each time\nnp.random.seed(42)\n\n# draw 60 uniformly random integers between 0 and 1\nrandom_arr = np.random.rand(60)\n\n# solution here\nvalid_values = ...\n\n\ngrader.check(\"q3\")\n\n\n\nA note on np.arange and np.linspace\nUsually we use np.arange to return an array that steps from a to b with a fixed step size s. While this is fine in some cases, we sometimes prefer to use np.linspace(a, b, N), which divides the interval [a, b] into N equally spaced points.\nnp.arange(start, stop, step) produces an array with all the numbers starting at start, incremendted up by step, stopping before stop is reached. For example, the value of np.arange(1, 6, 2) is an array with elements 1, 3, and 5 – it starts at 1 and counts up by 2, then stops before 6. np.arange(4, 9, 1) is an array with elements 4, 5, 6, 7, and 8. (It doesn’t contain 9 because np.arange stops before the stop value is reached.)\nnp.linspace always includes both end points while np.arange will not include the second end point b. For this reason, especially when we are plotting ranges of values we tend to prefer np.linspace.\nNotice how the following two statements have different parameters but return the same result.\n\nnp.arange(-5, 6, 1.0)\n\n\nnp.linspace(-5, 5, 11)\n\nCheck your understanding. Will np.arange(1, 10) produce an array that contains 10? Add a cell below and check to confirm your answer."
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab1-pandas.ipynb\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#creating-dataframes-basic-manipulations",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#creating-dataframes-basic-manipulations",
    "title": "PSTAT100",
    "section": "Creating DataFrames & Basic Manipulations",
    "text": "Creating DataFrames & Basic Manipulations\nA dataframe is a table in which each column has a type; there is an index over the columns (typically string labels) and an index over the rows (typically ordinal numbers). An index is represented by a series object, which is a one-dimensional labeled array. Here you’ll cover:\n\ncreating dataframes from scratch;\nretrieving attributes;\ndataframe indexing;\nadding, removing, and renaming columns.\n\n\nCreating dataframes from scratch\nThe documentation for the pandas DataFrame class provide two primary syntaxes to create a data frame from scratch:\n\nfrom a dictionary\nrow-wise tuples\n\nSyntax 1 (dictionary): You can create a data frame by specifying the columns and values using a dictionary (a concatenation of named lists) as shown below.\nThe keys of the dictionary are the column names, and the values of the dictionary are lists containing the row entries.\n\n# define a dataframe using dictionary syntax\nfruit_info = pd.DataFrame( \n    data = { 'fruit': ['apple', 'orange', 'banana', 'raspberry'],\n             'color': ['red', 'orange', 'yellow', 'pink']\n           })\n\n# print\nfruit_info\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      0\n      apple\n      red\n    \n    \n      1\n      orange\n      orange\n    \n    \n      2\n      banana\n      yellow\n    \n    \n      3\n      raspberry\n      pink\n    \n  \n\n\n\n\nSyntax 2 (row tuples): You can also define a dataframe by specifying the rows as tuples.\nEach row corresponds to a distinct tuple, and the column indices are specified separately.\n\n# define the same dataframe using tuple syntax \nfruit_info2 = pd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"]\n)\n\n# print\nfruit_info2\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      0\n      apple\n      red\n    \n    \n      1\n      orange\n      orange\n    \n    \n      2\n      banana\n      yellow\n    \n    \n      3\n      raspberry\n      pink\n    \n  \n\n\n\n\n\n\nDataframe Attributes\nDataFrames have several basic attributes:\n\n.shape contains dimensions;\n.dtypes contains data types (float, integer, object, etc.)\n.size first (row) dimension;\n.values contains an array comprising each entry in the dataframe.\n.columns contains the column index;\n.index contains the row index.\n\nYou can obtain these attributes by appending the attribute name to the dataframe name. For instance, the dimensions of a dataframe df can be retrieved by df.shape.\n\n# dimensions\nfruit_info.shape\n\n(4, 2)\n\n\nTo retrieve a two-dimensional numpy array with the values of the dataframe, use df.values. It is sometimes useful to extract this data structure in order to perform vectorized operations, linear algebra, and the like.\n\n# as array\nfruit_info.values\n\narray([['apple', 'red'],\n       ['orange', 'orange'],\n       ['banana', 'yellow'],\n       ['raspberry', 'pink']], dtype=object)\n\n\n\n\nDataframe Indexing\nThe entries in a dataframe are indexed. Indices for rows and columns are stored as the .index. and .columns attributes, respectively.\n\nfruit_info.columns\n\nIndex(['fruit', 'color'], dtype='object')\n\n\n\nfruit_info.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\nBy default, the row indexing is simply numbering by consecutive integers.\n\nfruit_info.index.values\n\narray([0, 1, 2, 3], dtype=int64)\n\n\nHowever, rows can alternatively be indexed by labels:\n\n# define with a row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = [\"fruit 1\", \"fruit 2\", \"fruit 3\", 'fruit 4']\n)\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      fruit 1\n      apple\n      red\n    \n    \n      fruit 2\n      orange\n      orange\n    \n    \n      fruit 3\n      banana\n      yellow\n    \n    \n      fruit 4\n      raspberry\n      pink\n    \n  \n\n\n\n\nUnlike data frames in R, the row index label figures prominently in certain operations. The elements of the dataframe can be retrived using .loc[ROW-INDEX, COL-INDEX] which specifies the location of data values by name (not by position).\n\n# retrieve row 0, column 'fruit'\nfruit_info.loc[0, 'fruit']\n\n'apple'\n\n\nMost of the time rows are indexed numerically, and somewhat confusingly, the syntax for .loc does not require putting the row index 0 in quotes, even though it refers to the row label and not the row number. This is important to remember, because often operations will scramble the order of rows. To see the difference, consider the following:\n\n# non-consecutive row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n)\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      8\n      apple\n      red\n    \n    \n      6\n      orange\n      orange\n    \n    \n      4\n      banana\n      yellow\n    \n    \n      2\n      raspberry\n      pink\n    \n  \n\n\n\n\nNow adding .loc[2, 'color'] will retrieve pink, the last row of the dataframe.\n\n# subset\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n).loc[2, 'color']\n\n'pink'\n\n\nTo retrieve values by position, use .iloc. For many, this is more intuitive, as it is most similar to matrix or array indexing in mathematical notation.\n\n# retrieve 0, 0 entry\nfruit_info.iloc[0, 0]\n\n'apple'\n\n\n\n\nAdding, removing, and renaming columns\nThere are two ways to add new columns:\n\ndirect specification;\nusing .loc[].\n\nDirect specification: For a dataFrame df, you can add a column with df['new column name'] = ... and assign a list or array of values to the column.\nUsing .loc[]: For a dataframe df, you can add a column with df.loc[:, 'new column name'] = ... and assign a list or array of values to the column.\nBoth accomplish the same task – adding a new column index and populating values for each row – but .loc[] is a little faster.\n\n\nQuestion 1\nUsing direct specification, add to the fruit_info table a new column called rank1 containing integers 1, 2, 3, and 4, which express your personal preference about the taste ordering for each fruit (1 is tastiest; 4 is least tasty). Make sure that the numbers utilized are unique - no ties are allowed.\n\n\nfruit_info['rank1'] = [1, 3, 4, 2] # SOLUTION\n\n# print\nfruit_info\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n      rank1\n    \n  \n  \n    \n      0\n      apple\n      red\n      1\n    \n    \n      1\n      orange\n      orange\n      3\n    \n    \n      2\n      banana\n      yellow\n      4\n    \n    \n      3\n      raspberry\n      pink\n      2\n    \n  \n\n\n\n\n\ngrader.check(\"q1\")\n\nNow, create a new dataframe fruit_info_mod1 with the same information as fruit_info_original, but has the additional column rank2. Let’s start off with making fruit_info_mod1 as a copy of fruit_info:\n\nfruit_info_mod1 = fruit_info.copy()\n\n\n\nQuestion 2\nUsing .loc[], add a column called rank2 to the fruit_info_mod1 table that contains the same values in the same order as the rank1 column.\nHint: .loc will parse : as shorthand for ‘all indices’.\n\n\nfruit_info_mod1.loc[:, 'rank2']  = [1, 3, 4, 2] #SOLUTION\n\n# print\nfruit_info_mod1\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n      rank1\n      rank2\n    \n  \n  \n    \n      0\n      apple\n      red\n      1\n      1\n    \n    \n      1\n      orange\n      orange\n      3\n      3\n    \n    \n      2\n      banana\n      yellow\n      4\n      4\n    \n    \n      3\n      raspberry\n      pink\n      2\n      2\n    \n  \n\n\n\n\n\ngrader.check(\"q2\")\n\nWhen using the .loc[] approach, the : specifies that values are assigned to all rows of the data frame, so the array assigned to the new variable must be the same length as the data frame. What if we only assign values to certain rows? Try running the cell below.\n\n# define new variable just for rows 1 and 2\nfruit_info_mod1.loc[1:2, 'rank3'] = [1, 2]\n\n# check result\nfruit_info_mod1\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n      rank1\n      rank2\n      rank3\n    \n  \n  \n    \n      0\n      apple\n      red\n      1\n      1\n      NaN\n    \n    \n      1\n      orange\n      orange\n      3\n      3\n      1.0\n    \n    \n      2\n      banana\n      yellow\n      4\n      4\n      2.0\n    \n    \n      3\n      raspberry\n      pink\n      2\n      2\n      NaN\n    \n  \n\n\n\n\nThe remaining rows are assigned missing values. Notice what this does to the data type:\n\n# check data types\nfruit_info_mod1.dtypes\n\nfruit     object\ncolor     object\nrank1      int64\nrank2      int64\nrank3    float64\ndtype: object\n\n\nWe can detect these missing values using .isna():\n\n# returns a logical data frame indicating whether each entry is missing or not\nfruit_info_mod1.isna()\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n      rank1\n      rank2\n      rank3\n    \n  \n  \n    \n      0\n      False\n      False\n      False\n      False\n      True\n    \n    \n      1\n      False\n      False\n      False\n      False\n      False\n    \n    \n      2\n      False\n      False\n      False\n      False\n      False\n    \n    \n      3\n      False\n      False\n      False\n      False\n      True\n    \n  \n\n\n\n\nIt would be more helpful to simply see by column whether there are missing values. Appending a .any() to the above command will do the trick:\n\n# detects whether any column has missing entries\nfruit_info_mod1.isna().any()\n\nfruit    False\ncolor    False\nrank1    False\nrank2    False\nrank3     True\ndtype: bool\n\n\nNow that we’ve had a bit of fun let’s remove those rank variables. Columns can be removed using .drop() with a list of column names to drop as its argument. For example:\n\n# first syntax for .drop()\nfruit_info_mod1.drop(columns = 'color')\n\n\n\n\n\n  \n    \n      \n      fruit\n      rank1\n      rank2\n      rank3\n    \n  \n  \n    \n      0\n      apple\n      1\n      1\n      NaN\n    \n    \n      1\n      orange\n      3\n      3\n      1.0\n    \n    \n      2\n      banana\n      4\n      4\n      2.0\n    \n    \n      3\n      raspberry\n      2\n      2\n      NaN\n    \n  \n\n\n\n\nThere is an alternate syntax to that shown above, which involves specifying the axis (row vs. column) and index name to drop:\n\n# second syntax for .drop()\nfruit_info_mod1.drop('color', axis = 1)\n\n\n\n\n\n  \n    \n      \n      fruit\n      rank1\n      rank2\n      rank3\n    \n  \n  \n    \n      0\n      apple\n      1\n      1\n      NaN\n    \n    \n      1\n      orange\n      3\n      3\n      1.0\n    \n    \n      2\n      banana\n      4\n      4\n      2.0\n    \n    \n      3\n      raspberry\n      2\n      2\n      NaN\n    \n  \n\n\n\n\n\n\nQuestion 3\nUse the .drop() method to drop all rank columns you created in fruit_info_mod1. Note that drop does not change the table, but instead returns a new table with fewer columns or rows. To store the result, assign a new name (or write over the old dataframe). Here, assign the result to fruit_info_original.\nHint: Look through the documentation to see how you can drop multiple columns of a Pandas dataframe at once using a list of column names.\n\n\nfruit_info_original = fruit_info_mod1.drop(columns = ['rank1', 'rank2', 'rank3']) #SOLUTION\n\n# print\nfruit_info_original\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      0\n      apple\n      red\n    \n    \n      1\n      orange\n      orange\n    \n    \n      2\n      banana\n      yellow\n    \n    \n      3\n      raspberry\n      pink\n    \n  \n\n\n\n\n\ngrader.check(\"q3\")\n\nNifty trick: Use df.columns[df.columns.str.startswith('STRING')] to retrieve all indices starting with STRING and ix.values.tolist() to convert an index to an array of index names to obtain a list of column names to drop. Combining these gives df.columns[df.columns.str.startswith('STRING')].values.tolist(), and will return a list of all column names starting with STRING. This can be used in conjunction with the hint to remove all columns starting with rank.\n\n# try the nifty trick here\n\nNow create a new dataframe fruit_info_mod2with the same information as fruit_info_original, but has the column names capitalized. Begin by creating a copy fruit_info_mod2 of fruit_info_original:\n\nfruit_info_mod2 = fruit_info_original.copy()\n\n\n\nQuestion 4\nReview the documentation for .rename(). Based on the examples, rename the columns of fruit_info_mod2 so they begin with capital letters.\nFor many operations, you can change the dataframe ‘in place’ without reassigning the result of the operation to a new name by setting the inplace parameter to True. Use that strategy here.\n\n\nfruit_info_mod2.rename(columns = {'fruit': 'Fruit', 'color': 'Color'}, inplace = True) #SOLUTION\n\n# print\nfruit_info_mod2\n\n\n\n\n\n  \n    \n      \n      Fruit\n      Color\n    \n  \n  \n    \n      0\n      apple\n      red\n    \n    \n      1\n      orange\n      orange\n    \n    \n      2\n      banana\n      yellow\n    \n    \n      3\n      raspberry\n      pink\n    \n  \n\n\n\n\n\ngrader.check(\"q4\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#operations-on-data-frames",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#operations-on-data-frames",
    "title": "PSTAT100",
    "section": "Operations on Data Frames",
    "text": "Operations on Data Frames\nWith some basics in place, here you’ll see how to perform subsetting operations on data frames that are useful for tidying up datasets.\n\nSlicing: selecting columns or rows in chunks or by position.\n\nOften imported data contain columns that are either superfluous or not of interest for a particular project.\nYou may also want to examine particular portions of a data frame.\n\nFiltering: selecting rows that meet certain criteria\n\nOften you’ll want to remove duplicate rows, filter missing observations, or select a structured subset of a data frame.\nAlso helpful for inspection.\n\n\nTo illustrate these operations, you’ll use a dataset comprising counts of the given names of babies born in California each year from 1990 - 2018. The cell below imports the baby names data as a data frame from a .csv file. .head() prints the first few rows of the dataset.\n\n# import baby names data\nbaby_names = pd.read_csv('data/baby_names.csv')\n\n# preview first few rows\nbaby_names.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1990\n      Jessica\n      6635\n    \n    \n      1\n      CA\n      F\n      1990\n      Ashley\n      4537\n    \n    \n      2\n      CA\n      F\n      1990\n      Stephanie\n      4001\n    \n    \n      3\n      CA\n      F\n      1990\n      Amanda\n      3856\n    \n    \n      4\n      CA\n      F\n      1990\n      Jennifer\n      3611\n    \n  \n\n\n\n\nYour focus here isn’t on analyzing this data, so we won’t ask you to spend too much effort getting acquainted with it. However, a brief inspection is always a good idea. Let’s check:\n\ndimensions (number of rows and columns);\nhow many distinct states, sexes, and years.\n\nNote that the above dataframe displayed is a preview of the full dataframe.\n\nQuestion 5\nYou’ve already seen how to examine dimensions using dataframe attributes. Check the dimensions of baby_names and store them in dimensions_baby_names.\n\n\ndimensions_baby_names = baby_names.shape #SOLUTION\n\n\ngrader.check(\"q5\")\n\nYou haven’t yet seen how to retrieve the distinct values of an array or series, without duplicates. There are a few different ways to go about this, but one is to count the number of occurrences of each distinct entry in a column. This can be done by retrieving the column as a series using syntax of the form df.colname, and then pass the result to .value_counts():\n\n# count distinct values\nbaby_names.Sex.value_counts()\n\nF    112196\nM     78566\nName: Sex, dtype: int64\n\n\n\n\nQuestion 6\nCount the number of occurences of each distinct year. Create a series occur_per_year that displays the number of occurrences, ordered by year (so that the years are displayed in order). If you add sort = False as an argument to value_counts, the distinct values will be displayed in the order they appear in the dataset.\nHow many years are represented in the dataset? Store your answer as num_years.\n\n\noccur_per_year = baby_names.Year.value_counts(sort = False) #SOLUTION \n\nnum_years = len(occur_per_year) #SOLUTION\n\nprint(occur_per_year)\nprint(num_years)\n\n1990    6261\n1991    6226\n1992    6304\n1993    6314\n1994    6241\n1995    6092\n1996    6036\n1997    5961\n1998    5976\n1999    6052\n2000    6284\n2001    6333\n2002    6414\n2003    6533\n2004    6708\n2005    6874\n2006    7075\n2007    7250\n2008    7158\n2009    7119\n2010    7010\n2011    6880\n2012    7007\n2013    6861\n2014    6952\n2015    6871\n2016    6770\n2017    6684\n2018    6516\nName: Year, dtype: int64\n29\n\n\n\ngrader.check(\"q6\")\n\n\n\nSlicing: selecting rows and columns\nThere are two fast and simple ways to slice dataframes:\n\nusing .loc to specify rows and columns by index;\nusing .iloc to specify rows and columns by position.\n\nYou have seen simple examples of both of these above. Here we’ll show how to use these two commands to retrieve multiple rows and columns.\n\nSlicing with .loc: specifying index names\nThis method retrieves entries by specifying row and column indexes using syntax of the form df.loc[rows, cols]. The rows and columns can be single indices, a list of indices, or a set of adjacent indices using a colon :. Examples of these usages are shown below.\n\n# single indices -- small slice\nbaby_names.loc[2, 'Name']\n\n'Stephanie'\n\n\n\n# a list of indices -- larger slice\nbaby_names.loc[[2, 3], ['Name', 'Count']]\n\n\n\n\n\n  \n    \n      \n      Name\n      Count\n    \n  \n  \n    \n      2\n      Stephanie\n      4001\n    \n    \n      3\n      Amanda\n      3856\n    \n  \n\n\n\n\n\n# consecutive indices -- a chunk\nbaby_names.loc[2:10, 'Year':'Count']\n\n\n\n\n\n  \n    \n      \n      Year\n      Name\n      Count\n    \n  \n  \n    \n      2\n      1990\n      Stephanie\n      4001\n    \n    \n      3\n      1990\n      Amanda\n      3856\n    \n    \n      4\n      1990\n      Jennifer\n      3611\n    \n    \n      5\n      1990\n      Elizabeth\n      3170\n    \n    \n      6\n      1990\n      Sarah\n      2843\n    \n    \n      7\n      1990\n      Brittany\n      2737\n    \n    \n      8\n      1990\n      Samantha\n      2720\n    \n    \n      9\n      1990\n      Michelle\n      2453\n    \n    \n      10\n      1990\n      Melissa\n      2442\n    \n  \n\n\n\n\n\n\n\nSlicing with .iloc: specifying entry positions\nAn alternative to specifying the indices in order to slice a dataframe is to specify the entry positions using .iloc (‘integer location’). You have seen an example of this too. As with .loc, .iloc can be used to select multiple rows/columns using either lists of positions or a consecutive set with from:to syntax.\n\n# single position\nbaby_names.iloc[2, 3]\n\n'Stephanie'\n\n\n\n# a list of positions\nbaby_names.iloc[[2, 3], [3, 4]]\n\n\n\n\n\n  \n    \n      \n      Name\n      Count\n    \n  \n  \n    \n      2\n      Stephanie\n      4001\n    \n    \n      3\n      Amanda\n      3856\n    \n  \n\n\n\n\n\n# consecutive positions\nbaby_names.iloc[2:11, 2:5]\n\n\n\n\n\n  \n    \n      \n      Year\n      Name\n      Count\n    \n  \n  \n    \n      2\n      1990\n      Stephanie\n      4001\n    \n    \n      3\n      1990\n      Amanda\n      3856\n    \n    \n      4\n      1990\n      Jennifer\n      3611\n    \n    \n      5\n      1990\n      Elizabeth\n      3170\n    \n    \n      6\n      1990\n      Sarah\n      2843\n    \n    \n      7\n      1990\n      Brittany\n      2737\n    \n    \n      8\n      1990\n      Samantha\n      2720\n    \n    \n      9\n      1990\n      Michelle\n      2453\n    \n    \n      10\n      1990\n      Melissa\n      2442\n    \n  \n\n\n\n\nWhile these commands may look very similar to their .loc analogs, there are some subtle but important differences. The row selection looks nearly identical, but recall that .loc uses the index and .iloc uses the position; they look so similar because typically index and position coincide.\nHowever, sorting the baby_names dataframe helps to reveal how the position of a row is not necessarily equal to the index of a row. For example, the first row is not necessarily the row associated with index 1. This distinction is important in understanding the difference between .loc[] and .iloc[].\n\n# sort and display\nsorted_baby_names = baby_names.sort_values(by=['Name'])\nsorted_baby_names.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      160797\n      CA\n      M\n      2008\n      Aadan\n      7\n    \n    \n      178791\n      CA\n      M\n      2014\n      Aadan\n      5\n    \n    \n      163914\n      CA\n      M\n      2009\n      Aadan\n      6\n    \n    \n      171112\n      CA\n      M\n      2012\n      Aaden\n      38\n    \n    \n      179928\n      CA\n      M\n      2015\n      Aaden\n      34\n    \n  \n\n\n\n\nHere is an example of how we would get the 2nd, 3rd, and 4th rows with only the Name column of the baby_names dataframe using both iloc[] and loc[]. Observe the difference, especially after sorting baby_names by name.\n\n# example iloc usage\nsorted_baby_names.iloc[1:4, 3]\n\n178791    Aadan\n163914    Aadan\n171112    Aaden\nName: Name, dtype: object\n\n\nNotice that using loc[] with 1:4 gives different results, since it selects using the index. The index gets moved around when you perform an operation like sort on the dataframe.\n\n# same syntax, different result\nsorted_baby_names.loc[1:4, \"Name\"]\n\n1           Ashley\n22219       Ashley\n138598      Ashley\n151978      Ashley\n120624      Ashley\n            ...   \n74380       Jennie\n19395       Jennie\n23061       Jennie\n91825       Jennie\n4         Jennifer\nName: Name, Length: 68640, dtype: object\n\n\nAbove, the .loc method retrieves all indexes between index 1 and index 4 in the order they appear in the sorted dataset. If instead we want to retrieve the same rows returned by the .iloc command, we need to specify the row indices explicitly as a list:\n\n# retrieve the same rows as iloc using loc\nsorted_baby_names.loc[[178791, 163914, 171112], 'Name']\n\n178791    Aadan\n163914    Aadan\n171112    Aaden\nName: Name, dtype: object\n\n\nSometimes it’s useful for slicing (and other operations) to set one of the columns to be a row index, effectively treating one column as a collection of row labels. This can be accomplished using set_index.\n\n# change the (row) index from 0,1,2,... to the name column\nbaby_names_nameindexed = baby_names.set_index(\"Name\") \nbaby_names_nameindexed.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Count\n    \n    \n      Name\n      \n      \n      \n      \n    \n  \n  \n    \n      Jessica\n      CA\n      F\n      1990\n      6635\n    \n    \n      Ashley\n      CA\n      F\n      1990\n      4537\n    \n    \n      Stephanie\n      CA\n      F\n      1990\n      4001\n    \n    \n      Amanda\n      CA\n      F\n      1990\n      3856\n    \n    \n      Jennifer\n      CA\n      F\n      1990\n      3611\n    \n  \n\n\n\n\nWe can now slice by name directly:\n\n# slice rows for ashley and jennifer\nbaby_names_nameindexed.loc[['Ashley', 'Jennifer'], :]\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Count\n    \n    \n      Name\n      \n      \n      \n      \n    \n  \n  \n    \n      Ashley\n      CA\n      F\n      1990\n      4537\n    \n    \n      Ashley\n      CA\n      F\n      1991\n      4233\n    \n    \n      Ashley\n      CA\n      F\n      1992\n      3966\n    \n    \n      Ashley\n      CA\n      F\n      1993\n      3591\n    \n    \n      Ashley\n      CA\n      F\n      1994\n      3202\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Jennifer\n      CA\n      M\n      1998\n      10\n    \n    \n      Jennifer\n      CA\n      M\n      1999\n      12\n    \n    \n      Jennifer\n      CA\n      M\n      2000\n      10\n    \n    \n      Jennifer\n      CA\n      M\n      2001\n      8\n    \n    \n      Jennifer\n      CA\n      M\n      2002\n      7\n    \n  \n\n88 rows Ã— 4 columns\n\n\n\n\n\nQuestion 7\nLook up the name of a friend! Store the name as friend_name. Use the name-indexed data frame to slice rows for the name of your choice and the Count, Sex, and Year columns in that order, and store the data frame as friend_slice.\n\n\n# if your friend's name is not in the database, use another name\n\nfriend_name = \"Trevor\" # SOLUTION\nfriend_slice = baby_names_nameindexed.loc[\"Trevor\", ['Count', 'Sex', 'Year']] #SOLUTION\n\n#print\nfriend_slice\n\n\n\n\n\n  \n    \n      \n      Count\n      Sex\n      Year\n    \n    \n      Name\n      \n      \n      \n    \n  \n  \n    \n      Trevor\n      5\n      F\n      1990\n    \n    \n      Trevor\n      823\n      M\n      1990\n    \n    \n      Trevor\n      836\n      M\n      1991\n    \n    \n      Trevor\n      897\n      M\n      1992\n    \n    \n      Trevor\n      737\n      M\n      1993\n    \n    \n      Trevor\n      675\n      M\n      1994\n    \n    \n      Trevor\n      682\n      M\n      1995\n    \n    \n      Trevor\n      609\n      M\n      1996\n    \n    \n      Trevor\n      590\n      M\n      1997\n    \n    \n      Trevor\n      647\n      M\n      1998\n    \n    \n      Trevor\n      673\n      M\n      1999\n    \n    \n      Trevor\n      545\n      M\n      2000\n    \n    \n      Trevor\n      535\n      M\n      2001\n    \n    \n      Trevor\n      488\n      M\n      2002\n    \n    \n      Trevor\n      425\n      M\n      2003\n    \n    \n      Trevor\n      369\n      M\n      2004\n    \n    \n      Trevor\n      372\n      M\n      2005\n    \n    \n      Trevor\n      335\n      M\n      2006\n    \n    \n      Trevor\n      302\n      M\n      2007\n    \n    \n      Trevor\n      281\n      M\n      2008\n    \n    \n      Trevor\n      252\n      M\n      2009\n    \n    \n      Trevor\n      219\n      M\n      2010\n    \n    \n      Trevor\n      194\n      M\n      2011\n    \n    \n      Trevor\n      161\n      M\n      2012\n    \n    \n      Trevor\n      142\n      M\n      2013\n    \n    \n      Trevor\n      126\n      M\n      2014\n    \n    \n      Trevor\n      114\n      M\n      2015\n    \n    \n      Trevor\n      103\n      M\n      2016\n    \n    \n      Trevor\n      90\n      M\n      2017\n    \n    \n      Trevor\n      78\n      M\n      2018\n    \n  \n\n\n\n\n\ngrader.check(\"q7\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#filtering",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#filtering",
    "title": "PSTAT100",
    "section": "Filtering",
    "text": "Filtering\nFiltering is sifting out rows according to a criterion, and can be accomplished using an array or series of Trues and Falses defined by a comparison. To take a simple example, say you wanted to filter out all names with fewer than 1000 occurrences. First you could define a logical series:\n\n# true if filtering criterion is met, false otherwise\narr = baby_names.Count > 1000\n\nThen you can filter using that array:\n\n# filter\nbaby_names_filtered = baby_names[arr]\nbaby_names_filtered.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1990\n      Jessica\n      6635\n    \n    \n      1\n      CA\n      F\n      1990\n      Ashley\n      4537\n    \n    \n      2\n      CA\n      F\n      1990\n      Stephanie\n      4001\n    \n    \n      3\n      CA\n      F\n      1990\n      Amanda\n      3856\n    \n    \n      4\n      CA\n      F\n      1990\n      Jennifer\n      3611\n    \n  \n\n\n\n\nNotice that the filtered array is much smaller than the overall array – only about 2000 rows correspond to a name occurring more than 1000 times in a year for a gender.\n\n# compare dimensions\nprint(baby_names_filtered.shape)\nprint(baby_names.shape)\n\n(2517, 5)\n(190762, 5)\n\n\nYou have already encountered this concept in lab 0 when subsetting an array. For your reference, some commonly used comparison operators are given below.\n\n\n\nSymbol\nUsage\nMeaning\n\n\n\n\n==\na == b\nDoes a equal b?\n\n\n<=\na <= b\nIs a less than or equal to b?\n\n\n>=\na >= b\nIs a greater than or equal to b?\n\n\n<\na < b\nIs a less than b?\n\n\n>\na > b\nIs a greater than b?\n\n\n~\n~p\nReturns negation of p\n\n\n|\np | q\np OR q\n\n\n&\np & q\np AND q\n\n\n^\np ^ q\np XOR q (exclusive or)\n\n\n\nWhat if instead you wanted to filter using multiple conditions? Here’s an example of retrieving rows with counts exceeding 1000 for only the year 2001:\n\n# filter using two conditions\nbaby_names[(baby_names.Year == 2000) & (baby_names.Count > 1000)]\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      36416\n      CA\n      F\n      2000\n      Emily\n      2958\n    \n    \n      36417\n      CA\n      F\n      2000\n      Ashley\n      2831\n    \n    \n      36418\n      CA\n      F\n      2000\n      Samantha\n      2579\n    \n    \n      36419\n      CA\n      F\n      2000\n      Jessica\n      2484\n    \n    \n      36420\n      CA\n      F\n      2000\n      Jennifer\n      2263\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      137298\n      CA\n      M\n      2000\n      Oscar\n      1089\n    \n    \n      137299\n      CA\n      M\n      2000\n      Thomas\n      1061\n    \n    \n      137300\n      CA\n      M\n      2000\n      Cameron\n      1052\n    \n    \n      137301\n      CA\n      M\n      2000\n      Austin\n      1010\n    \n    \n      137302\n      CA\n      M\n      2000\n      Richard\n      1001\n    \n  \n\n98 rows Ã— 5 columns\n\n\n\n\nQuestion 8\nSelect the girl names in 2010 that were given more than 3000 times, and store them as common_girl_names_2010.\nNote: Any time you use p & q to filter the dataframe, make sure to use df[df[(p) & (q)]] or df.loc[df[(p) & (q)]]). That is, make sure to wrap conditions with parentheses to ensure the intended order of operations.\n\n\ncommon_girl_names_2010 = baby_names[(baby_names.Sex == 'F') & (baby_names.Year == 2010) & (baby_names.Count > 3000)] #SOLUTION\n\ncommon_girl_names_2010\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      76793\n      CA\n      F\n      2010\n      Isabella\n      3368\n    \n    \n      76794\n      CA\n      F\n      2010\n      Sophia\n      3361\n    \n  \n\n\n\n\n\ngrader.check(\"q8\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#grouping-and-aggregation",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#grouping-and-aggregation",
    "title": "PSTAT100",
    "section": "Grouping and aggregation",
    "text": "Grouping and aggregation\nGrouping and aggregation are useful in generating data summaries, which are often important starting points in exploring a dataset.\n\nAggregation\nAggregation literally means ‘putting together’ (etymologically the word means ‘joining the herd’) – in statistics and data science, this refers to data summaries like an average, a minimum, or a measure of spread such as the sample variance or mean absolute deviation (data herding!). From a technical point of view, operations that take multiple values as inputs and return a single output are considered summaries – in other words, statistics. Some of the most common aggregations are:\n\nsum\nproduct\ncount\nnumber of distinct values\nmean\nmedian\nvariance\nstandard deviation\nminimum/maximum\nquantiles\n\nPandas has built-in dataframe operations that compute most of these summaries across either axis (column-wise or row-wise):\n\n.sum()\n.prod()\n.mean()\n.median()\n.var()\n.std()\n.nunique()\n.min() and .max()\n.quantile()\n\nTo illustrate these operations, let’s filter out all names in 1995.\n\n# filter 1995 names\nnames_95 = baby_names[baby_names.Year == 1995]\n\nHow many individuals were counted in total in 1995? We can address that by computing a sum of the counts:\n\n# n for 1995\nnames_95.Count.sum()\n\n494580\n\n\nWhat is the typical frequency of all names in 1995? We can address that by computing the average count:\n\n# average count for a name in 1995\nnames_95.Count.mean()\n\n81.18516086671043\n\n\n\n\nQuestion 9\nFind how often the most common name 1995 was given and store this as names_95_max_count. Use this value to filter names_95 and find which name was most common that year. Store the filtered dataframe as names_95_most_common_name.\n\n\nnames_95_max_count = names_95.Count.max() #SOLUTION\nnames_95_most_common_name = (names_95.loc[names_95.Count == names_95.Count.max(),  'Name']) #SOLUTION\n\n\nprint(\"Number of people with the most frequent name in 1995 is :\", names_95_max_count, \"people\")\nprint(\"Most frequent name in 1995 is:\", names_95_most_common_name.values[0])\n\nNumber of people with the most frequent name in 1995 is : 5003 people\nMost frequent name in 1995 is: Daniel\n\n\n\ngrader.check(\"q9\")\n\nCaution! If applied to the entire dataframe, the operation df.max() (or any other aggregation) will return the maximum of each column. Notice that the cell below does not return the row you found just now, but could easily be misinterpreted as such. The cell does tell you that the maximum value of sex (alphabetically last) is M and the maximum name (alphabetically last) is Zyanya and the maximum count is 5003; it does not tell you that 5003 boys were named Zyanya in 1995.\n\n# maximum of each variable\nnames_95.max()\n\nState        CA\nSex           M\nYear       1995\nName     Zyanya\nCount      5003\ndtype: object\n\n\n\n\nGrouping\nWhat if you want to know the most frequent male and female names? If so, you’ll need to repeat the above operations group-wise by sex.\nIn general, any variable in a dataframe can be used to define a grouping structure on the rows (or, less commonly, columns). After grouping, any dataframe operations will be executed within each group, but not across groups. This can be used to generate grouped summaries, such as the maximum count for boys and girls; as a point of terminology, we’d describe this summary as ‘maximum count by sex’.\nThe .groupby() function defines such a structure; here is the documentation. The cell below groups the names_95 dataframe by sex. Notice that when the grouped dataframe is previewed with .head(), the first few rows are returned for each group.\n\n# grouped dataframe\nnames_95_bysex = names_95.groupby('Sex')\n\n# print\nnames_95_bysex.head(2)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      18604\n      CA\n      F\n      1995\n      Jessica\n      4620\n    \n    \n      18605\n      CA\n      F\n      1995\n      Ashley\n      2903\n    \n    \n      124938\n      CA\n      M\n      1995\n      Daniel\n      5003\n    \n    \n      124939\n      CA\n      M\n      1995\n      Michael\n      4783\n    \n  \n\n\n\n\nAny aggregation operations applied to the grouped dataframe will be applied separately to the rows where Sex == M and the rows where Sex == F. For example, computing .sum() on the grouped dataframe will show the total number of individuals in the data for 1995 by sex:\n\n# number of individuals by sex\nnames_95_bysex.Count.sum()\n\nSex\nF    234552\nM    260028\nName: Count, dtype: int64\n\n\nThe most frequent boy and girl names can be found using .idxmax() groupwise to obtain the index of the first occurence of the maximum count for each sex, and then slicing with .loc:\n\n# first most common names by sex\nnames_95.loc[names_95_bysex.Count.idxmax(), :]\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      18604\n      CA\n      F\n      1995\n      Jessica\n      4620\n    \n    \n      124938\n      CA\n      M\n      1995\n      Daniel\n      5003\n    \n  \n\n\n\n\nSince .idxmax() gives the index of the first occurrence, these are the alphabetically first most common names; there could be ties. You know from your work so far that there are no ties for the male names; another filtering step can be used to check for ties among the female names.\n\n# ties?\nnames_95[names_95_bysex.Count.max().values[0] == names_95['Count']]\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      18604\n      CA\n      F\n      1995\n      Jessica\n      4620\n    \n  \n\n\n\n\nSo, no ties.\n\n\nQuestion 10\nAre there more girl names or boy names in 1995? Use the grouped dataframe names_95_bysex with the .count() aggregation to find the total number of names for each sex. Store the female and male counts as girl_name_count and boy_name_count, respectfully.\n\n\ngirl_name_count = names_95_bysex.Count.count()['F'] #SOLUTION\nboy_name_count = names_95_bysex.Count.count()['M'] #SOLUTION\n\n#print\nprint(girl_name_count)\nprint(boy_name_count)\n\n3614\n2478\n\n\n\ngrader.check(\"q10\")\n\n\n\nChaining operations\nYou have already seen examples of this, but pandas and numpy operations can be chained together in sequence. For example, names_95.Count.max() is a chain with two steps: first select the Count column (.count); then compute the maximum (.max()).\nGrouped summaries are often convenient to compute in a chained fashion, rather than by assigning the grouped dataframe a new name and performing operations on the resulting object. For example, finding the total number of boys and girls recorded in the 1995 data can be done with the following chain:\n\n# repeating previous calculation, more streamlined\nnames_95.groupby('Sex').Count.sum()\n\nSex\nF    234552\nM    260028\nName: Count, dtype: int64\n\n\nWe can take this even one step further and also perform the filtering in sequence as part of the chain:\n\n# longer chain\nbaby_names[baby_names.Year == 1995].groupby('Sex').Count.sum()\n\nSex\nF    234552\nM    260028\nName: Count, dtype: int64\n\n\nChains can get somewhat long, but they have the advantage of making codes more efficient, and often more readable. We did above in one step what took several lines before. Further, this chain can almost be read aloud:\n“Take baby names, filter on year, then group by sex, then select name counts, then compute the sum.”\nLet’s now consider computing the average counts of boy and girl names for each year 1990-1995. This can be accomplished by the following chain (notice it is possible to group by multiple variables).\n\n# average counts by sex and year\nbaby_names[baby_names.Year <= 1995].groupby(['Year', 'Sex']).mean(numeric_only = True)\n\n\n\n\n\n  \n    \n      \n      \n      Count\n    \n    \n      Year\n      Sex\n      \n    \n  \n  \n    \n      1990\n      F\n      70.085760\n    \n    \n      M\n      115.231930\n    \n    \n      1991\n      F\n      70.380888\n    \n    \n      M\n      114.608124\n    \n    \n      1992\n      F\n      68.744510\n    \n    \n      M\n      110.601556\n    \n    \n      1993\n      F\n      66.330675\n    \n    \n      M\n      107.896552\n    \n    \n      1994\n      F\n      66.426301\n    \n    \n      M\n      102.967966\n    \n    \n      1995\n      F\n      64.900941\n    \n    \n      M\n      104.934625\n    \n  \n\n\n\n\nThis display is not ideal. We can ‘pivot’ the table into a wide format by adding a few extra steps in the chain: change the indices to columns; then define a new shape by specifying which column should be the new row index, which should be the new column index, and which values should populate the table.\n\n# average counts by sex and year\nbaby_names[baby_names.Year <= 1995].groupby(\n    ['Year', 'Sex']\n    ).mean(\n    numeric_only = True\n    ).reset_index().pivot(\n    index = 'Sex', columns = 'Year', values = 'Count'\n    )\n\n\n\n\n\n  \n    \n      Year\n      1990\n      1991\n      1992\n      1993\n      1994\n      1995\n    \n    \n      Sex\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      F\n      70.08576\n      70.380888\n      68.744510\n      66.330675\n      66.426301\n      64.900941\n    \n    \n      M\n      115.23193\n      114.608124\n      110.601556\n      107.896552\n      102.967966\n      104.934625\n    \n  \n\n\n\n\nStyle comment: break long chains over multiple lines with indentation. The above chain is too long to be readable in one line. To balance the readability of codes with the efficiency of chaining, it is good practice to break long chains over several lines, with appropriate indentations.\nHere are some rules of thumb on style.\n\nSeparate comparisons by spaces (a<b as a < b)\nSplit chains longer than 30-40 characters over multiple lines\nSplit lines between delimiters (, )\nIncrease indentation for lines between delimiters\nFor chained operations, try to get each step in the chain shown on a separate line\nFor functions with multiple arguments, split lines so that each argument is on its own line\n\n\n\nQuestion 11\nWrite a chain with appropriate style to display the (first) most common boy and girl names in each of the years 2005-2015. Do this in two steps:\n\nFirst filter baby_names by year, then group by year and sex, and then find the indices of first occurence of the largest counts. Store these indices as ind.\nThen use .loc[] with your stored indices to slice baby_names so as to retrieve the rows corresponding to each most frequent name each year and for each sex; then pivot this table so that the columns are years, the rows are sexes, and the entries are names. Store this as pivot_names.\n\n\n\n# BEGIN SOLUTION\nind =  baby_names[(baby_names.Year <= 2015) & (baby_names.Year >= 2005)].groupby(\n    ['Sex', 'Year']\n).Count.idxmax(\n).values\n\npivot_names = baby_names.loc[ind, :].pivot(\n    index = 'Sex', \n    columns = 'Year', \n    values = 'Name'\n)\n\n# END SOLUTION\nprint(ind)\npivot_names\n\n[ 55767  59866  64073  68355  72602  76793  80890  84883  88981  92944\n  96958 150164 152939 155807 158775 161686 164614 167527 170414 173323\n 176221 179159]\n\n\n\n\n\n\n  \n    \n      Year\n      2005\n      2006\n      2007\n      2008\n      2009\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n    \n    \n      Sex\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      F\n      Emily\n      Emily\n      Emily\n      Isabella\n      Isabella\n      Isabella\n      Sophia\n      Sophia\n      Sophia\n      Sophia\n      Sophia\n    \n    \n      M\n      Daniel\n      Daniel\n      Daniel\n      Daniel\n      Daniel\n      Jacob\n      Jacob\n      Jacob\n      Jacob\n      Noah\n      Noah\n    \n  \n\n\n\n\n\ngrader.check(\"q11\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html",
    "href": "labs/lab1-pandas/lab1-pandas.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab1-pandas.ipynb\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#creating-dataframes-basic-manipulations",
    "href": "labs/lab1-pandas/lab1-pandas.html#creating-dataframes-basic-manipulations",
    "title": "PSTAT100",
    "section": "Creating DataFrames & Basic Manipulations",
    "text": "Creating DataFrames & Basic Manipulations\nA dataframe is a table in which each column has a type; there is an index over the columns (typically string labels) and an index over the rows (typically ordinal numbers). An index is represented by a series object, which is a one-dimensional labeled array. Here you’ll cover:\n\ncreating dataframes from scratch;\nretrieving attributes;\ndataframe indexing;\nadding, removing, and renaming columns.\n\n\nCreating dataframes from scratch\nThe documentation for the pandas DataFrame class provide two primary syntaxes to create a data frame from scratch:\n\nfrom a dictionary\nrow-wise tuples\n\nSyntax 1 (dictionary): You can create a data frame by specifying the columns and values using a dictionary (a concatenation of named lists) as shown below.\nThe keys of the dictionary are the column names, and the values of the dictionary are lists containing the row entries.\n\n# define a dataframe using dictionary syntax\nfruit_info = pd.DataFrame( \n    data = { 'fruit': ['apple', 'orange', 'banana', 'raspberry'],\n             'color': ['red', 'orange', 'yellow', 'pink']\n           })\n\n# print\nfruit_info\n\nSyntax 2 (row tuples): You can also define a dataframe by specifying the rows as tuples.\nEach row corresponds to a distinct tuple, and the column indices are specified separately.\n\n# define the same dataframe using tuple syntax \nfruit_info2 = pd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"]\n)\n\n# print\nfruit_info2\n\n\n\nDataframe Attributes\nDataFrames have several basic attributes:\n\n.shape contains dimensions;\n.dtypes contains data types (float, integer, object, etc.)\n.size first (row) dimension;\n.values contains an array comprising each entry in the dataframe.\n.columns contains the column index;\n.index contains the row index.\n\nYou can obtain these attributes by appending the attribute name to the dataframe name. For instance, the dimensions of a dataframe df can be retrieved by df.shape.\n\n# dimensions\nfruit_info.shape\n\nTo retrieve a two-dimensional numpy array with the values of the dataframe, use df.values. It is sometimes useful to extract this data structure in order to perform vectorized operations, linear algebra, and the like.\n\n# as array\nfruit_info.values\n\n\n\nDataframe Indexing\nThe entries in a dataframe are indexed. Indices for rows and columns are stored as the .index. and .columns attributes, respectively.\n\nfruit_info.columns\n\n\nfruit_info.index\n\nBy default, the row indexing is simply numbering by consecutive integers.\n\nfruit_info.index.values\n\nHowever, rows can alternatively be indexed by labels:\n\n# define with a row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = [\"fruit 1\", \"fruit 2\", \"fruit 3\", 'fruit 4']\n)\n\nUnlike data frames in R, the row index label figures prominently in certain operations. The elements of the dataframe can be retrived using .loc[ROW-INDEX, COL-INDEX] which specifies the location of data values by name (not by position).\n\n# retrieve row 0, column 'fruit'\nfruit_info.loc[0, 'fruit']\n\nMost of the time rows are indexed numerically, and somewhat confusingly, the syntax for .loc does not require putting the row index 0 in quotes, even though it refers to the row label and not the row number. This is important to remember, because often operations will scramble the order of rows. To see the difference, consider the following:\n\n# non-consecutive row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n)\n\nNow adding .loc[2, 'color'] will retrieve pink, the last row of the dataframe.\n\n# subset\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n).loc[2, 'color']\n\nTo retrieve values by position, use .iloc. For many, this is more intuitive, as it is most similar to matrix or array indexing in mathematical notation.\n\n# retrieve 0, 0 entry\nfruit_info.iloc[0, 0]\n\n\n\nAdding, removing, and renaming columns\nThere are two ways to add new columns:\n\ndirect specification;\nusing .loc[].\n\nDirect specification: For a dataFrame df, you can add a column with df['new column name'] = ... and assign a list or array of values to the column.\nUsing .loc[]: For a dataframe df, you can add a column with df.loc[:, 'new column name'] = ... and assign a list or array of values to the column.\nBoth accomplish the same task – adding a new column index and populating values for each row – but .loc[] is a little faster.\n\n\nQuestion 1\nUsing direct specification, add to the fruit_info table a new column called rank1 containing integers 1, 2, 3, and 4, which express your personal preference about the taste ordering for each fruit (1 is tastiest; 4 is least tasty). Make sure that the numbers utilized are unique - no ties are allowed.\n\n\n...\n\n# print\nfruit_info\n\n\ngrader.check(\"q1\")\n\nNow, create a new dataframe fruit_info_mod1 with the same information as fruit_info_original, but has the additional column rank2. Let’s start off with making fruit_info_mod1 as a copy of fruit_info:\n\nfruit_info_mod1 = fruit_info.copy()\n\n\n\nQuestion 2\nUsing .loc[], add a column called rank2 to the fruit_info_mod1 table that contains the same values in the same order as the rank1 column.\nHint: .loc will parse : as shorthand for ‘all indices’.\n\n\n...\n\n# print\nfruit_info_mod1\n\n\ngrader.check(\"q2\")\n\nWhen using the .loc[] approach, the : specifies that values are assigned to all rows of the data frame, so the array assigned to the new variable must be the same length as the data frame. What if we only assign values to certain rows? Try running the cell below.\n\n# define new variable just for rows 1 and 2\nfruit_info_mod1.loc[1:2, 'rank3'] = [1, 2]\n\n# check result\nfruit_info_mod1\n\nThe remaining rows are assigned missing values. Notice what this does to the data type:\n\n# check data types\nfruit_info_mod1.dtypes\n\nWe can detect these missing values using .isna():\n\n# returns a logical data frame indicating whether each entry is missing or not\nfruit_info_mod1.isna()\n\nIt would be more helpful to simply see by column whether there are missing values. Appending a .any() to the above command will do the trick:\n\n# detects whether any column has missing entries\nfruit_info_mod1.isna().any()\n\nNow that we’ve had a bit of fun let’s remove those rank variables. Columns can be removed using .drop() with a list of column names to drop as its argument. For example:\n\n# first syntax for .drop()\nfruit_info_mod1.drop(columns = 'color')\n\nThere is an alternate syntax to that shown above, which involves specifying the axis (row vs. column) and index name to drop:\n\n# second syntax for .drop()\nfruit_info_mod1.drop('color', axis = 1)\n\n\n\nQuestion 3\nUse the .drop() method to drop all rank columns you created in fruit_info_mod1. Note that drop does not change the table, but instead returns a new table with fewer columns or rows. To store the result, assign a new name (or write over the old dataframe). Here, assign the result to fruit_info_original.\nHint: Look through the documentation to see how you can drop multiple columns of a Pandas dataframe at once using a list of column names.\n\n\nfruit_info_original = ...\n\n# print\nfruit_info_original\n\n\ngrader.check(\"q3\")\n\nNifty trick: Use df.columns[df.columns.str.startswith('STRING')] to retrieve all indices starting with STRING and ix.values.tolist() to convert an index to an array of index names to obtain a list of column names to drop. Combining these gives df.columns[df.columns.str.startswith('STRING')].values.tolist(), and will return a list of all column names starting with STRING. This can be used in conjunction with the hint to remove all columns starting with rank.\n\n# try the nifty trick here\n\nNow create a new dataframe fruit_info_mod2with the same information as fruit_info_original, but has the column names capitalized. Begin by creating a copy fruit_info_mod2 of fruit_info_original:\n\nfruit_info_mod2 = fruit_info_original.copy()\n\n\n\nQuestion 4\nReview the documentation for .rename(). Based on the examples, rename the columns of fruit_info_mod2 so they begin with capital letters.\nFor many operations, you can change the dataframe ‘in place’ without reassigning the result of the operation to a new name by setting the inplace parameter to True. Use that strategy here.\n\n\n...\n\n# print\nfruit_info_mod2\n\n\ngrader.check(\"q4\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#operations-on-data-frames",
    "href": "labs/lab1-pandas/lab1-pandas.html#operations-on-data-frames",
    "title": "PSTAT100",
    "section": "Operations on Data Frames",
    "text": "Operations on Data Frames\nWith some basics in place, here you’ll see how to perform subsetting operations on data frames that are useful for tidying up datasets.\n\nSlicing: selecting columns or rows in chunks or by position.\n\nOften imported data contain columns that are either superfluous or not of interest for a particular project.\nYou may also want to examine particular portions of a data frame.\n\nFiltering: selecting rows that meet certain criteria\n\nOften you’ll want to remove duplicate rows, filter missing observations, or select a structured subset of a data frame.\nAlso helpful for inspection.\n\n\nTo illustrate these operations, you’ll use a dataset comprising counts of the given names of babies born in California each year from 1990 - 2018. The cell below imports the baby names data as a data frame from a .csv file. .head() prints the first few rows of the dataset.\n\n# import baby names data\nbaby_names = pd.read_csv('data/baby_names.csv')\n\n# preview first few rows\nbaby_names.head()\n\nYour focus here isn’t on analyzing this data, so we won’t ask you to spend too much effort getting acquainted with it. However, a brief inspection is always a good idea. Let’s check:\n\ndimensions (number of rows and columns);\nhow many distinct states, sexes, and years.\n\nNote that the above dataframe displayed is a preview of the full dataframe.\n\nQuestion 5\nYou’ve already seen how to examine dimensions using dataframe attributes. Check the dimensions of baby_names and store them in dimensions_baby_names.\n\n\ndimensions_baby_names = ...\n\n\ngrader.check(\"q5\")\n\nYou haven’t yet seen how to retrieve the distinct values of an array or series, without duplicates. There are a few different ways to go about this, but one is to count the number of occurrences of each distinct entry in a column. This can be done by retrieving the column as a series using syntax of the form df.colname, and then pass the result to .value_counts():\n\n# count distinct values\nbaby_names.Sex.value_counts()\n\n\n\nQuestion 6\nCount the number of occurences of each distinct year. Create a series occur_per_year that displays the number of occurrences, ordered by year (so that the years are displayed in order). If you add sort = False as an argument to value_counts, the distinct values will be displayed in the order they appear in the dataset.\nHow many years are represented in the dataset? Store your answer as num_years.\n\n\noccur_per_year = ...\n\nnum_years = ...\n\nprint(occur_per_year)\nprint(num_years)\n\n\ngrader.check(\"q6\")\n\n\n\nSlicing: selecting rows and columns\nThere are two fast and simple ways to slice dataframes:\n\nusing .loc to specify rows and columns by index;\nusing .iloc to specify rows and columns by position.\n\nYou have seen simple examples of both of these above. Here we’ll show how to use these two commands to retrieve multiple rows and columns.\n\nSlicing with .loc: specifying index names\nThis method retrieves entries by specifying row and column indexes using syntax of the form df.loc[rows, cols]. The rows and columns can be single indices, a list of indices, or a set of adjacent indices using a colon :. Examples of these usages are shown below.\n\n# single indices -- small slice\nbaby_names.loc[2, 'Name']\n\n\n# a list of indices -- larger slice\nbaby_names.loc[[2, 3], ['Name', 'Count']]\n\n\n# consecutive indices -- a chunk\nbaby_names.loc[2:10, 'Year':'Count']\n\n\n\n\nSlicing with .iloc: specifying entry positions\nAn alternative to specifying the indices in order to slice a dataframe is to specify the entry positions using .iloc (‘integer location’). You have seen an example of this too. As with .loc, .iloc can be used to select multiple rows/columns using either lists of positions or a consecutive set with from:to syntax.\n\n# single position\nbaby_names.iloc[2, 3]\n\n\n# a list of positions\nbaby_names.iloc[[2, 3], [3, 4]]\n\n\n# consecutive positions\nbaby_names.iloc[2:11, 2:5]\n\nWhile these commands may look very similar to their .loc analogs, there are some subtle but important differences. The row selection looks nearly identical, but recall that .loc uses the index and .iloc uses the position; they look so similar because typically index and position coincide.\nHowever, sorting the baby_names dataframe helps to reveal how the position of a row is not necessarily equal to the index of a row. For example, the first row is not necessarily the row associated with index 1. This distinction is important in understanding the difference between .loc[] and .iloc[].\n\n# sort and display\nsorted_baby_names = baby_names.sort_values(by=['Name'])\nsorted_baby_names.head()\n\nHere is an example of how we would get the 2nd, 3rd, and 4th rows with only the Name column of the baby_names dataframe using both iloc[] and loc[]. Observe the difference, especially after sorting baby_names by name.\n\n# example iloc usage\nsorted_baby_names.iloc[1:4, 3]\n\nNotice that using loc[] with 1:4 gives different results, since it selects using the index. The index gets moved around when you perform an operation like sort on the dataframe.\n\n# same syntax, different result\nsorted_baby_names.loc[1:4, \"Name\"]\n\nAbove, the .loc method retrieves all indexes between index 1 and index 4 in the order they appear in the sorted dataset. If instead we want to retrieve the same rows returned by the .iloc command, we need to specify the row indices explicitly as a list:\n\n# retrieve the same rows as iloc using loc\nsorted_baby_names.loc[[178791, 163914, 171112], 'Name']\n\nSometimes it’s useful for slicing (and other operations) to set one of the columns to be a row index, effectively treating one column as a collection of row labels. This can be accomplished using set_index.\n\n# change the (row) index from 0,1,2,... to the name column\nbaby_names_nameindexed = baby_names.set_index(\"Name\") \nbaby_names_nameindexed.head()\n\nWe can now slice by name directly:\n\n# slice rows for ashley and jennifer\nbaby_names_nameindexed.loc[['Ashley', 'Jennifer'], :]\n\n\n\nQuestion 7\nLook up the name of a friend! Store the name as friend_name. Use the name-indexed data frame to slice rows for the name of your choice and the Count, Sex, and Year columns in that order, and store the data frame as friend_slice.\n\n\n# if your friend's name is not in the database, use another name\n\nfriend_name = ...\nfriend_slice = ...\n\n#print\nfriend_slice\n\n\ngrader.check(\"q7\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#filtering",
    "href": "labs/lab1-pandas/lab1-pandas.html#filtering",
    "title": "PSTAT100",
    "section": "Filtering",
    "text": "Filtering\nFiltering is sifting out rows according to a criterion, and can be accomplished using an array or series of Trues and Falses defined by a comparison. To take a simple example, say you wanted to filter out all names with fewer than 1000 occurrences. First you could define a logical series:\n\n# true if filtering criterion is met, false otherwise\narr = baby_names.Count > 1000\n\nThen you can filter using that array:\n\n# filter\nbaby_names_filtered = baby_names[arr]\nbaby_names_filtered.head()\n\nNotice that the filtered array is much smaller than the overall array – only about 2000 rows correspond to a name occurring more than 1000 times in a year for a gender.\n\n# compare dimensions\nprint(baby_names_filtered.shape)\nprint(baby_names.shape)\n\nYou have already encountered this concept in lab 0 when subsetting an array. For your reference, some commonly used comparison operators are given below.\n\n\n\nSymbol\nUsage\nMeaning\n\n\n\n\n==\na == b\nDoes a equal b?\n\n\n<=\na <= b\nIs a less than or equal to b?\n\n\n>=\na >= b\nIs a greater than or equal to b?\n\n\n<\na < b\nIs a less than b?\n\n\n>\na > b\nIs a greater than b?\n\n\n~\n~p\nReturns negation of p\n\n\n|\np | q\np OR q\n\n\n&\np & q\np AND q\n\n\n^\np ^ q\np XOR q (exclusive or)\n\n\n\nWhat if instead you wanted to filter using multiple conditions? Here’s an example of retrieving rows with counts exceeding 1000 for only the year 2001:\n\n# filter using two conditions\nbaby_names[(baby_names.Year == 2000) & (baby_names.Count > 1000)]\n\n\nQuestion 8\nSelect the girl names in 2010 that were given more than 3000 times, and store them as common_girl_names_2010.\nNote: Any time you use p & q to filter the dataframe, make sure to use df[df[(p) & (q)]] or df.loc[df[(p) & (q)]]). That is, make sure to wrap conditions with parentheses to ensure the intended order of operations.\n\n\ncommon_girl_names_2010 = ...\n\ncommon_girl_names_2010\n\n\ngrader.check(\"q8\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#grouping-and-aggregation",
    "href": "labs/lab1-pandas/lab1-pandas.html#grouping-and-aggregation",
    "title": "PSTAT100",
    "section": "Grouping and aggregation",
    "text": "Grouping and aggregation\nGrouping and aggregation are useful in generating data summaries, which are often important starting points in exploring a dataset.\n\nAggregation\nAggregation literally means ‘putting together’ (etymologically the word means ‘joining the herd’) – in statistics and data science, this refers to data summaries like an average, a minimum, or a measure of spread such as the sample variance or mean absolute deviation (data herding!). From a technical point of view, operations that take multiple values as inputs and return a single output are considered summaries – in other words, statistics. Some of the most common aggregations are:\n\nsum\nproduct\ncount\nnumber of distinct values\nmean\nmedian\nvariance\nstandard deviation\nminimum/maximum\nquantiles\n\nPandas has built-in dataframe operations that compute most of these summaries across either axis (column-wise or row-wise):\n\n.sum()\n.prod()\n.mean()\n.median()\n.var()\n.std()\n.nunique()\n.min() and .max()\n.quantile()\n\nTo illustrate these operations, let’s filter out all names in 1995.\n\n# filter 1995 names\nnames_95 = baby_names[baby_names.Year == 1995]\n\nHow many individuals were counted in total in 1995? We can address that by computing a sum of the counts:\n\n# n for 1995\nnames_95.Count.sum()\n\nWhat is the typical frequency of all names in 1995? We can address that by computing the average count:\n\n# average count for a name in 1995\nnames_95.Count.mean()\n\n\n\nQuestion 9\nFind how often the most common name 1995 was given and store this as names_95_max_count. Use this value to filter names_95 and find which name was most common that year. Store the filtered dataframe as names_95_most_common_name.\n\n\nnames_95_max_count = ...\nnames_95_most_common_name = ...\n\n\nprint(\"Number of people with the most frequent name in 1995 is :\", names_95_max_count, \"people\")\nprint(\"Most frequent name in 1995 is:\", names_95_most_common_name.values[0])\n\n\ngrader.check(\"q9\")\n\nCaution! If applied to the entire dataframe, the operation df.max() (or any other aggregation) will return the maximum of each column. Notice that the cell below does not return the row you found just now, but could easily be misinterpreted as such. The cell does tell you that the maximum value of sex (alphabetically last) is M and the maximum name (alphabetically last) is Zyanya and the maximum count is 5003; it does not tell you that 5003 boys were named Zyanya in 1995.\n\n# maximum of each variable\nnames_95.max()\n\n\n\nGrouping\nWhat if you want to know the most frequent male and female names? If so, you’ll need to repeat the above operations group-wise by sex.\nIn general, any variable in a dataframe can be used to define a grouping structure on the rows (or, less commonly, columns). After grouping, any dataframe operations will be executed within each group, but not across groups. This can be used to generate grouped summaries, such as the maximum count for boys and girls; as a point of terminology, we’d describe this summary as ‘maximum count by sex’.\nThe .groupby() function defines such a structure; here is the documentation. The cell below groups the names_95 dataframe by sex. Notice that when the grouped dataframe is previewed with .head(), the first few rows are returned for each group.\n\n# grouped dataframe\nnames_95_bysex = names_95.groupby('Sex')\n\n# print\nnames_95_bysex.head(2)\n\nAny aggregation operations applied to the grouped dataframe will be applied separately to the rows where Sex == M and the rows where Sex == F. For example, computing .sum() on the grouped dataframe will show the total number of individuals in the data for 1995 by sex:\n\n# number of individuals by sex\nnames_95_bysex.Count.sum()\n\nThe most frequent boy and girl names can be found using .idxmax() groupwise to obtain the index of the first occurence of the maximum count for each sex, and then slicing with .loc:\n\n# first most common names by sex\nnames_95.loc[names_95_bysex.Count.idxmax(), :]\n\nSince .idxmax() gives the index of the first occurrence, these are the alphabetically first most common names; there could be ties. You know from your work so far that there are no ties for the male names; another filtering step can be used to check for ties among the female names.\n\n# ties?\nnames_95[names_95_bysex.Count.max().values[0] == names_95['Count']]\n\nSo, no ties.\n\n\nQuestion 10\nAre there more girl names or boy names in 1995? Use the grouped dataframe names_95_bysex with the .count() aggregation to find the total number of names for each sex. Store the female and male counts as girl_name_count and boy_name_count, respectfully.\n\n\ngirl_name_count = ...\nboy_name_count = ...\n\n#print\nprint(girl_name_count)\nprint(boy_name_count)\n\n\ngrader.check(\"q10\")\n\n\n\nChaining operations\nYou have already seen examples of this, but pandas and numpy operations can be chained together in sequence. For example, names_95.Count.max() is a chain with two steps: first select the Count column (.count); then compute the maximum (.max()).\nGrouped summaries are often convenient to compute in a chained fashion, rather than by assigning the grouped dataframe a new name and performing operations on the resulting object. For example, finding the total number of boys and girls recorded in the 1995 data can be done with the following chain:\n\n# repeating previous calculation, more streamlined\nnames_95.groupby('Sex').Count.sum()\n\nWe can take this even one step further and also perform the filtering in sequence as part of the chain:\n\n# longer chain\nbaby_names[baby_names.Year == 1995].groupby('Sex').Count.sum()\n\nChains can get somewhat long, but they have the advantage of making codes more efficient, and often more readable. We did above in one step what took several lines before. Further, this chain can almost be read aloud:\n“Take baby names, filter on year, then group by sex, then select name counts, then compute the sum.”\nLet’s now consider computing the average counts of boy and girl names for each year 1990-1995. This can be accomplished by the following chain (notice it is possible to group by multiple variables).\n\n# average counts by sex and year\nbaby_names[baby_names.Year <= 1995].groupby(['Year', 'Sex']).mean(numeric_only = True)\n\nThis display is not ideal. We can ‘pivot’ the table into a wide format by adding a few extra steps in the chain: change the indices to columns; then define a new shape by specifying which column should be the new row index, which should be the new column index, and which values should populate the table.\n\n# average counts by sex and year\nbaby_names[baby_names.Year <= 1995].groupby(\n    ['Year', 'Sex']\n    ).mean(\n    numeric_only = True\n    ).reset_index().pivot(\n    index = 'Sex', columns = 'Year', values = 'Count'\n    )\n\nStyle comment: break long chains over multiple lines with indentation. The above chain is too long to be readable in one line. To balance the readability of codes with the efficiency of chaining, it is good practice to break long chains over several lines, with appropriate indentations.\nHere are some rules of thumb on style.\n\nSeparate comparisons by spaces (a<b as a < b)\nSplit chains longer than 30-40 characters over multiple lines\nSplit lines between delimiters (, )\nIncrease indentation for lines between delimiters\nFor chained operations, try to get each step in the chain shown on a separate line\nFor functions with multiple arguments, split lines so that each argument is on its own line\n\n\n\nQuestion 11\nWrite a chain with appropriate style to display the (first) most common boy and girl names in each of the years 2005-2015. Do this in two steps:\n\nFirst filter baby_names by year, then group by year and sex, and then find the indices of first occurence of the largest counts. Store these indices as ind.\nThen use .loc[] with your stored indices to slice baby_names so as to retrieve the rows corresponding to each most frequent name each year and for each sex; then pivot this table so that the columns are years, the rows are sexes, and the entries are names. Store this as pivot_names.\n\n\n\n...\nprint(ind)\npivot_names\n\n\ngrader.check(\"q11\")"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab2-sampling.ipynb\")"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#sampling-designs",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#sampling-designs",
    "title": "PSTAT100",
    "section": "Sampling designs",
    "text": "Sampling designs\nThe sampling design of a study refers to the way observational units are selected from the collection of all observational units. Any design can be expressed by the probability that each unit is included in the sample. In a random sample, all units are equally likely to be included.\nFor example, you might want to learn about U.S. residents (population), but only be able for ethical or practical reasons to study adults (sampling frame), and decide to do a mail survey of 2000 randomly selected addresses in each state (sampling design). Each collection of 2000 addresses may constitute a random sample of households, but even with a 100% response rate the survey results will not be a random sample of adult U.S. residents because individuals share addresses and the population sizes are different from state to state."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#bias",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#bias",
    "title": "PSTAT100",
    "section": "Bias",
    "text": "Bias\nFormally, bias describes the ‘typical’ deviation of a sample statistic the correspongind population value.\nFor example, if a particular sampling design tends to produce an average measurement around 1.5 units, but the true average in the population is 2 units, then the estimate has a bias of -0.5 units. The language ‘typical’ and ‘tends to’ is important here. Estimates are never perfect, so just because an estimate is off by -0.5 units for one sample doesn’t make it biased – it is only biased if it is consistently off.\nAlthough bias is technically a property of a sample statistic (like the sample average), it’s common to talk about a biased sample – this term refers to a dataset collected using a sampling design that produces biased statistics.\nThis is exactly what you’ll explore in this lab – the relationship between sampling design and bias."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#simulated-data",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#simulated-data",
    "title": "PSTAT100",
    "section": "Simulated data",
    "text": "Simulated data\nYou will be simulating data in this lab. Simulation is a great means of exploration because you can control the population properties, which are generally unknown in practice.\nWhen working with real data, you just have one dataset, and you don’t know any of the properties of the population or what might have happened if a different sample were collected. That makes it difficult to understand sampling variation and impossible to directly compare the sample properties to the population properties.\nWith simulated data, by contrast, you control how data are generated with exact precision – so by extension, you know everything there is to know about the population. In addition, repeated simulation of data makes it possible to explore the typical behavior of a particular sampling design, so you can learn ‘what usually happens’ for a particular sampling design by direct observation."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#hypothetical-population",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#hypothetical-population",
    "title": "PSTAT100",
    "section": "Hypothetical population",
    "text": "Hypothetical population\nTo provide a little context to this scenario, imagine that you’re measuring eucalyptus seeds to determine their typical diameter. The cell below simulates diameter measurements for a hypothetical population of 5000 seeds; imagine that this is the total number of seeds in a small grove at some point in time.\n\n# simulate seed diameters\nnp.random.seed(40221) # for reproducibility\npopulation = pd.DataFrame(\n    data = {'diameter': np.random.gamma(shape = 2, scale = 1/2, size = 5000), \n            'seed': np.arange(5000)}\n).set_index('seed')\n\n# check first few rows\npopulation.head(3)\n\n\n\n\n\n  \n    \n      \n      diameter\n    \n    \n      seed\n      \n    \n  \n  \n    \n      0\n      0.831973\n    \n    \n      1\n      1.512187\n    \n    \n      2\n      0.977392\n    \n  \n\n\n\n\n\nQuestion 1\nCalculate the mean diameter for the hypothetical population and store the value as mean_diameter.\n\nmean_pop_diameter = population.diameter.mean() #SOLUTION\n\nmean_pop_diameter\n\n1.0189291497049837\n\n\n\ngrader.check(\"q1\")\n\n\n\nQuestion 2\nCalculate the standard deviation of diameters for the hypothetical population and store the value as std_dev_pop_diameter.\n\nstd_dev_pop_diameter = population.diameter.std() # SOLUTION\nstd_dev_pop_diameter\n\n0.7239297185874436\n\n\n\ngrader.check(\"q2\")\n\nThe cell below produces a histogram of the population values – the distribution of diameter measurements among the hypothetical population – with a vertical line indicating the population mean.\n\n# base layer\nbase_pop = alt.Chart(population).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_pop = base_pop.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20), \n              title = 'Diameter (mm)', \n              scale = alt.Scale(domain = (0, 6))),\n    y = alt.Y('count()', title = 'Number of seeds in population')\n)\n\n# vertical line for population mean\nmean_pop = base_pop.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# display\nhist_pop + mean_pop"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#random-sampling",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#random-sampling",
    "title": "PSTAT100",
    "section": "Random sampling",
    "text": "Random sampling\nImagine that your sampling design involves collecting bunches of plant material from several locations in the grove and sifting out the seeds with a fine sieve until you obtaining 250 seeds. We’ll suppose that using your collection method, any of the 5000 seeds is equally likely to be obtained, so that your 250 seeds comprise a random sample of the population.\nWe can simulate samples obtained using your hypothetical design by drawing values without replacement from the population.\n\n# draw a random sample of seeds\nnp.random.seed(40221) # for reproducibility\nsample = population.sample(n = 250, replace = False)\n\n\nQuestion 3\nCalculate the mean diameter of seeds in the simulated sample and store the value as mean_sample_diameter.\n\nmean_sample_diameter = sample.diameter.mean() # SOLUTION\nmean_sample_diameter\n\n0.9777218824084053\n\n\n\ngrader.check(\"q3\")\n\nYou should see above that the sample mean is close to the population mean. In fact, all sample statistics are close to the population; this can be seen by comparing the distribution of sample values with the distribution of population values.\n\n# base layer\nbase_samp = alt.Chart(sample).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_samp = base_samp.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20),\n              scale = alt.Scale(domain = (0, 6)),\n              title = 'Diameter (mm)'),\n    y = alt.Y('count()', title = 'Number of seeds in sample')\n)\n\n# vertical line for population mean\nmean_samp = base_samp.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# display\nhist_samp + mean_samp | hist_pop + mean_pop\n\n\n\n\n\n\nWhile there are some small differences, the overall shape is similar and the sample mean is almost exactly the same as the population mean. So with this sampling design, you obtained a dataset with few distortions of the population properties, and the sample mean is a good estimate of the population mean.\n\n\nAssessing bias through simulation\nYou may wonder: does that happen all the time, or was this just a lucky draw? This question can be answered by simulating a large number of samples and checking the average behavior to see whether the undistorted representation of the population is typical for this sampling design.\nThe cell below estimates the bias of the sample mean by:\n\ndrawing 1000 samples of size 300;\nstoring the sample mean from each sample;\ncomputing the average difference between the sample means and the population mean.\n\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means[i] = population.sample(n = 250, replace = False).diameter.mean()\n\nThe bias of the sample mean is its average distance from the population mean. We can estimate this using our simulation results as follows:\n\n# bias\nsamp_means.mean() - population.diameter.mean()\n\n-0.0012458197406362004\n\n\nSo the average error observed in 1000 simulations was about 0.001 mm! This suggests that the sample mean is unbiased: on average, there is no error. Therefore, at least with respect to estimating the population mean, random samples appear to be unbiased samples.\nHowever, unbiasedness does not mean that you won’t observe estimation error. There is a natural amount of variability from sample to sample, because in each sample a different collection of seeds is measured. We can estimate this as well using the simulation results by checking the standard deviation of the sample means across all 1000 samples:\n\nsamp_means.std()\n\n0.04285044949708931\n\n\nSo on average, the sample mean varies by about 0.04 mm from sample to sample.\nWe could also check how much the sample mean deviates from the population mean on average by computing root mean squared error:\n\nnp.sqrt(np.sum((samp_means - population.diameter.mean())**2)/1000)\n\n0.0428685559463899\n\n\nNote that this is very close to the variance of the sample mean across simulations, but not exactly the same; this latter calculation measures the spread around the population mean, and is a conventional measure of estimation accuracy.\nThe cell below plots a histogram representing the distribution of values of the sample mean across the 1000 samples you simulated (this is known as the sampling distribution of the sample mean). It shows a peak right at the population mean (blue vertical line) but some symmetric variation to either side – most values are between about 0.93 and 1.12.\n\n# plot the simulated sampling distribution\nsampling_dist = alt.Chart(pd.DataFrame({'sample mean': samp_means})).mark_bar().encode(\n    x = alt.X('sample mean', bin = alt.Bin(maxbins = 30), title = 'Value of sample mean'),\n    y = alt.Y('count()', title = 'Number of simulations')\n)\n\nsampling_dist + mean_pop"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#biased-sampling",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#biased-sampling",
    "title": "PSTAT100",
    "section": "Biased sampling",
    "text": "Biased sampling\nIn this scenario, you’ll use the same hypothetical population of eucalyptus seed diameter measurements and explore the impact of a biased sampling design.\nIn the first design, you were asked to imagine that you collected and sifted plant material to obtain seeds. Suppose you didn’t know that the typical seed is about 1mm in diameter and decided to use a sieve that is a little too coarse, tending only to sift out larger seeds and letting smaller seeds pass through. As a result, small seeds have a lower probability of being included in the sample and large seeds have a higher probability of being included in the sample.\nThis kind of sampling design can be described by assigning differential sampling weights \\(w_1, \\dots, w_N\\) to each observation. The cell below defines some hypothetical weights such that larger diameters are more likely to be sampled.\n\npopulation_mod1 = population.copy()\n\n\n# inclusion weight as a function of seed diameter\ndef weight_fn(x, r = 10, c = 1.5):\n    out = 1/(1 + np.e**(-r*(x - c)))\n    return out\n\n# create a grid of values to use in plotting the function\ngrid = np.linspace(0, 6, 100)\nweight_df = pd.DataFrame(\n    {'seed diameter': grid,\n     'weight': weight_fn(grid)}\n)\n\n# plot of inclusion probability against diameter\nweight_plot = alt.Chart(weight_df).mark_area(opacity = 0.3, line = True).encode(\n    x = 'seed diameter',\n    y = 'weight'\n).properties(height = 100)\n\n# show plot\nweight_plot\n\n\n\n\n\n\nThe actual probability that a seed is included in the sample – its inclusion probability – is proportional to the sampling weight. These inclusion probabilities \\(\\pi_i\\) can be calculated by normalizing the weights \\(w_i\\) over all seeds in the population \\(i = 1, \\dots, 5000\\):\n\\[\\pi_i = \\frac{w_i}{\\sum_i w_i}\\]\nIt may help you to picture how the weights will be used in sampling to line up this plot with the population distribution. In effect, we will sample more from the right tail of the population distribution, where the weight is nearest to 1.\n\nhist_pop & weight_plot\n\n\n\n\n\n\nThe following cell draws a sample with replacement from the hypothetical seed population with seeds weighted according to the inclusion probability given by the function above.\n\n# assign weight to each seed\npopulation_mod1['weight'] = weight_fn(population_mod1.diameter)\n\n# draw weighted sample\nnp.random.seed(40721)\nsample2 = population_mod1.sample(n = 250, replace = False, weights = 'weight').loc[:, ['diameter']]\n\n\nQuestion 4\nCalculate the mean diameter of seeds in the simulated sample and store the value as mean_sample2_diameter.\n\nmean_sample2_diameter = sample2.diameter.mean() #SOLUTION\n\nmean_sample2_diameter\n\n2.026720254934241\n\n\n\ngrader.check(\"q4\")\n\n\n\n\nQuestion 5\nShow side-by-side plots of the distribution of sample values and the distribution of population values, with vertical lines indicating the corresponding mean on each plot.\nHint: copy the cell that produced this plot in scenario 1 and replace sample with sample2. Utilizing different methods is also welcome.\n\n# BEGIN SOLUTION NO PROMPT\n\n# base layer\nbase_samp = alt.Chart(sample2).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_samp = base_samp.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20),\n              scale = alt.Scale(domain = (0, 6)),\n              title = 'Diameter (mm)'),\n    y = alt.Y('count()', title = 'Number of seeds in sample')\n)\n\n# vertical line for population mean\nmean_samp = base_samp.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# base layer\nbase_samp = ...\n\n# histogram of diameter measurements\nhist_samp = ...\n\n# vertical line for population mean\nmean_samp = ...\n\n# combine layers\n\"\"\" # END PROMPT\n\n# display\nhist_samp + mean_samp | hist_pop + mean_pop\n\n\n\n\n\n\n\n\n\nAssessing bias through simulation\nHere you’ll mimic the simulation done in scenario 1 to assess the bias of the sample mean under this new sampling design.\n\npopulation_mod1.head()\n\n\n\n\n\n  \n    \n      \n      diameter\n      weight\n    \n    \n      seed\n      \n      \n    \n  \n  \n    \n      0\n      0.831973\n      0.001254\n    \n    \n      1\n      1.512187\n      0.530430\n    \n    \n      2\n      0.977392\n      0.005346\n    \n    \n      3\n      2.874944\n      0.999999\n    \n    \n      4\n      0.506508\n      0.000048\n    \n  \n\n\n\n\n\n\nQuestion 6\nInvestigate the bias of the sample mean by:\n\ndrawing 1000 samples with observations weighted by inclusion probability;\nstoring the collection of sample means from each sample as samp_means;\ncomputing the average difference between the sample means and the population mean (in that order!) and storing the result as avg_diff.\n\n(Hint: copy the cell that performs this simulation in scenario 1, and be sure to replace population with population_mod1 and adjust the sampling step to include weights = ... with the appropriate argument.)\n\n# BEGIN SOLUTION NO PROMPT\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means[i] = population_mod1.sample(\n        n = 250, \n        replace = False, \n        weights = 'weight'\n    ).diameter.mean()\n\n# bias\navg_diff = samp_means.mean() - population_mod1.diameter.mean()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\n\n# storage for the sample means\n\n# repeatedly sample and store the sample mean in the samp_means array\n\n# bias\n\"\"\" # END PROMPT\n\navg_diff\n\n1.0576986191465758\n\n\n\ngrader.check(\"q6\")\n\n\n\n\nQuestion 7\nDoes this sampling design seem to introduce bias? If so, does the sample mean tend to over-estimate or under-estimate the population mean?\nType your answer here, replacing this text.\nSOLUTION: Yes, this sampling design seems to introduce bias, where the sample mean tends to over-estimate the population mean."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#hypothetical-population-1",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#hypothetical-population-1",
    "title": "PSTAT100",
    "section": "Hypothetical population",
    "text": "Hypothetical population\nSuppose you’re interested in determining the average beak-to-tail length of red-tailed hawks to help differentiate them from other hawks by sight at a distance. Females and males differ slightly in length – females are generally larger than males. The cell below generates length measurements for a hypothetical population of 3000 females and 2000 males.\n\n# for reproducibility\nnp.random.seed(40721)\n\n# simulate hypothetical population\nfemale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 57.5, scale = 3, size = 3000),\n            'sex': np.repeat('female', 3000)}\n)\n\nmale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 50.5, scale = 3, size = 2000),\n            'sex': np.repeat('male', 2000)}\n)\n\npopulation_hawks = pd.concat([female_hawks, male_hawks], axis = 0)\n\n# preview\npopulation_hawks.groupby('sex').head(2)\n\n\n\n\n\n  \n    \n      \n      length\n      sex\n    \n  \n  \n    \n      0\n      53.975230\n      female\n    \n    \n      1\n      60.516768\n      female\n    \n    \n      0\n      53.076663\n      male\n    \n    \n      1\n      49.933166\n      male\n    \n  \n\n\n\n\nThe cell below produces a histogram of the lengths in the population overall (bottom panel) and when distinguished by sex (top panel).\n\nbase = alt.Chart(population_hawks).properties(height = 200)\n\nhist = base.mark_bar(opacity = 0.5, color = 'red').encode(\n    x = alt.X('length', \n              bin = alt.Bin(maxbins = 40), \n              scale = alt.Scale(domain = (40, 70)),\n              title = 'length (cm)'),\n    y = alt.Y('count()', \n              stack = None,\n              title = 'number of birds')\n)\n\nhist_bysex = hist.encode(color = 'sex').properties(height = 100)\n\nhist_bysex & hist\n\n\n\n\n\n\nThe population mean – average length of both female and male red-tailed hawks – is shown below.\n\n# population mean\npopulation_hawks.mean(numeric_only = True)\n\nlength    54.737717\ndtype: float64\n\n\nFirst try drawing a random sample from the population:\n\n# for reproducibility\nnp.random.seed(40821)\n\n# randomly sample\nsample_hawks = population_hawks.sample(n = 300, replace = False)\n\n\nQuestion 8\nDo you expect that the sample will contain equal numbers of male and female hawks? Think about this for a moment (you don’t have to provide a written answer), and then compute the proportions of individuals in the sample of each sex and store the result as a dataframe named proportion_hawks_sample. The dataframe should have one column named proportion and two rows indexed by sex.\nHint: group by sex, use .count(), and divide by the sample size. Be sure to rename the output column appropriately, as the default behavior produces a column called length.\n\nproportion_hawks_sample = sample_hawks.groupby('sex').count().rename(columns = {'length': 'proportion'})/300 #SOLUTION\n\nproportion_hawks_sample\n\n\n\n\n\n  \n    \n      \n      proportion\n    \n    \n      sex\n      \n    \n  \n  \n    \n      female\n      0.596667\n    \n    \n      male\n      0.403333\n    \n  \n\n\n\n\n\ngrader.check(\"q8\")\n\nThe sample mean is shown below, and is fairly close to the population mean. This should be expected, since you already saw in scenario 1 that random sampling is an unbiased sampling design with respect to the mean.\n\nsample_hawks.mean(numeric_only = True)\n\nlength    54.952103\ndtype: float64"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#biased-sampling-1",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#biased-sampling-1",
    "title": "PSTAT100",
    "section": "Biased sampling",
    "text": "Biased sampling\nLet’s now consider a biased sampling design. Usually, length measurements are collected from dead specimens collected opportunistically. Imagine that male mortality is higher, so there are better chances of finding dead males than dead females. Suppose in particular that specimens are five times as likely to be male; to represent this situation, we’ll assign sampling weights of 5/6 to all male hawks and weights of 1/6 to all female hawks.\n\ndef weight_fn(sex, p = 5/6):\n    if sex == 'male':\n        out = p\n    else:\n        out = 1 - p\n    return out\n\nweight_df = pd.DataFrame(\n    {'length': [50.5, 57.5],\n     'weight': [5/6, 1/6],\n     'sex': ['male', 'female']})\n\nwt = alt.Chart(weight_df).mark_bar(opacity = 0.5).encode(\n    x = alt.X('length', scale = alt.Scale(domain = (40, 70))),\n    y = alt.Y('weight', scale = alt.Scale(domain = (0, 1))),\n    color = 'sex'\n).properties(height = 70)\n\nhist_bysex & wt\n\n\n\n\n\n\n\nQuestion 9\nDraw a weighted sample sample_hawks_biased from the population population_hawks using the weights defined by weight_fn, and compute and store the value of the sample mean as sample_hawks_biased_mean.\n\n# BEGIN SOLUTION NO PROMPT\n\n# for reproducibility\nnp.random.seed(40821)\n\n# assign weights\npopulation_hawks['weight'] = population_hawks.sex.aggregate(func = weight_fn)\n\n# randomly sample\nsample_hawks_biased = population_hawks.sample(n = 300, replace = False, weights = 'weight').loc[:, ['length', 'sex']]\n\n# compute mean\nsample_hawks_biased_mean = sample_hawks_biased.length.mean()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# for reproducibility\nnp.random.seed(40821)\n\n# assign weights\n\n# randomly sample\n\n# compute mean\nsample_hawks_biased_mean = ...\n\"\"\" # END PROMPT\n\nsample_hawks_biased_mean\n\n51.88710627046723\n\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10\nInvestigate the bias of the sample mean by:\n\ndrawing 1000 samples with observations weighted by weight_fn;\nstoring the sample mean from each sample as samp_means_hawks;\ncomputing the average difference between the sample means and the population mean and storing the resulting value as avg_diff_hawks.\n\n\n# BEGIN SOLUTION NO PROMPT\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means_hawks = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means_hawks[i] = population_hawks.sample(\n        n = 300, \n        replace = False, \n        weights = 'weight'\n    ).length.mean(numeric_only = True)\n\n# bias\navg_diff_hawks = samp_means_hawks.mean() - population_hawks.length.mean()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\n\n# storage for the sample means\nsamp_means_hawks = ...\n\n# repeatedly sample and store the sample mean in the samp_means array\n\n# bias\navg_diff_hawks = ...\n\"\"\" # END PROMPT\n\navg_diff_hawks\n\n-2.5720649894415715\n\n\n\ngrader.check(\"q10\")\n\n\n\n\nQuestion 11\nReflect a moment on your simulation result in question 3c. If instead female mortality is higher and specimens for measurement are collected opportunistically, as described in the previous sampling design, do you expect that the average length in the sample will be an underestimate or an overestimate of the population mean? Explain why in 1-2 sentences, and carry out a simulation to check your intuition.\nType your answer here, replacing this text.\nSOLUTION: If female mortality was higher, then it would be an overestimate.\n\n# BEGIN SOLUTION NO PROMPT\n\nnp.random.seed(40221) # for reproducibility\n\n# invert weights\npopulation_hawks['weight_inv'] = 1 - population_hawks.weight\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means_hawks = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means_hawks[i] = population_hawks.sample(\n        n = 300, \n        replace = False, \n        weights = 'weight_inv'\n    ).length.mean(numeric_only = True)\n\n# bias\nestimated_bias = samp_means_hawks.mean() - population_hawks.length.mean()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\nnp.random.seed(40221) # for reproducibility\n\n# invert weights\n\n# storage for the sample means\n\n# repeatedly sample and store the sample mean\n\n# compute bias\nestimated_bias = ...\n\"\"\" # END PROMPT\n\nestimated_bias\n\n1.9796123471243803"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html",
    "href": "labs/lab2-sampling/lab2-sampling.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab2-sampling.ipynb\")"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#sampling-designs",
    "href": "labs/lab2-sampling/lab2-sampling.html#sampling-designs",
    "title": "PSTAT100",
    "section": "Sampling designs",
    "text": "Sampling designs\nThe sampling design of a study refers to the way observational units are selected from the collection of all observational units. Any design can be expressed by the probability that each unit is included in the sample. In a random sample, all units are equally likely to be included.\nFor example, you might want to learn about U.S. residents (population), but only be able for ethical or practical reasons to study adults (sampling frame), and decide to do a mail survey of 2000 randomly selected addresses in each state (sampling design). Each collection of 2000 addresses may constitute a random sample of households, but even with a 100% response rate the survey results will not be a random sample of adult U.S. residents because individuals share addresses and the population sizes are different from state to state."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#bias",
    "href": "labs/lab2-sampling/lab2-sampling.html#bias",
    "title": "PSTAT100",
    "section": "Bias",
    "text": "Bias\nFormally, bias describes the ‘typical’ deviation of a sample statistic the correspongind population value.\nFor example, if a particular sampling design tends to produce an average measurement around 1.5 units, but the true average in the population is 2 units, then the estimate has a bias of -0.5 units. The language ‘typical’ and ‘tends to’ is important here. Estimates are never perfect, so just because an estimate is off by -0.5 units for one sample doesn’t make it biased – it is only biased if it is consistently off.\nAlthough bias is technically a property of a sample statistic (like the sample average), it’s common to talk about a biased sample – this term refers to a dataset collected using a sampling design that produces biased statistics.\nThis is exactly what you’ll explore in this lab – the relationship between sampling design and bias."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#simulated-data",
    "href": "labs/lab2-sampling/lab2-sampling.html#simulated-data",
    "title": "PSTAT100",
    "section": "Simulated data",
    "text": "Simulated data\nYou will be simulating data in this lab. Simulation is a great means of exploration because you can control the population properties, which are generally unknown in practice.\nWhen working with real data, you just have one dataset, and you don’t know any of the properties of the population or what might have happened if a different sample were collected. That makes it difficult to understand sampling variation and impossible to directly compare the sample properties to the population properties.\nWith simulated data, by contrast, you control how data are generated with exact precision – so by extension, you know everything there is to know about the population. In addition, repeated simulation of data makes it possible to explore the typical behavior of a particular sampling design, so you can learn ‘what usually happens’ for a particular sampling design by direct observation."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#hypothetical-population",
    "href": "labs/lab2-sampling/lab2-sampling.html#hypothetical-population",
    "title": "PSTAT100",
    "section": "Hypothetical population",
    "text": "Hypothetical population\nTo provide a little context to this scenario, imagine that you’re measuring eucalyptus seeds to determine their typical diameter. The cell below simulates diameter measurements for a hypothetical population of 5000 seeds; imagine that this is the total number of seeds in a small grove at some point in time.\n\n# simulate seed diameters\nnp.random.seed(40221) # for reproducibility\npopulation = pd.DataFrame(\n    data = {'diameter': np.random.gamma(shape = 2, scale = 1/2, size = 5000), \n            'seed': np.arange(5000)}\n).set_index('seed')\n\n# check first few rows\npopulation.head(3)\n\n\nQuestion 1\nCalculate the mean diameter for the hypothetical population and store the value as mean_diameter.\n\nmean_pop_diameter = ...\n\nmean_pop_diameter\n\n\ngrader.check(\"q1\")\n\n\n\nQuestion 2\nCalculate the standard deviation of diameters for the hypothetical population and store the value as std_dev_pop_diameter.\n\nstd_dev_pop_diameter = ...\nstd_dev_pop_diameter\n\n\ngrader.check(\"q2\")\n\nThe cell below produces a histogram of the population values – the distribution of diameter measurements among the hypothetical population – with a vertical line indicating the population mean.\n\n# base layer\nbase_pop = alt.Chart(population).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_pop = base_pop.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20), \n              title = 'Diameter (mm)', \n              scale = alt.Scale(domain = (0, 6))),\n    y = alt.Y('count()', title = 'Number of seeds in population')\n)\n\n# vertical line for population mean\nmean_pop = base_pop.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# display\nhist_pop + mean_pop"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#random-sampling",
    "href": "labs/lab2-sampling/lab2-sampling.html#random-sampling",
    "title": "PSTAT100",
    "section": "Random sampling",
    "text": "Random sampling\nImagine that your sampling design involves collecting bunches of plant material from several locations in the grove and sifting out the seeds with a fine sieve until you obtaining 250 seeds. We’ll suppose that using your collection method, any of the 5000 seeds is equally likely to be obtained, so that your 250 seeds comprise a random sample of the population.\nWe can simulate samples obtained using your hypothetical design by drawing values without replacement from the population.\n\n# draw a random sample of seeds\nnp.random.seed(40221) # for reproducibility\nsample = population.sample(n = 250, replace = False)\n\n\nQuestion 3\nCalculate the mean diameter of seeds in the simulated sample and store the value as mean_sample_diameter.\n\nmean_sample_diameter = ...\nmean_sample_diameter\n\n\ngrader.check(\"q3\")\n\nYou should see above that the sample mean is close to the population mean. In fact, all sample statistics are close to the population; this can be seen by comparing the distribution of sample values with the distribution of population values.\n\n# base layer\nbase_samp = alt.Chart(sample).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_samp = base_samp.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20),\n              scale = alt.Scale(domain = (0, 6)),\n              title = 'Diameter (mm)'),\n    y = alt.Y('count()', title = 'Number of seeds in sample')\n)\n\n# vertical line for population mean\nmean_samp = base_samp.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# display\nhist_samp + mean_samp | hist_pop + mean_pop\n\nWhile there are some small differences, the overall shape is similar and the sample mean is almost exactly the same as the population mean. So with this sampling design, you obtained a dataset with few distortions of the population properties, and the sample mean is a good estimate of the population mean.\n\n\nAssessing bias through simulation\nYou may wonder: does that happen all the time, or was this just a lucky draw? This question can be answered by simulating a large number of samples and checking the average behavior to see whether the undistorted representation of the population is typical for this sampling design.\nThe cell below estimates the bias of the sample mean by:\n\ndrawing 1000 samples of size 300;\nstoring the sample mean from each sample;\ncomputing the average difference between the sample means and the population mean.\n\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means[i] = population.sample(n = 250, replace = False).diameter.mean()\n\nThe bias of the sample mean is its average distance from the population mean. We can estimate this using our simulation results as follows:\n\n# bias\nsamp_means.mean() - population.diameter.mean()\n\nSo the average error observed in 1000 simulations was about 0.001 mm! This suggests that the sample mean is unbiased: on average, there is no error. Therefore, at least with respect to estimating the population mean, random samples appear to be unbiased samples.\nHowever, unbiasedness does not mean that you won’t observe estimation error. There is a natural amount of variability from sample to sample, because in each sample a different collection of seeds is measured. We can estimate this as well using the simulation results by checking the standard deviation of the sample means across all 1000 samples:\n\nsamp_means.std()\n\nSo on average, the sample mean varies by about 0.04 mm from sample to sample.\nWe could also check how much the sample mean deviates from the population mean on average by computing root mean squared error:\n\nnp.sqrt(np.sum((samp_means - population.diameter.mean())**2)/1000)\n\nNote that this is very close to the variance of the sample mean across simulations, but not exactly the same; this latter calculation measures the spread around the population mean, and is a conventional measure of estimation accuracy.\nThe cell below plots a histogram representing the distribution of values of the sample mean across the 1000 samples you simulated (this is known as the sampling distribution of the sample mean). It shows a peak right at the population mean (blue vertical line) but some symmetric variation to either side – most values are between about 0.93 and 1.12.\n\n# plot the simulated sampling distribution\nsampling_dist = alt.Chart(pd.DataFrame({'sample mean': samp_means})).mark_bar().encode(\n    x = alt.X('sample mean', bin = alt.Bin(maxbins = 30), title = 'Value of sample mean'),\n    y = alt.Y('count()', title = 'Number of simulations')\n)\n\nsampling_dist + mean_pop"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#biased-sampling",
    "href": "labs/lab2-sampling/lab2-sampling.html#biased-sampling",
    "title": "PSTAT100",
    "section": "Biased sampling",
    "text": "Biased sampling\nIn this scenario, you’ll use the same hypothetical population of eucalyptus seed diameter measurements and explore the impact of a biased sampling design.\nIn the first design, you were asked to imagine that you collected and sifted plant material to obtain seeds. Suppose you didn’t know that the typical seed is about 1mm in diameter and decided to use a sieve that is a little too coarse, tending only to sift out larger seeds and letting smaller seeds pass through. As a result, small seeds have a lower probability of being included in the sample and large seeds have a higher probability of being included in the sample.\nThis kind of sampling design can be described by assigning differential sampling weights \\(w_1, \\dots, w_N\\) to each observation. The cell below defines some hypothetical weights such that larger diameters are more likely to be sampled.\n\npopulation_mod1 = population.copy()\n\n\n# inclusion weight as a function of seed diameter\ndef weight_fn(x, r = 10, c = 1.5):\n    out = 1/(1 + np.e**(-r*(x - c)))\n    return out\n\n# create a grid of values to use in plotting the function\ngrid = np.linspace(0, 6, 100)\nweight_df = pd.DataFrame(\n    {'seed diameter': grid,\n     'weight': weight_fn(grid)}\n)\n\n# plot of inclusion probability against diameter\nweight_plot = alt.Chart(weight_df).mark_area(opacity = 0.3, line = True).encode(\n    x = 'seed diameter',\n    y = 'weight'\n).properties(height = 100)\n\n# show plot\nweight_plot\n\nThe actual probability that a seed is included in the sample – its inclusion probability – is proportional to the sampling weight. These inclusion probabilities \\(\\pi_i\\) can be calculated by normalizing the weights \\(w_i\\) over all seeds in the population \\(i = 1, \\dots, 5000\\):\n\\[\\pi_i = \\frac{w_i}{\\sum_i w_i}\\]\nIt may help you to picture how the weights will be used in sampling to line up this plot with the population distribution. In effect, we will sample more from the right tail of the population distribution, where the weight is nearest to 1.\n\nhist_pop & weight_plot\n\nThe following cell draws a sample with replacement from the hypothetical seed population with seeds weighted according to the inclusion probability given by the function above.\n\n# assign weight to each seed\npopulation_mod1['weight'] = weight_fn(population_mod1.diameter)\n\n# draw weighted sample\nnp.random.seed(40721)\nsample2 = population_mod1.sample(n = 250, replace = False, weights = 'weight').loc[:, ['diameter']]\n\n\nQuestion 4\nCalculate the mean diameter of seeds in the simulated sample and store the value as mean_sample2_diameter.\n\nmean_sample2_diameter = ...\n\nmean_sample2_diameter\n\n\ngrader.check(\"q4\")\n\n\n\n\nQuestion 5\nShow side-by-side plots of the distribution of sample values and the distribution of population values, with vertical lines indicating the corresponding mean on each plot.\nHint: copy the cell that produced this plot in scenario 1 and replace sample with sample2. Utilizing different methods is also welcome.\n\n# base layer\nbase_samp = ...\n\n# histogram of diameter measurements\nhist_samp = ...\n\n# vertical line for population mean\nmean_samp = ...\n\n# combine layers\n\n# display\nhist_samp + mean_samp | hist_pop + mean_pop\n\n\n\n\nAssessing bias through simulation\nHere you’ll mimic the simulation done in scenario 1 to assess the bias of the sample mean under this new sampling design.\n\npopulation_mod1.head()\n\n\n\nQuestion 6\nInvestigate the bias of the sample mean by:\n\ndrawing 1000 samples with observations weighted by inclusion probability;\nstoring the collection of sample means from each sample as samp_means;\ncomputing the average difference between the sample means and the population mean (in that order!) and storing the result as avg_diff.\n\n(Hint: copy the cell that performs this simulation in scenario 1, and be sure to replace population with population_mod1 and adjust the sampling step to include weights = ... with the appropriate argument.)\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\n\n# storage for the sample means\n\n# repeatedly sample and store the sample mean in the samp_means array\n\n# bias\n\navg_diff\n\n\ngrader.check(\"q6\")\n\n\n\n\nQuestion 7\nDoes this sampling design seem to introduce bias? If so, does the sample mean tend to over-estimate or under-estimate the population mean?\nType your answer here, replacing this text."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#hypothetical-population-1",
    "href": "labs/lab2-sampling/lab2-sampling.html#hypothetical-population-1",
    "title": "PSTAT100",
    "section": "Hypothetical population",
    "text": "Hypothetical population\nSuppose you’re interested in determining the average beak-to-tail length of red-tailed hawks to help differentiate them from other hawks by sight at a distance. Females and males differ slightly in length – females are generally larger than males. The cell below generates length measurements for a hypothetical population of 3000 females and 2000 males.\n\n# for reproducibility\nnp.random.seed(40721)\n\n# simulate hypothetical population\nfemale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 57.5, scale = 3, size = 3000),\n            'sex': np.repeat('female', 3000)}\n)\n\nmale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 50.5, scale = 3, size = 2000),\n            'sex': np.repeat('male', 2000)}\n)\n\npopulation_hawks = pd.concat([female_hawks, male_hawks], axis = 0)\n\n# preview\npopulation_hawks.groupby('sex').head(2)\n\nThe cell below produces a histogram of the lengths in the population overall (bottom panel) and when distinguished by sex (top panel).\n\nbase = alt.Chart(population_hawks).properties(height = 200)\n\nhist = base.mark_bar(opacity = 0.5, color = 'red').encode(\n    x = alt.X('length', \n              bin = alt.Bin(maxbins = 40), \n              scale = alt.Scale(domain = (40, 70)),\n              title = 'length (cm)'),\n    y = alt.Y('count()', \n              stack = None,\n              title = 'number of birds')\n)\n\nhist_bysex = hist.encode(color = 'sex').properties(height = 100)\n\nhist_bysex & hist\n\nThe population mean – average length of both female and male red-tailed hawks – is shown below.\n\n# population mean\npopulation_hawks.mean(numeric_only = True)\n\nFirst try drawing a random sample from the population:\n\n# for reproducibility\nnp.random.seed(40821)\n\n# randomly sample\nsample_hawks = population_hawks.sample(n = 300, replace = False)\n\n\nQuestion 8\nDo you expect that the sample will contain equal numbers of male and female hawks? Think about this for a moment (you don’t have to provide a written answer), and then compute the proportions of individuals in the sample of each sex and store the result as a dataframe named proportion_hawks_sample. The dataframe should have one column named proportion and two rows indexed by sex.\nHint: group by sex, use .count(), and divide by the sample size. Be sure to rename the output column appropriately, as the default behavior produces a column called length.\n\nproportion_hawks_sample = ...\n\nproportion_hawks_sample\n\n\ngrader.check(\"q8\")\n\nThe sample mean is shown below, and is fairly close to the population mean. This should be expected, since you already saw in scenario 1 that random sampling is an unbiased sampling design with respect to the mean.\n\nsample_hawks.mean(numeric_only = True)"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#biased-sampling-1",
    "href": "labs/lab2-sampling/lab2-sampling.html#biased-sampling-1",
    "title": "PSTAT100",
    "section": "Biased sampling",
    "text": "Biased sampling\nLet’s now consider a biased sampling design. Usually, length measurements are collected from dead specimens collected opportunistically. Imagine that male mortality is higher, so there are better chances of finding dead males than dead females. Suppose in particular that specimens are five times as likely to be male; to represent this situation, we’ll assign sampling weights of 5/6 to all male hawks and weights of 1/6 to all female hawks.\n\ndef weight_fn(sex, p = 5/6):\n    if sex == 'male':\n        out = p\n    else:\n        out = 1 - p\n    return out\n\nweight_df = pd.DataFrame(\n    {'length': [50.5, 57.5],\n     'weight': [5/6, 1/6],\n     'sex': ['male', 'female']})\n\nwt = alt.Chart(weight_df).mark_bar(opacity = 0.5).encode(\n    x = alt.X('length', scale = alt.Scale(domain = (40, 70))),\n    y = alt.Y('weight', scale = alt.Scale(domain = (0, 1))),\n    color = 'sex'\n).properties(height = 70)\n\nhist_bysex & wt\n\n\nQuestion 9\nDraw a weighted sample sample_hawks_biased from the population population_hawks using the weights defined by weight_fn, and compute and store the value of the sample mean as sample_hawks_biased_mean.\n\n# for reproducibility\nnp.random.seed(40821)\n\n# assign weights\n\n# randomly sample\n\n# compute mean\nsample_hawks_biased_mean = ...\n\nsample_hawks_biased_mean\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10\nInvestigate the bias of the sample mean by:\n\ndrawing 1000 samples with observations weighted by weight_fn;\nstoring the sample mean from each sample as samp_means_hawks;\ncomputing the average difference between the sample means and the population mean and storing the resulting value as avg_diff_hawks.\n\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\n\n# storage for the sample means\nsamp_means_hawks = ...\n\n# repeatedly sample and store the sample mean in the samp_means array\n\n# bias\navg_diff_hawks = ...\n\navg_diff_hawks\n\n\ngrader.check(\"q10\")\n\n\n\n\nQuestion 11\nReflect a moment on your simulation result in question 3c. If instead female mortality is higher and specimens for measurement are collected opportunistically, as described in the previous sampling design, do you expect that the average length in the sample will be an underestimate or an overestimate of the population mean? Explain why in 1-2 sentences, and carry out a simulation to check your intuition.\nType your answer here, replacing this text.\n\nnp.random.seed(40221) # for reproducibility\n\n# invert weights\n\n# storage for the sample means\n\n# repeatedly sample and store the sample mean\n\n# compute bias\nestimated_bias = ...\n\nestimated_bias"
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab3-visualization.ipynb\")"
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html#axes",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html#axes",
    "title": "PSTAT100",
    "section": "Axes",
    "text": "Axes\nAxes establish a reference system for a graphic: they define a space within which the graphic will be constructed. Usually these are coordinate systems defined at a particular scale, like Cartesian coordinates on the region (0, 100) x (0, 100), or polar coordinates on the unit circle, or geographic coordinates for the globe.\nIn Altair, axes are automatically determined based on encodings, but are customizable to an extent."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html#geometric-objects",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html#geometric-objects",
    "title": "PSTAT100",
    "section": "Geometric objects",
    "text": "Geometric objects\nGeometric objects are any objects superimposed on a set of axes: points, lines, polygons, circles, bars, arcs, curves, and the like. Often, visualizations are characterized according to the type of object used to display data – for example, the scatterplot consists of points, a bar plot consists of bars, a line plot consists of one or more lines, and so on.\nIn Altair, geometric objects are called marks."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html#aesthetic-attributes",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html#aesthetic-attributes",
    "title": "PSTAT100",
    "section": "Aesthetic attributes",
    "text": "Aesthetic attributes\nThe word ‘aesthetics’ is used in a variety of ways in relation to graphics; you will see this in your reading. For us, ‘aesthetic attirbutes’ will refer to attributes of geometric objects like color. The primary aesthetics in statistical graphics are color, opacity, shape, and size.\nIn Altair, aesthetic attributes are called mark properties."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html#text",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html#text",
    "title": "PSTAT100",
    "section": "Text",
    "text": "Text\nText is used in graphics to label axes, geometric objects, and legends for aesthetic mappings. Text specification is usually a step in customization for presentation graphics, but often skipped in exploratory graphics. Carefully chosen text is very important in this context, because it provides essential information that a general reader needs to interpret a plot.\nIn Altair, text is usually controlled as part of encoding specification."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html#basic-scatterplots",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html#basic-scatterplots",
    "title": "PSTAT100",
    "section": "Basic scatterplots",
    "text": "Basic scatterplots\nThe following cell constructs a scatterplot of life expectancy at birth against GDP per capita; each point corresponds to one country in one year. The syntax works as follows: * alt.Chart() begins by constructing a ‘chart’ object constructed from the dataframe; * the result is passed to .mark_circle(), which specifies a geometric object (circles) to add to the chart; * the result is passed to .encode(), which specifies which columns should be used to determine the coordinates of the circles.\n\n# basic scatterplot\nalt.Chart(data).mark_circle().encode(\n    x = 'GDP per capita',\n    y = 'Life Expectancy'\n)\n\n\n\n\n\n\n\n\nQuestion 1: Different marks\nThe cell below is a copy of the previous cell. Have a look at the documentation on marks for a list of the possible mark types. Try out a few alternatives to see what they look like. Once you’re satisfied, change the mark to points.\n\n# BEGIN SOLUTION NO PROMPT\n# basic scatterplot\nfig_q1 = alt.Chart(data).mark_point().encode( # change made here from circle to point\n    x = 'GDP per capita',\n    y = 'Life Expectancy'\n)\n# END SOLUTION\n\n\"\"\" # BEGIN PROMPT\n# basic scatterplot\nalt.Chart(data).mark_circle().encode( # tinker here with different marks\n    x = 'GDP per capita',\n    y = 'Life Expectancy'\n)\n\"\"\"; # END PROMPT\n\nfig_q1 # SOLUTION NO PROMPT\n\n\n\n\n\n\n\n\n\n\nQuestion 2: Mark properties\nWhat is the difference between points and circles, according to the documentation?\nType your answer here, replacing this text.\nSOLUTION: Points can have many shapes; circles can not."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html#axis-adjustments-with-alt.x-and-alt.y",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html#axis-adjustments-with-alt.x-and-alt.y",
    "title": "PSTAT100",
    "section": "Axis adjustments with alt.X() and alt.Y()",
    "text": "Axis adjustments with alt.X() and alt.Y()\nAn initial problem that would be good to resolve before continuing is that the y axis label isn’t informative. Let’s change that by wrapping the column to encode in alt.Y() and specifying the title manually.\n\n# change axis label\nalt.Chart(data).mark_circle().encode(\n    x = 'GDP per capita',\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth')\n)\n\n\n\n\n\n\nalt.Y() and alt.X() are helper functions that modify encoding specifications. The cell below adjusts the scale of the y axis as well; since above there are no life expectancies below 30, starting the y axis at 0 adds whitespace.\n\n# don't start y axis at zero\nalt.Chart(data).mark_circle().encode(\n    x = 'GDP per capita',\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False))\n)\n\n\n\n\n\n\nIn the plot above, there are a lot of points squished together near \\(x = 0\\). It will make it easier to see the pattern of scatter in that region to adjust the x axis so that values are not displayed on a linear scale. Using alt.Scale() allows for efficient axis rescaling; the cell below puts GDP per capita on a log scale.\n\n# log scale for x axis\nalt.Chart(data).mark_circle().encode(\n    x = alt.X('GDP per capita', scale = alt.Scale(type = 'log')),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False))\n)\n\n\n\n\n\n\n\n\nQuestion 3: Changing axis scale\nTry a different scale by modifying the type = ... argument of alt.Scale in the cell below. Look at the altair documentation for a list of the possible types.\n\n# try another axis scale\nalt.Chart(data).mark_circle().encode(\n    x = alt.X('GDP per capita', scale = alt.Scale(type = 'sqrt')), # SOLUTION\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False))\n)"
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization-soln.html#submission-checklist",
    "href": "labs/lab3-visualization/lab3-visualization-soln.html#submission-checklist",
    "title": "PSTAT100",
    "section": "Submission Checklist",
    "text": "Submission Checklist\n\nSave file to confirm all changes are on disk\nRun Kernel > Restart & Run All to execute all code from top to bottom\nSave file again to write any new output to disk\nSelect File > Download as > HTML.\nOpen in Google Chrome and print to PDF.\nSubmit to Gradescope"
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html",
    "href": "labs/lab3-visualization/lab3-visualization.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab3-visualization.ipynb\")"
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html#axes",
    "href": "labs/lab3-visualization/lab3-visualization.html#axes",
    "title": "PSTAT100",
    "section": "Axes",
    "text": "Axes\nAxes establish a reference system for a graphic: they define a space within which the graphic will be constructed. Usually these are coordinate systems defined at a particular scale, like Cartesian coordinates on the region (0, 100) x (0, 100), or polar coordinates on the unit circle, or geographic coordinates for the globe.\nIn Altair, axes are automatically determined based on encodings, but are customizable to an extent."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html#geometric-objects",
    "href": "labs/lab3-visualization/lab3-visualization.html#geometric-objects",
    "title": "PSTAT100",
    "section": "Geometric objects",
    "text": "Geometric objects\nGeometric objects are any objects superimposed on a set of axes: points, lines, polygons, circles, bars, arcs, curves, and the like. Often, visualizations are characterized according to the type of object used to display data – for example, the scatterplot consists of points, a bar plot consists of bars, a line plot consists of one or more lines, and so on.\nIn Altair, geometric objects are called marks."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html#aesthetic-attributes",
    "href": "labs/lab3-visualization/lab3-visualization.html#aesthetic-attributes",
    "title": "PSTAT100",
    "section": "Aesthetic attributes",
    "text": "Aesthetic attributes\nThe word ‘aesthetics’ is used in a variety of ways in relation to graphics; you will see this in your reading. For us, ‘aesthetic attirbutes’ will refer to attributes of geometric objects like color. The primary aesthetics in statistical graphics are color, opacity, shape, and size.\nIn Altair, aesthetic attributes are called mark properties."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html#text",
    "href": "labs/lab3-visualization/lab3-visualization.html#text",
    "title": "PSTAT100",
    "section": "Text",
    "text": "Text\nText is used in graphics to label axes, geometric objects, and legends for aesthetic mappings. Text specification is usually a step in customization for presentation graphics, but often skipped in exploratory graphics. Carefully chosen text is very important in this context, because it provides essential information that a general reader needs to interpret a plot.\nIn Altair, text is usually controlled as part of encoding specification."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html#basic-scatterplots",
    "href": "labs/lab3-visualization/lab3-visualization.html#basic-scatterplots",
    "title": "PSTAT100",
    "section": "Basic scatterplots",
    "text": "Basic scatterplots\nThe following cell constructs a scatterplot of life expectancy at birth against GDP per capita; each point corresponds to one country in one year. The syntax works as follows: * alt.Chart() begins by constructing a ‘chart’ object constructed from the dataframe; * the result is passed to .mark_circle(), which specifies a geometric object (circles) to add to the chart; * the result is passed to .encode(), which specifies which columns should be used to determine the coordinates of the circles.\n\n# basic scatterplot\nalt.Chart(data).mark_circle().encode(\n    x = 'GDP per capita',\n    y = 'Life Expectancy'\n)\n\n\n\nQuestion 1: Different marks\nThe cell below is a copy of the previous cell. Have a look at the documentation on marks for a list of the possible mark types. Try out a few alternatives to see what they look like. Once you’re satisfied, change the mark to points.\n\n\n# basic scatterplot\nalt.Chart(data).mark_circle().encode( # tinker here with different marks\n    x = 'GDP per capita',\n    y = 'Life Expectancy'\n)\n\n\n\n\n\nQuestion 2: Mark properties\nWhat is the difference between points and circles, according to the documentation?\nType your answer here, replacing this text."
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html#axis-adjustments-with-alt.x-and-alt.y",
    "href": "labs/lab3-visualization/lab3-visualization.html#axis-adjustments-with-alt.x-and-alt.y",
    "title": "PSTAT100",
    "section": "Axis adjustments with alt.X() and alt.Y()",
    "text": "Axis adjustments with alt.X() and alt.Y()\nAn initial problem that would be good to resolve before continuing is that the y axis label isn’t informative. Let’s change that by wrapping the column to encode in alt.Y() and specifying the title manually.\n\n# change axis label\nalt.Chart(data).mark_circle().encode(\n    x = 'GDP per capita',\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth')\n)\n\nalt.Y() and alt.X() are helper functions that modify encoding specifications. The cell below adjusts the scale of the y axis as well; since above there are no life expectancies below 30, starting the y axis at 0 adds whitespace.\n\n# don't start y axis at zero\nalt.Chart(data).mark_circle().encode(\n    x = 'GDP per capita',\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False))\n)\n\nIn the plot above, there are a lot of points squished together near \\(x = 0\\). It will make it easier to see the pattern of scatter in that region to adjust the x axis so that values are not displayed on a linear scale. Using alt.Scale() allows for efficient axis rescaling; the cell below puts GDP per capita on a log scale.\n\n# log scale for x axis\nalt.Chart(data).mark_circle().encode(\n    x = alt.X('GDP per capita', scale = alt.Scale(type = 'log')),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False))\n)\n\n\n\nQuestion 3: Changing axis scale\nTry a different scale by modifying the type = ... argument of alt.Scale in the cell below. Look at the altair documentation for a list of the possible types.\n\n# try another axis scale\nalt.Chart(data).mark_circle().encode(\n    x = ...\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False))\n)"
  },
  {
    "objectID": "labs/lab3-visualization/lab3-visualization.html#submission-checklist",
    "href": "labs/lab3-visualization/lab3-visualization.html#submission-checklist",
    "title": "PSTAT100",
    "section": "Submission Checklist",
    "text": "Submission Checklist\n\nSave file to confirm all changes are on disk\nRun Kernel > Restart & Run All to execute all code from top to bottom\nSave file again to write any new output to disk\nSelect File > Download as > HTML.\nOpen in Google Chrome and print to PDF.\nSubmit to Gradescope"
  },
  {
    "objectID": "miscellany.html",
    "href": "miscellany.html",
    "title": "Miscellany",
    "section": "",
    "text": "Automated tests in assignment notebooks are a guide, not a confirmation or refutation of your answer. Don’t rely too heavily on them, but do read the output message if they fail and think about what the message is telling you. On some occasions they will fail despite a correct answer; on others they will pass despite an incorrect answer. Furthermore, they will guide you to one particular strategy for obtaining the solution; most problems admit a few possible strategies.\nAll assignments are due on Mondays. You get two free late assignments. Late submissions are due Wednesdays.\nStart your homeworks and mini-projects early.\nTake your own notes during class; don’t simply rely on lecture slides."
  },
  {
    "objectID": "miscellany.html#troubleshooting",
    "href": "miscellany.html#troubleshooting",
    "title": "Miscellany",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nIf you need to recover the distribution copy of any assignment notebook, perhaps due to accidentally deleting cells or similar issues, simply rename the notebook containing your work on the LSIT server and then redeploy the notebook from the course website link.\nIf you try to open a notebook and the server fails at the ‘synchronizing git repository’ stage, open the LSIT server separately, rename the pstat100-content directory, and then try opening the notebook from the website link again. If successfull, you will need to migrate all of your previous work into the new pstat100-content directory."
  },
  {
    "objectID": "projects/mp1/mp1-airquality-soln.html",
    "href": "projects/mp1/mp1-airquality-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# packages\nimport numpy as np\nimport pandas as pd\n\n# read in raw data file\nair_raw = pd.read_csv('air-raw.csv')\n\n# split off city info\ncbsa_info = air_raw.iloc[:, 0:2].dropna().set_index('CBSA')\ncbsa_info.to_csv('cbsa-info.csv')\n\n# remove city, state from air quality data\nair_quality = air_raw.drop(columns = 'Core Based Statistical Area').set_index('CBSA')\nair_quality.to_csv('air-quality.csv')"
  },
  {
    "objectID": "projects/mp1/mp1-airquality-soln.html#part-i-dataset",
    "href": "projects/mp1/mp1-airquality-soln.html#part-i-dataset",
    "title": "PSTAT100",
    "section": "Part I: Dataset",
    "text": "Part I: Dataset\nMerge the city information with the air quality data and tidy the dataset (see notes below). Write a brief description of the data.\nIn your description, answer the following questions:\n\nWhat is a CBSA (the geographic unit of measurement)?\nHow many CBSA’s are included in the data?\nIn how many states and territories do the CBSA’s reside?\nIn which years were data values recorded?\nHow many observations are recorded?\nHow many variables are measured?\nWhich variables are non-missing most of the time (i.e., in at least 50% of instances)?\nWhat is PM 2.5 and why is it important?\n\nPlease write your description in narrative fashion; please do not list answers to the questions above one by one. A few brief paragraphs should suffice; please limit your data description to three paragraphs or less.\n\nAir quality data\nWrite your description here."
  },
  {
    "objectID": "projects/mp1/mp1-airquality-soln.html#part-ii-descriptive-analysis",
    "href": "projects/mp1/mp1-airquality-soln.html#part-ii-descriptive-analysis",
    "title": "PSTAT100",
    "section": "Part II: Descriptive analysis",
    "text": "Part II: Descriptive analysis\nFocus on the PM2.5 measurements that are non-missing most of the time. Answer each of the following questions in a brief paragraph or two. Your paragraph(s) should indicate both your answer and a description of how you obtained it; please do not include codes with your answers.\n\nHas PM 2.5 air pollution improved in the average U.S. city since 2000?\nWrite your answer here.\n\n\nOver time, has PM 2.5 pollution become more variable, less variable, or about the same from city to city?\nWrite your answer here.\n\n\nWhich state has seen the greatest improvement over time?\nWrite your answer here.\n\n\nChoose a location with some meaning to you (e.g. hometown, family lives there, took a vacation there, etc.). Was that location in compliance with EPA primary standards as of the most recent measurement?\nWrite your answer here."
  },
  {
    "objectID": "projects/mp1/mp1-airquality.html",
    "href": "projects/mp1/mp1-airquality.html",
    "title": "PSTAT100",
    "section": "",
    "text": "In a way, this project is simple: you are given some data on air quality in U.S. metropolitan areas over time together with several questions of interest, and your objective is to answer the questions.\nHowever, unlike the homeworks and labs, there is no explicit instruction provided about how to answer the questions or where exactly to begin. Thus, you will need to discern for yourself how to manipulate and summarize the data in order to answer the questions of interest, and you will need to write your own codes from scratch to obtain results. It is recommended that you examine the data, consider the questions, and plan a rough approach before you begin doing any computations.\nYou have some latitude for creativity: although there are accurate answers to each question – namely, those that are consistent with the data – there is no singularly correct answer. Most students will perform similar operations and obtain similar answers, but there’s no specific result that must be considered to answer the questions accurately. As a result, your approaches and answers may differ from those of your classmates. If you choose to discuss your work with others, you may even find that disagreements prove to be fertile learning opportunities.\nThe questions can be answered using computing skills taught in class so far and basic internet searches for domain background; for this project, you may wish to refer to HW1 and Lab1 for code examples and the EPA website on PM pollution for background. However, you are also encouraged to refer to external resources (package documentation, vignettes, stackexchange, internet searches, etc.) as needed – this may be an especially good idea if you find yourself thinking, ‘it would be really handy to do X, but I haven’t seen that in class anywhere’.\nThe broader goal of these mini projects is to cultivate your problem-solving ability in an unstructured setting. Your work will be evaluated based on the following: - choice of method(s) used to answer questions; - clarity of presentation; - code style and documentation.\nPlease write up your results separately from your codes; codes should be included at the end of the notebook.\n\n\n\nMerge the city information with the air quality data and tidy the dataset (see notes below). Write a one- to two-paragraph description of the data.\nIn your description, answer the following questions:\n\nWhat is a CBSA (the geographic unit of measurement)?\nHow many CBSA’s are included in the data?\nIn how many states and territories do the CBSA’s reside? (Hint: str.split())\nIn which years were data values recorded?\nHow many observations are recorded?\nHow many variables are measured?\nWhich variables are non-missing most of the time (i.e., in at least 50% of instances)?\nWhat is PM 2.5 and why is it important?\nWhat are the basic statistical properties of the variable(s) of interest?\n\nPlease write your description in narrative fashion; please do not list answers to the questions above one by one.\n\n\nWrite your description here.\n\n\n\n\nFocus on the PM2.5 measurements that are non-missing most of the time. Answer each of the following questions in a brief paragraph or two. Do not describe your analyses step-by-step for your answers; instead, report your findings. Your paragraph(s) should indicate both your answer to the question and a justification for your answer; please do not include codes with your answers.\n\n\nWrite your answer here.\n\n\n\nWrite your answer here.\n\n\n\nWrite your answer here. Be sure to explain how you defined ‘best improvement’ in each case.\n\n\n\nWrite your answer here.\n\n\n\n\nOne strategy for filling in missing values (‘imputation’) is to use non-missing values to predict the missing ones; the success of this strategy depends in part on the strength of relationship between the variable(s) used as predictors of missing values.\nIdentify one other pollutant that might be a good candidate for imputation based on the PM 2.5 measurements and explain why you selected the variable you did. Can you envision any potential pitfalls to this technique?"
  },
  {
    "objectID": "projects/mp1/mp1-airquality.html#notes-on-merging-keep-at-bottom-of-notebook",
    "href": "projects/mp1/mp1-airquality.html#notes-on-merging-keep-at-bottom-of-notebook",
    "title": "PSTAT100",
    "section": "Notes on merging (keep at bottom of notebook)",
    "text": "Notes on merging (keep at bottom of notebook)\nTo combine datasets based on shared information, you can use the pd.merge(A, B, how = ..., on = SHARED_COLS) function, which will match the rows of A and B based on the shared columns SHARED_COLS. If how = 'left', then only rows in A will be retained in the output (so B will be merged to A); conversely, if how = 'right', then only rows in B will be retained in the output (so A will be merged to B).\nA simple example of the use of pd.merge is illustrated below:\n\n# toy data frames\nA = pd.DataFrame(\n    {'shared_col': ['a', 'b', 'c'], \n    'x1': [1, 2, 3], \n    'x2': [4, 5, 6]}\n)\n\nB = pd.DataFrame(\n    {'shared_col': ['a', 'b'], \n    'y1': [7, 8]}\n)\n\n\nA\n\n\nB\n\nBelow, if A and B are merged retaining the rows in A, notice that a missing value is input because B has no row where the shared column (on which the merging is done) has value c. In other words, the third row of A has no match in B.\n\n# left join\npd.merge(A, B, how = 'left', on = 'shared_col')\n\nIf the direction of merging is reversed, and the row structure of B is dominant, then the third row of A is dropped altogether because it has no match in B.\n\n# right join\npd.merge(A, B, how = 'right', on = 'shared_col')"
  },
  {
    "objectID": "slides/week1-intro.html#attendance-form",
    "href": "slides/week1-intro.html#attendance-form",
    "title": "Course introduction",
    "section": "Attendance form",
    "text": "Attendance form"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-1-ace-and-health",
    "href": "slides/week1-intro.html#case-study-1-ace-and-health",
    "title": "Course introduction",
    "section": "Case study 1: ACE and health",
    "text": "Case study 1: ACE and health\n\nAssociation between adverse childhood experiences and general health, by sex."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-1-ace-and-health-1",
    "href": "slides/week1-intro.html#case-study-1-ace-and-health-1",
    "title": "Course introduction",
    "section": "Case study 1: ACE and health",
    "text": "Case study 1: ACE and health\nYou will:\n\nprocess and recode 10K survey responses from CDC’s 2019 behavior risk factor surveillance survey (BRFSS)\ncross-tabulate health-related measurements with frequency of adverse childhood experiences"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-2-seda",
    "href": "slides/week1-intro.html#case-study-2-seda",
    "title": "Course introduction",
    "section": "Case study 2: SEDA",
    "text": "Case study 2: SEDA\n\nEducation achievement gaps as functions of socioeconomic indicators, by gender."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-2-seda-1",
    "href": "slides/week1-intro.html#case-study-2-seda-1",
    "title": "Course introduction",
    "section": "Case study 2: SEDA",
    "text": "Case study 2: SEDA\nYou will:\n\nmerge test scores and socioeconomic indicators from the 2018 Standford Education Data Archive by school district\nvisually assess correlations between gender achievement gaps among grade schoolers and socioeconomic indicators across school districts in CA"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\n\nSea surface temperature reconstruction over the past 16,000 years."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology-1",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology-1",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\n\nClustering of diatom relative abundances in pleistocene (pre-11KyBP) vs. holocene (post-11KyBP) epochs."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology-2",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology-2",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\nYou will:\n\nexplore ecological community structure from relative abundances of diatoms measured in ocean sediment core samples spanning ~15,000 years\nuse dimension reduction techniques to obtain measures of community structure\nidentify shifts associated with the transition from pleistocene to holocene epochs"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nApparent disparity in allocation of DDS benefits across racial groups."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-1",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-1",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nExpenditure is strongly associated with age."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-2",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-2",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nCorrecting for age shows comparable expenditure across racial groups."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-3",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-3",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\nYou will:\n\nassess the case for discrimination in allocation of DDS benefits\nidentify confounding factors present in the sample\nmodel median expenditure by racial group after correcting for age"
  },
  {
    "objectID": "slides/week1-intro.html#scope",
    "href": "slides/week1-intro.html#scope",
    "title": "Course introduction",
    "section": "Scope",
    "text": "Scope\nThis course is about developing your data science toolkit with foundational skills:\n\nCore competency with Python data science libraries\nCritical thinking about data\nVisualization and exploratory analysis\nApplication of basic statistical concepts and methods in practice\nCommunication and interpretation of results"
  },
  {
    "objectID": "slides/week1-intro.html#whats-unique-about-pstat100",
    "href": "slides/week1-intro.html#whats-unique-about-pstat100",
    "title": "Course introduction",
    "section": "What’s unique about PSTAT100?",
    "text": "What’s unique about PSTAT100?\nThere are a few distinctive aspects:\n\nmultiple end-to-end case studies\nquestion-driven rather than method-driven\nemphasis on project workflow\ndata storytelling and communication"
  },
  {
    "objectID": "slides/week1-intro.html#limitations",
    "href": "slides/week1-intro.html#limitations",
    "title": "Course introduction",
    "section": "Limitations",
    "text": "Limitations\nThere are also some things we won’t cover:\n\nPredictive modeling or machine learning\nAlgorithm design and implementation\nTechniques and methods for big data\nTheoretical basis for methods"
  },
  {
    "objectID": "slides/week1-intro.html#weekly-pattern",
    "href": "slides/week1-intro.html#weekly-pattern",
    "title": "Course introduction",
    "section": "Weekly Pattern",
    "text": "Weekly Pattern\nWe’ll follow a simple weekly pattern:\n\nMondays\n\nLecture\nSections\nAssignments due 11:59pm PST\n\nWednesdays\n\nLecture\nLate work due 11:59pm PST"
  },
  {
    "objectID": "slides/week1-intro.html#course-pages-materials",
    "href": "slides/week1-intro.html#course-pages-materials",
    "title": "Course introduction",
    "section": "Course pages & materials",
    "text": "Course pages & materials\n\nMaterials via course website ruizt.github.io/pstat100\nComputing at pstat100.lsit.ucsb.edu\nAssignments/gradebook at Gradescope\nDiscussion board (TBA)"
  },
  {
    "objectID": "slides/week1-intro.html#tentative-schedule",
    "href": "slides/week1-intro.html#tentative-schedule",
    "title": "Course introduction",
    "section": "Tentative schedule",
    "text": "Tentative schedule\n\n\n\nWeek\nTopic\nLab\nHomework\nProject\n\n\n\n\n1\nData science life cycle\n\n\n\n\n\n2\nTidy data\nL0\n\n\n\n\n3\nSampling and bias\nL1\n\n\n\n\n4\nStatistical graphics\nL2\nH1\n\n\n\n5\nKernel density estimation\nL3\n\nMP1\n\n\n6\nPrincipal components\nL4\nH2\n\n\n\n7\nSimple regression\n\n\nMP2\n\n\n8\nMultiple regression\nL5\nH3\n\n\n\n9\nClassification and clustering\n\n\nCP1\n\n\n10\nCase study\n\nH4\n\n\n\n11\nFinals week\n\n\nCP2"
  },
  {
    "objectID": "slides/week1-intro.html#assessments",
    "href": "slides/week1-intro.html#assessments",
    "title": "Course introduction",
    "section": "Assessments",
    "text": "Assessments\n\nLabs introduce and develop core skills\nHomeworks apply core skills to case studies\nProjects practice creative problem-solving"
  },
  {
    "objectID": "slides/week1-intro.html#policies",
    "href": "slides/week1-intro.html#policies",
    "title": "Course introduction",
    "section": "Policies",
    "text": "Policies\n\nCommunication\n\nIf you have questions, please come to office hours\nAvoid email except for personal matters\n\nDeadlines and late work\n\nOne-hour grace period on all deadlines\n48-hour late submissions\nTwo free lates on any assignment (except last assignment)\n75% partial credit thereafter for late work"
  },
  {
    "objectID": "slides/week1-intro.html#policies-1",
    "href": "slides/week1-intro.html#policies-1",
    "title": "Course introduction",
    "section": "Policies",
    "text": "Policies\n\nGrades\n\nRoughly 10-20-30-40 attendance-labs-homeworks-projects\nFinal weighting and grade assignment at instructor’s discretion\nDo not expect 92+% = A, 90-92% = A-, 87-89.9 = B+, etc.\nA’s are awarded sparingly and indicate exceptional work"
  },
  {
    "objectID": "slides/week1-intro.html#other-info",
    "href": "slides/week1-intro.html#other-info",
    "title": "Course introduction",
    "section": "Other info",
    "text": "Other info\n\nInformal section swaps are allowed with TA permission\nAttendance required at all class meetings, but a few absences without notice are okay\nHonors contracts not available this quarter\nOffice hours start week 2, check website for schedule"
  },
  {
    "objectID": "slides/week1-intro.html#getting-started",
    "href": "slides/week1-intro.html#getting-started",
    "title": "Course introduction",
    "section": "Getting started",
    "text": "Getting started\n\nLab this week will introduce you to computing and course infrastructure\nPlease fill out intake survey ASAP\nCheck access to Gradescope, LSIT, course page\nReview syllabus"
  },
  {
    "objectID": "slides/week1-lifecycle.html#whats-data-science",
    "href": "slides/week1-lifecycle.html#whats-data-science",
    "title": "Data science lifecycle",
    "section": "What’s data science?",
    "text": "What’s data science?\nData science is a term of art encompassing a wide range of activities that involve uncovering insights from quantitative information.\n\nPeople that refer to themselves as data scientists typically combine specific interests (“domain knowledge”, e.g., biology) with computation, mathematics, and statistics and probability to contribute to knowledge in their communities.\n\nIntersectional in nature\nNo singular disciplinary background among practitioners"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-science-lifecycle",
    "href": "slides/week1-lifecycle.html#data-science-lifecycle",
    "title": "Data science lifecycle",
    "section": "Data science lifecycle",
    "text": "Data science lifecycle\n\nData science lifecycle: an end-to-end process resulting in a data analysis product\n\n\nQuestion formulation\nData collection and cleaning\nExploration\nAnalysis\n\n\nThese form a cycle in the sense that the steps are iterated for question refinement and futher discovery."
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-science-lifecylce",
    "href": "slides/week1-lifecycle.html#data-science-lifecylce",
    "title": "Data science lifecycle",
    "section": "Data science lifecylce",
    "text": "Data science lifecylce\n\n\nThe point isn’t really the exact steps, but rather the notion of an iterative process."
  },
  {
    "objectID": "slides/week1-lifecycle.html#starting-with-a-question",
    "href": "slides/week1-lifecycle.html#starting-with-a-question",
    "title": "Data science lifecycle",
    "section": "Starting with a question",
    "text": "Starting with a question\nThe scaling of brains with bodies is thought to contain clues about evolutionary patterns pertaining to intelligence.\n\nThere are lots of datasets out there with brain and body weight measurements, so let’s consider the question:\n\nWhat is the relationship between an animal’s brain and body weight?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-acquisition",
    "href": "slides/week1-lifecycle.html#data-acquisition",
    "title": "Data science lifecycle",
    "section": "Data acquisition",
    "text": "Data acquisition\nFrom Allison et al. 1976, average body and brain weights for 62 mammals.\n\n\n\n\n\n\n  \n    \n      \n      species\n      body_wt\n      brain_wt\n    \n  \n  \n    \n      0\n      Africanelephant\n      6654.000\n      5712.0\n    \n    \n      1\n      Africangiantpouchedrat\n      1.000\n      6.6\n    \n    \n      2\n      ArcticFox\n      3.385\n      44.5\n    \n  \n\n\n\n\nUnits of measurement\n\nbody weight in kilograms\nbrain weight in grams"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment",
    "href": "slides/week1-lifecycle.html#data-assessment",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nHow well-matched is the data to our question?\n\nMammals only (no birds, fish, reptiles, etc.)\nSpecies are those for which convenient specimens were available\nAverages across specimens are reported (‘aggregated’ data)\n\n\nWhat do you think? Take a moment to discuss with your neighbor."
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment-1",
    "href": "slides/week1-lifecycle.html#data-assessment-1",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nBased on the great points you just made, we really only stand to learn something about this particular sample of animals.\n\nIn other words, no inference is possible.\n\n\n\nDo you think the data are still useful?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#inpection",
    "href": "slides/week1-lifecycle.html#inpection",
    "title": "Data science lifecycle",
    "section": "Inpection",
    "text": "Inpection\nThis dataset is already impeccably neat: each row is an observation for some species of mammal, and the columns are the two variables (average weight).\nSo no tidying needed – we’ll just check the dimensions and see if any values are missing.\n\n# dimensions?\nbb_weights.shape\n\n(62, 3)\n\n\n\n# missing values?\nbb_weights.isna().sum(axis = 0)\n\nspecies     0\nbody_wt     0\nbrain_wt    0\ndtype: int64"
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration",
    "href": "slides/week1-lifecycle.html#exploration",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nVisualization is usually a good starting point for exploring data.\n\n\n\n\n\n\n\nNotice the apparent density of points near \\((0, 0)\\) – that suggests we shouldn’t look for a relationship on the scale of kg/g."
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration-1",
    "href": "slides/week1-lifecycle.html#exploration-1",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nA simple transformation of the axes reveals a clearer pattern."
  },
  {
    "objectID": "slides/week1-lifecycle.html#analysis",
    "href": "slides/week1-lifecycle.html#analysis",
    "title": "Data science lifecycle",
    "section": "Analysis",
    "text": "Analysis\nThe plot shows us that there’s a roughly linear relationship on the log scale:\n\\[\\log(\\text{brain}) = \\alpha \\log(\\text{body}) + c\\]\n\nSo what does that mean in terms of brain and body weights? A little algebra and we have a “power law”:\n\\[(\\text{brain}) \\propto (\\text{body})^\\alpha\\]\n\n\nCheck your understanding: what’s the proportionality constant?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#interpretation",
    "href": "slides/week1-lifecycle.html#interpretation",
    "title": "Data science lifecycle",
    "section": "Interpretation",
    "text": "Interpretation\nSo it appears that the brain-body scaling is well-described by a power law:\n\namong selected specimens of these 62 species of mammal, species average brain weight is approximately proportional to a power of species average body weight\n\n\nNotice that I did not say:\n\nanimals’ brains are proportional to a power of their bodies\namong these 62 mammals, average brain weight is approximately proportional to a power of average body weight"
  },
  {
    "objectID": "slides/week1-lifecycle.html#question-refinement",
    "href": "slides/week1-lifecycle.html#question-refinement",
    "title": "Data science lifecycle",
    "section": "Question refinement",
    "text": "Question refinement\nWe can now ask further, more specific questions:\n\nDo other types of animals exhibit the same power law relationship?\n\n\nTo investigate, we need richer data."
  },
  {
    "objectID": "slides/week1-lifecycle.html#more-data-acquisition",
    "href": "slides/week1-lifecycle.html#more-data-acquisition",
    "title": "Data science lifecycle",
    "section": "(More) data acquisition",
    "text": "(More) data acquisition\nA number of authors have compiled and published ‘meta-analysis’ datasets by combining the results of multiple studies.\nBelow we’ll import a few of these for three different animal classes.\n\n# import metaanalysis datasets\nreptiles = pd.read_csv('data/reptile_meta.csv')\nbirds = pd.read_csv('data/bird_meta.csv', encoding = 'latin1')\nmammals = pd.read_csv('data/mammal_meta.csv', encoding = 'latin1')"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment-2",
    "href": "slides/week1-lifecycle.html#data-assessment-2",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nWhere does this data come from? It’s kind of a convenience sample of scientific data:\n\nMultiple studies \\(\\rightarrow\\) possibly different sampling and measurement protocols\nCriteria for inclusion unknown \\(\\rightarrow\\) probably neither comprehensive nor representative of all such measurements taken\n\n\nSo these data, while richer, are still relatively narrow in terms of generalizability."
  },
  {
    "objectID": "slides/week1-lifecycle.html#a-comment-on-scope-of-inference",
    "href": "slides/week1-lifecycle.html#a-comment-on-scope-of-inference",
    "title": "Data science lifecycle",
    "section": "A comment on scope of inference",
    "text": "A comment on scope of inference\nThese data don’t support general inferences (e.g., to all animals, all mammals, etc.) because they weren’t collected for the purpose to which we’re putting them.\n\nUsually, if data are not collected for the explicit purpose of the question you’re trying to answer, they won’t constitute a representative sample."
  },
  {
    "objectID": "slides/week1-lifecycle.html#tidying",
    "href": "slides/week1-lifecycle.html#tidying",
    "title": "Data science lifecycle",
    "section": "Tidying",
    "text": "Tidying\nBack to the task at hand, in order to comine the datasets one must:\n\nSelect columns of interest;\nPut in consistent order;\nGive consistent names;\nConcatenate row-wise.\n\n\nWe’ll skip the details for now."
  },
  {
    "objectID": "slides/week1-lifecycle.html#inspection",
    "href": "slides/week1-lifecycle.html#inspection",
    "title": "Data science lifecycle",
    "section": "Inspection",
    "text": "Inspection\nThis dataset has quite a lot of missing brain weight measurements: many of the studies combined to form these datasets did not include that particular measurement.\n\n# missing values?\ndata.isna().mean(axis = 0)\n\nOrder      0.00000\nFamily     0.00000\nGenus      0.00000\nSpecies    0.00000\nSex        0.00000\nbody       0.00000\nbrain      0.57404\nclass      0.00000\ndtype: float64"
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration-2",
    "href": "slides/week1-lifecycle.html#exploration-2",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nFocusing on the nonmissing values, we see the same power law relationship but with different proportionality constants and exponents for the three classes of animals."
  },
  {
    "objectID": "slides/week1-lifecycle.html#analysis-1",
    "href": "slides/week1-lifecycle.html#analysis-1",
    "title": "Data science lifecycle",
    "section": "Analysis",
    "text": "Analysis\nSo we might hypothesize that:\n\\[\n(\\text{brain}) = \\beta_1(\\text{body})^{\\alpha_1} \\qquad \\text{(mammal)} \\\\\n(\\text{brain}) = \\beta_2(\\text{body})^{\\alpha_2} \\qquad \\text{(reptile)} \\\\\n(\\text{brain}) = \\beta_3(\\text{body})^{\\alpha_3} \\qquad \\text{(bird)} \\\\\n\\beta_i \\neq \\beta_j, \\alpha_i \\neq \\alpha_j \\quad \\text{for } i \\neq j\n\\]"
  },
  {
    "objectID": "slides/week1-lifecycle.html#interpretation-1",
    "href": "slides/week1-lifecycle.html#interpretation-1",
    "title": "Data science lifecycle",
    "section": "Interpretation",
    "text": "Interpretation\nIt seems that the average brain and body weights of the birds, mammals, and reptiles measured in these studies exhibit distinct power law relationships.\n\nWhat would you investigate next?\n\nCorrelates of body weight?\nAdjust for lifespan, habitat, predation, etc.?\nEstimate the \\(\\alpha_i\\)’s and \\(\\beta_i\\)’s?\nPredict brain weights for unobserved species?\nSomething else?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#a-comment",
    "href": "slides/week1-lifecycle.html#a-comment",
    "title": "Data science lifecycle",
    "section": "A comment",
    "text": "A comment\nNotice that I did not mention the word ‘model’ anywhere!\n\nThis was intentional – it is a common misconception that analyzing data always involves fitting models.\n\nModels are not not always necessary or appropriate\nYou can learn a lot from exploratory techniques\nModels approximate specific kinds of relationships in data\nExploratory analysis can reveal unexpected structure"
  },
  {
    "objectID": "slides/week1-lifecycle.html#but-if-we-did-want-to-fit-a-model",
    "href": "slides/week1-lifecycle.html#but-if-we-did-want-to-fit-a-model",
    "title": "Data science lifecycle",
    "section": "But if we did want to fit a model…",
    "text": "But if we did want to fit a model…\n\\((\\text{brain}) = \\beta_j(\\text{body})^{\\alpha_j} \\quad \\text{animal class } j = 1, 2, 3\\)\n\nFigureEstimates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                      coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Bird                -1.9574     0.040   -49.118  0.000    -2.036    -1.879\n\n\n  Mammal              -2.9391     0.029  -100.061  0.000    -2.997    -2.882\n\n\n  Reptile             -4.0335     0.083   -48.577  0.000    -4.196    -3.871\n\n\n  log.body.bird        0.5653     0.008    66.566  0.000     0.549     0.582\n\n\n  log.body.mammal      0.7651     0.004   191.544  0.000     0.757     0.773\n\n\n  log.body.reptile     0.5293     0.017    31.375  0.000     0.496     0.562"
  },
  {
    "objectID": "slides/week1-lifecycle.html#model-limitations",
    "href": "slides/week1-lifecycle.html#model-limitations",
    "title": "Data science lifecycle",
    "section": "Model limitations",
    "text": "Model limitations\nBack to the issue of representativeness:\n\nshouldn’t use this model for inferences\nmight not be reliable for prediction either\nbut does capture/convey some suggestive comparisons\n\n\nSo, just be careful with interpretation of results:\n\n“For this particular collection of specimens, we estimated…”"
  },
  {
    "objectID": "slides/week1-lifecycle.html#zooming-out",
    "href": "slides/week1-lifecycle.html#zooming-out",
    "title": "Data science lifecycle",
    "section": "Zooming out",
    "text": "Zooming out\nThis example illustrates the aspects of the lifecylce we’ll cover in this class:\n\ndata retrieval and import\ntidying and transformation\nvisualization\nexploratory analysis\nmodeling\n\n\nWe’ll address these topics in sequence."
  },
  {
    "objectID": "slides/week1-lifecycle.html#next-week",
    "href": "slides/week1-lifecycle.html#next-week",
    "title": "Data science lifecycle",
    "section": "Next week",
    "text": "Next week\n\nTabular data structure\nData semantics\nTidy data\nTransformations of tabular data\nAggregation and grouping"
  },
  {
    "objectID": "slides/week2-tidy.html#announcements",
    "href": "slides/week2-tidy.html#announcements",
    "title": "Tidy data",
    "section": "Announcements",
    "text": "Announcements\n\nPDF export fixed on JupyterHub\nLab 0 due today 11:59PM; late submissions allowed until Wednesday 11:59PM\nComplete Q1-Q4 (fruit_info section) of Lab 1 before section"
  },
  {
    "objectID": "slides/week2-tidy.html#this-week",
    "href": "slides/week2-tidy.html#this-week",
    "title": "Tidy data",
    "section": "This week",
    "text": "This week\n\nTabular data\n\nMany ways to structure a dataset\nFew organizational constraints ‘in the wild’\n\nPrinciples of tidy data: matching semantics with structure\n\nData semantics: observations and variables\nTabular structure: rows and columns\nThe tidy standard\nCommon messes\nTidying operations\n\nTransforming data frames\n\nSubsetting (slicing and filtering)\nDerived variables\nAggregation and summary statistics"
  },
  {
    "objectID": "slides/week2-tidy.html#tabular-data",
    "href": "slides/week2-tidy.html#tabular-data",
    "title": "Tidy data",
    "section": "Tabular data",
    "text": "Tabular data\n\nMany possible layouts for tabular data\n‘Real’ datasets have few organizational constraints\n\n\nMost data are stored in tables, but there are always multiple possible tabular layouts for the same underlying data.\n\n\nLet’s look at some examples."
  },
  {
    "objectID": "slides/week2-tidy.html#mammal-data-long-layouts",
    "href": "slides/week2-tidy.html#mammal-data-long-layouts",
    "title": "Tidy data",
    "section": "Mammal data: long layouts",
    "text": "Mammal data: long layouts\nBelow is the Allison 1976 mammal brain-body weight dataset from last time shown in two ‘long’ layouts:\n\n\n\n\n\n\n\n  \n    \n      \n      body_wt\n      brain_wt\n    \n    \n      species\n      \n      \n    \n  \n  \n    \n      Africanelephant\n      6654.0\n      5712.0\n    \n    \n      Africangiantpouchedrat\n      1.0\n      6.6\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      measurement\n      weight\n    \n    \n      species\n      \n      \n    \n  \n  \n    \n      Africanelephant\n      brain_wt\n      5712.0\n    \n    \n      Africanelephant\n      body_wt\n      6654.0\n    \n    \n      Africangiantpouchedrat\n      brain_wt\n      6.6\n    \n    \n      Africangiantpouchedrat\n      body_wt\n      1.0"
  },
  {
    "objectID": "slides/week2-tidy.html#mammal-data-wide-layout",
    "href": "slides/week2-tidy.html#mammal-data-wide-layout",
    "title": "Tidy data",
    "section": "Mammal data: wide layout",
    "text": "Mammal data: wide layout\nHere’s a third possible layout for the mammal brain-body weight data:\n\n\n\n\n\n\n\n  \n    \n      species\n      Africanelephant\n      Africangiantpouchedrat\n      ArcticFox\n      Arcticgroundsquirrel\n    \n    \n      measurement\n      \n      \n      \n      \n    \n  \n  \n    \n      body_wt\n      6654.0\n      1.0\n      3.385\n      0.92\n    \n    \n      brain_wt\n      5712.0\n      6.6\n      44.500\n      5.70"
  },
  {
    "objectID": "slides/week2-tidy.html#gdp-growth-data-wide-layout",
    "href": "slides/week2-tidy.html#gdp-growth-data-wide-layout",
    "title": "Tidy data",
    "section": "GDP growth data: wide layout",
    "text": "GDP growth data: wide layout\nHere’s another example: World Bank data on annual GDP growth for 264 countries from 1961 – 2019.\n\n\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Country Code\n      2009\n      2010\n      2011\n    \n  \n  \n    \n      0\n      Aruba\n      ABW\n      -10.519749\n      -3.685029\n      3.446055\n    \n    \n      1\n      Afghanistan\n      AFG\n      21.390528\n      14.362441\n      0.426355\n    \n    \n      2\n      Angola\n      AGO\n      0.858713\n      4.403933\n      3.471976\n    \n    \n      3\n      Albania\n      ALB\n      3.350067\n      3.706892\n      2.545322\n    \n    \n      4\n      Andorra\n      AND\n      -5.302847\n      -1.974958\n      -0.008070"
  },
  {
    "objectID": "slides/week2-tidy.html#gdp-growth-data-long-layout",
    "href": "slides/week2-tidy.html#gdp-growth-data-long-layout",
    "title": "Tidy data",
    "section": "GDP growth data: long layout",
    "text": "GDP growth data: long layout\nHere’s an alternative layout for the annual GDP growth data:\n\n\n\n\n\n\n  \n    \n      \n      year\n      growth_pct\n    \n    \n      Country Name\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      2009\n      21.390528\n    \n    \n      Aruba\n      2009\n      -10.519749\n    \n    \n      Afghanistan\n      2010\n      14.362441\n    \n    \n      Aruba\n      2010\n      -3.685029\n    \n    \n      Afghanistan\n      2011\n      0.426355\n    \n    \n      Aruba\n      2011\n      3.446055"
  },
  {
    "objectID": "slides/week2-tidy.html#sb-weather-data-long-layouts",
    "href": "slides/week2-tidy.html#sb-weather-data-long-layouts",
    "title": "Tidy data",
    "section": "SB weather data: long layouts",
    "text": "SB weather data: long layouts\nA third example: daily minimum and maximum temperatures recorded at Santa Barbara Municipal Airport from January 2021 through March 2021.\n\n\n\n\n\n\n\n  \n    \n      \n      STATION\n      TMAX\n      TMIN\n      MONTH\n      DAY\n      YEAR\n    \n  \n  \n    \n      0\n      USW00023190\n      65\n      37\n      1\n      1\n      2021\n    \n    \n      1\n      USW00023190\n      62\n      38\n      1\n      2\n      2021\n    \n    \n      2\n      USW00023190\n      60\n      42\n      1\n      3\n      2021"
  },
  {
    "objectID": "slides/week2-tidy.html#sb-weather-data-wide-layout",
    "href": "slides/week2-tidy.html#sb-weather-data-wide-layout",
    "title": "Tidy data",
    "section": "SB weather data: wide layout",
    "text": "SB weather data: wide layout\nHere’s a wide layout for the SB weather data:\n\n\n\n\n\n\n\n  \n    \n      \n      DAY\n      1\n      2\n      3\n      4\n    \n    \n      MONTH\n      type\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      TMAX\n      65.0\n      62.0\n      60.0\n      72.0\n    \n    \n      TMIN\n      37.0\n      38.0\n      42.0\n      43.0\n    \n    \n      2\n      TMAX\n      66.0\n      67.0\n      69.0\n      63.0\n    \n    \n      TMIN\n      45.0\n      40.0\n      44.0\n      37.0\n    \n    \n      3\n      TMAX\n      68.0\n      66.0\n      59.0\n      62.0"
  },
  {
    "objectID": "slides/week2-tidy.html#un-development-data-multiple-tables",
    "href": "slides/week2-tidy.html#un-development-data-multiple-tables",
    "title": "Tidy data",
    "section": "UN development data: multiple tables",
    "text": "UN development data: multiple tables\nA final example: United Nations country development data organized into different tables according to variable type.\nHere is a table of population measurements:\n\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n    \n    \n      country\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n    \n  \n\n\n\n\nAnd here is a table of a few gender-related variables:\n\n\n\n\n\n\n\n\n  \n    \n      \n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n    \n  \n  \n    \n      Norway\n      0.045\n      40.8\n      60.4\n      67.2\n    \n    \n      Ireland\n      0.093\n      24.3\n      56.0\n      68.4"
  },
  {
    "objectID": "slides/week2-tidy.html#un-development-data-one-table",
    "href": "slides/week2-tidy.html#un-development-data-one-table",
    "title": "Tidy data",
    "section": "UN development data: one table",
    "text": "UN development data: one table\nHere are both tables merged by country:\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n      0.655\n      27.2\n      21.6\n      74.7\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n      0.181\n      29.5\n      46.7\n      64.6\n    \n    \n      Algeria\n      43.1\n      73.2\n      5.0\n      27.1\n      2.8\n      0.429\n      21.5\n      14.6\n      67.4"
  },
  {
    "objectID": "slides/week2-tidy.html#un-development-data-one-longer-table",
    "href": "slides/week2-tidy.html#un-development-data-one-longer-table",
    "title": "Tidy data",
    "section": "UN development data: one (longer) table",
    "text": "UN development data: one (longer) table\nAnd here is another arrangement of the merged table:\n\n\n\n\n\n\n  \n    \n      \n      gender_variable\n      gender_value\n      population_variable\n      population_value\n    \n    \n      country\n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      gender_inequality\n      0.655\n      total_pop\n      38.0\n    \n    \n      Albania\n      gender_inequality\n      0.181\n      total_pop\n      2.9\n    \n    \n      Algeria\n      gender_inequality\n      0.429\n      total_pop\n      43.1\n    \n    \n      Andorra\n      gender_inequality\n      NaN\n      total_pop\n      0.1\n    \n    \n      Angola\n      gender_inequality\n      0.536\n      total_pop\n      31.8"
  },
  {
    "objectID": "slides/week2-tidy.html#what-are-the-differences",
    "href": "slides/week2-tidy.html#what-are-the-differences",
    "title": "Tidy data",
    "section": "What are the differences?",
    "text": "What are the differences?\nIn short, the alternate layouts differ in three respects:\n\nRows\nColumns\nNumber of tables"
  },
  {
    "objectID": "slides/week2-tidy.html#how-to-choose",
    "href": "slides/week2-tidy.html#how-to-choose",
    "title": "Tidy data",
    "section": "How to choose?",
    "text": "How to choose?\nReturn to one of the examples and review the different layouts with your neighbor.\n\nList a few advantages and disadvantages for each layout.\nWhich do you prefer and why?"
  },
  {
    "objectID": "slides/week2-tidy.html#few-organizational-constraints",
    "href": "slides/week2-tidy.html#few-organizational-constraints",
    "title": "Tidy data",
    "section": "Few organizational constraints",
    "text": "Few organizational constraints\nIt’s surprisingly difficult to articulate reasons why one layout might be preferable to another.\n\nUsually the choice of layout isn’t principled\nIdiosyncratic: two people are likely to make different choices\n\n\nAs a result:\n\nFew widely used conventions\nLots of variability ‘in the wild’\nDatasets are often organized in bizarre ways"
  },
  {
    "objectID": "slides/week2-tidy.html#form-and-representation",
    "href": "slides/week2-tidy.html#form-and-representation",
    "title": "Tidy data",
    "section": "Form and representation",
    "text": "Form and representation\nBecause of the wide range of possible layouts for a dataset, and the variety of choices that are made about how to store data, data scientists are constantly faced with determining how best to reorganize datasets in a way that facilitates exploration and analysis.\n\nBroadly, this involves two interdependent choices:\n\nChoice of representation: how to encode information.\n\nExample: parse dates as ‘MM/DD/YYYY’ (one variable) or ‘MM’, ‘DD’, ‘YYYY’ (three variables)?\nExample: use values 1, 2, 3 or ‘low’, ‘med’, ‘high’?\nExample: name variables ‘question1’, ‘question2’, …, or ‘age’, ‘income’, …?\n\nChoice of form: how to display information\n\nExample: wide table or long table?\nExample: one table or many?"
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-data",
    "href": "slides/week2-tidy.html#tidy-data",
    "title": "Tidy data",
    "section": "Tidy data",
    "text": "Tidy data\nThe tidy data standard is a principled way of organizing tabular data. It has two main advantages:\n\nFacilitates workflow by establishing a consistent dataset structure.\nPrinciples are designed to make transformation, exploration, visualization, and modeling easy."
  },
  {
    "objectID": "slides/week2-tidy.html#semantics-and-structure",
    "href": "slides/week2-tidy.html#semantics-and-structure",
    "title": "Tidy data",
    "section": "Semantics and structure",
    "text": "Semantics and structure\n\n“Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored.” Wickham and Grolemund, R for Data Science, 2017.\n\n\nA dataset is a collection of values.\n\nthe semantics of a dataset are the meanings of the values\nthe structure of a dataset is the arrangement of the values"
  },
  {
    "objectID": "slides/week2-tidy.html#data-semantics",
    "href": "slides/week2-tidy.html#data-semantics",
    "title": "Tidy data",
    "section": "Data semantics",
    "text": "Data semantics\nTo introduce some general vocabulary, each value in a dataset is\n\nan observation\nof a variable\ntaken on an observational unit."
  },
  {
    "objectID": "slides/week2-tidy.html#units-variables-and-observations",
    "href": "slides/week2-tidy.html#units-variables-and-observations",
    "title": "Tidy data",
    "section": "Units, variables, and observations",
    "text": "Units, variables, and observations\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n    \n    \n      country\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n    \n    \n      Albania\n      2.9\n      61.2\n    \n  \n\n\n\n\n\nAn observational unit is the entity measured.\n\nAbove, country\n\nA variable is an attribute measured on each unit.\n\nAbove, total population and urban percentage\n\nAn observation is a collection of measurements taken on one unit.\n\nAbove, 38.0 and 25.8"
  },
  {
    "objectID": "slides/week2-tidy.html#identifying-units-variables-and-observations",
    "href": "slides/week2-tidy.html#identifying-units-variables-and-observations",
    "title": "Tidy data",
    "section": "Identifying units, variables, and observations",
    "text": "Identifying units, variables, and observations\nLet’s do an example. Here’s one record from the GDP growth data:\n\n\n\n\n\n\n\n  \n    \n      \n      year\n      growth_pct\n    \n    \n      Country Name\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      2010\n      14.362441\n    \n  \n\n\n\n\n\n\nAbove, the values -13.605441 and 1961 are observations of the variables GDP growth and year recorded for the observational unit Algeria."
  },
  {
    "objectID": "slides/week2-tidy.html#your-turn",
    "href": "slides/week2-tidy.html#your-turn",
    "title": "Tidy data",
    "section": "Your turn",
    "text": "Your turn\nWhat are the units, variables and observations?\n\n\n\n\n\n\n\n  \n    \n      \n      DAY\n      1\n      2\n      3\n      4\n    \n    \n      MONTH\n      type\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      TMAX\n      65.0\n      62.0\n      60.0\n      72.0\n    \n    \n      TMIN\n      37.0\n      38.0\n      42.0\n      43.0\n    \n    \n      2\n      TMAX\n      66.0\n      67.0\n      69.0\n      63.0\n    \n    \n      TMIN\n      45.0\n      40.0\n      44.0\n      37.0\n    \n  \n\n\n\n\n\n\nThink about it, then confer with your neighbor."
  },
  {
    "objectID": "slides/week2-tidy.html#data-structure",
    "href": "slides/week2-tidy.html#data-structure",
    "title": "Tidy data",
    "section": "Data structure",
    "text": "Data structure\nData structure refers to the form in which it is stored.\n\nTabular data is arranged in rows and columns.\n\n\nAs we saw, there are multiple structures – arrangements of rows and columns – available to represent any dataset."
  },
  {
    "objectID": "slides/week2-tidy.html#the-tidy-standard",
    "href": "slides/week2-tidy.html#the-tidy-standard",
    "title": "Tidy data",
    "section": "The tidy standard",
    "text": "The tidy standard\nThe tidy standard consists in matching semantics and structure. A dataset is tidy if:\n\nEach variable is a column.\nEach observation is a row.\nEach table contains measurements on only one type of observational unit.\n\n\nTidy data."
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy",
    "href": "slides/week2-tidy.html#tidy-or-messy",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\nLet’s revisit some of our examples of multiple layouts.\n\n\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Country Code\n      2009\n      2010\n      2011\n    \n  \n  \n    \n      0\n      Aruba\n      ABW\n      -10.519749\n      -3.685029\n      3.446055\n    \n    \n      1\n      Afghanistan\n      AFG\n      21.390528\n      14.362441\n      0.426355\n    \n    \n      2\n      Angola\n      AGO\n      0.858713\n      4.403933\n      3.471976\n    \n  \n\n\n\n\n\n\nWe can compare the semantics and structure for alignment:\n\n\n\nSemantics\n\nStructure\n\n\n\n\n\nObservations\nAnnual records\nRows\nCountries\n\n\nVariables\nGDP growth and year\nColumns\nValue of year\n\n\nObservational units\nCountries\nTables\nJust one\n\n\n\n\n\nRules 1 and 2 are violated, since column names are values (of year), not variables. Not tidy."
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy-1",
    "href": "slides/week2-tidy.html#tidy-or-messy-1",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\n\n\n\n\n\n\n\n  \n    \n      \n      year\n      growth_pct\n    \n    \n      Country Name\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      2009\n      21.390528\n    \n    \n      Aruba\n      2009\n      -10.519749\n    \n    \n      Afghanistan\n      2010\n      14.362441\n    \n    \n      Aruba\n      2010\n      -3.685029\n    \n  \n\n\n\n\n\n\nComparison of semantics and structure:\n\n\n\nSemantics\n\nStructure\n\n\n\n\n\nObservations\nAnnual records\nRows\nAnnual records\n\n\nVariables\nGDP growth and year\nColumns\nGDP growth and year\n\n\nObservational units\nCountries\nTables\nJust one\n\n\n\n\n\nAll three rules are met: rows are observations, columns are variables, and there’s one unit type and one table. Tidy."
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy-2",
    "href": "slides/week2-tidy.html#tidy-or-messy-2",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\n\n\n\n\n\n\n\n  \n    \n      \n      STATION\n      TMAX\n      TMIN\n      MONTH\n      DAY\n      YEAR\n    \n  \n  \n    \n      0\n      USW00023190\n      65\n      37\n      1\n      1\n      2021\n    \n    \n      1\n      USW00023190\n      62\n      38\n      1\n      2\n      2021\n    \n    \n      2\n      USW00023190\n      60\n      42\n      1\n      3\n      2021\n    \n  \n\n\n\n\nTry this one on your own. Then compare with your neighbor.\n\nIdentify the observations and variables\nWhat are the observational units?"
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy-3",
    "href": "slides/week2-tidy.html#tidy-or-messy-3",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\nIn undev1 and undev2:\n\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n    \n    \n      country\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n    \n  \n  \n    \n      Norway\n      0.045\n      40.8\n      60.4\n      67.2\n    \n    \n      Ireland\n      0.093\n      24.3\n      56.0\n      68.4\n    \n  \n\n\n\n\n\n\nHere there are multiple tables. To discuss:\n\nAre the observational units the same or different?\nBased on your answer above, is the data tidy or not?"
  },
  {
    "objectID": "slides/week2-tidy.html#common-messes",
    "href": "slides/week2-tidy.html#common-messes",
    "title": "Tidy data",
    "section": "Common messes",
    "text": "Common messes\n\n“Well, here’s another nice mess you’ve gotten me into” – Oliver Hardy\n\nThese examples illustrate some common messes:\n\nColumns are values, not variables\n\nGDP data: columns are 1961, 1962, …\n\nMultiple variables are stored in one column\n\nMammal data: weight column contains both body and brain weights\n\nVariables or values are stored in rows and columns\n\nWeather data: date values are stored in rows and columns, each column contains both min and max temperatures\n\nMeasurements on one type of observational unit are divided into multiple tables.\n\nUN development data: one table for population statistics and a separate table for gender statistics."
  },
  {
    "objectID": "slides/week2-tidy.html#tidying-operations",
    "href": "slides/week2-tidy.html#tidying-operations",
    "title": "Tidy data",
    "section": "Tidying operations",
    "text": "Tidying operations\nThese common messes can be cleaned up by some simple operations:\n\nmelt\n\nreshape a dataframe from wide to long format\n\npivot\n\nreshape a dataframe from long to wide format\n\nmerge\n\ncombine two dataframes row-wise by matching the values of certain columns"
  },
  {
    "objectID": "slides/week2-tidy.html#melt",
    "href": "slides/week2-tidy.html#melt",
    "title": "Tidy data",
    "section": "Melt",
    "text": "Melt\nMelting resolves the problem of having values stored as columns (common mess 1)."
  },
  {
    "objectID": "slides/week2-tidy.html#melt-1",
    "href": "slides/week2-tidy.html#melt-1",
    "title": "Tidy data",
    "section": "Melt",
    "text": "Melt\n\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Country Code\n      1961\n      1962\n      1963\n      1964\n      1965\n      1966\n      1967\n      1968\n      ...\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n  \n  \n    \n      0\n      Aruba\n      ABW\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      -3.685029\n      3.446055\n      -1.369863\n      4.198232\n      0.300000\n      5.700001\n      2.100000\n      1.999999\n      NaN\n      NaN\n    \n    \n      1\n      Afghanistan\n      AFG\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      14.362441\n      0.426355\n      12.752287\n      5.600745\n      2.724543\n      1.451315\n      2.260314\n      2.647003\n      1.189228\n      3.911603\n    \n  \n\n2 rows × 61 columns\n\n\n\n\n\n# in pandas\ngdp1.melt(\n    id_vars = ['Country Name', 'Country Code'], # which variables do you want to retain for each row? .\n    var_name = 'Year', # what do you want to name the variable that will contain the column names?\n    value_name = 'GDP Growth', # what do you want to name the variable that will contain the values?\n).head(2)\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Country Code\n      Year\n      GDP Growth\n    \n  \n  \n    \n      0\n      Aruba\n      ABW\n      1961\n      NaN\n    \n    \n      1\n      Afghanistan\n      AFG\n      1961\n      NaN"
  },
  {
    "objectID": "slides/week2-tidy.html#pivot",
    "href": "slides/week2-tidy.html#pivot",
    "title": "Tidy data",
    "section": "Pivot",
    "text": "Pivot\nPivoting resolves the issue of having multiple variables stored in one column (common mess 2). It’s the inverse operation of melting."
  },
  {
    "objectID": "slides/week2-tidy.html#pivot-1",
    "href": "slides/week2-tidy.html#pivot-1",
    "title": "Tidy data",
    "section": "Pivot",
    "text": "Pivot\n\n\n\n\n\n\n  \n    \n      \n      measurement\n      weight\n    \n    \n      species\n      \n      \n    \n  \n  \n    \n      Africanelephant\n      brain_wt\n      5712.0\n    \n    \n      Africanelephant\n      body_wt\n      6654.0\n    \n    \n      Africangiantpouchedrat\n      brain_wt\n      6.6\n    \n    \n      Africangiantpouchedrat\n      body_wt\n      1.0\n    \n  \n\n\n\n\n\n# in pandas\nmammal2.pivot(\n    columns = 'measurement', # which variable(s) do you want to send to new column names?\n    values = 'weight' # which variable(s) do you want to use to populate the new columns?\n).head(2)\n\n\n\n\n\n  \n    \n      measurement\n      body_wt\n      brain_wt\n    \n    \n      species\n      \n      \n    \n  \n  \n    \n      Africanelephant\n      6654.0\n      5712.0\n    \n    \n      Africangiantpouchedrat\n      1.0\n      6.6"
  },
  {
    "objectID": "slides/week2-tidy.html#pivot-and-melt",
    "href": "slides/week2-tidy.html#pivot-and-melt",
    "title": "Tidy data",
    "section": "Pivot and melt",
    "text": "Pivot and melt\nCommon mess 3 is a combination of messes 1 and 2: values or variables are stored in both rows and columns. Pivoting and melting in sequence can usually fix this.\n\n\n\n\n\n\n\n  \n    \n      \n      DAY\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      22\n      23\n      24\n      25\n      26\n      27\n      28\n      29\n      30\n      31\n    \n    \n      MONTH\n      type\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      TMAX\n      65.0\n      62.0\n      60.0\n      72.0\n      61.0\n      71.0\n      73.0\n      79.0\n      71.0\n      67.0\n      ...\n      61.0\n      59.0\n      65.0\n      55.0\n      57.0\n      54.0\n      55.0\n      55.0\n      58.0\n      63.0\n    \n    \n      TMIN\n      37.0\n      38.0\n      42.0\n      43.0\n      40.0\n      39.0\n      38.0\n      36.0\n      39.0\n      37.0\n      ...\n      41.0\n      40.0\n      38.0\n      44.0\n      40.0\n      48.0\n      49.0\n      42.0\n      37.0\n      37.0\n    \n    \n      2\n      TMAX\n      66.0\n      67.0\n      69.0\n      63.0\n      66.0\n      68.0\n      60.0\n      57.0\n      59.0\n      61.0\n      ...\n      75.0\n      75.0\n      70.0\n      66.0\n      69.0\n      76.0\n      68.0\n      NaN\n      NaN\n      NaN\n    \n    \n      TMIN\n      45.0\n      40.0\n      44.0\n      37.0\n      38.0\n      38.0\n      38.0\n      49.0\n      49.0\n      41.0\n      ...\n      37.0\n      39.0\n      41.0\n      39.0\n      36.0\n      43.0\n      38.0\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      TMAX\n      68.0\n      66.0\n      59.0\n      62.0\n      67.0\n      69.0\n      60.0\n      69.0\n      65.0\n      58.0\n      ...\n      71.0\n      72.0\n      67.0\n      65.0\n      63.0\n      72.0\n      73.0\n      77.0\n      NaN\n      NaN\n    \n    \n      TMIN\n      37.0\n      36.0\n      36.0\n      37.0\n      39.0\n      43.0\n      47.0\n      47.0\n      47.0\n      43.0\n      ...\n      50.0\n      49.0\n      41.0\n      44.0\n      40.0\n      41.0\n      41.0\n      42.0\n      NaN\n      NaN\n    \n  \n\n6 rows × 31 columns"
  },
  {
    "objectID": "slides/week2-tidy.html#pivot-and-melt-1",
    "href": "slides/week2-tidy.html#pivot-and-melt-1",
    "title": "Tidy data",
    "section": "Pivot and melt",
    "text": "Pivot and melt\n\nFirst, meltThen, pivot\n\n\n\nweather3.melt(\n    ignore_index = False,\n    var_name = 'day',\n    value_name = 'temp'\n).head()\n\n\n\n\n\n  \n    \n      \n      \n      day\n      temp\n    \n    \n      MONTH\n      type\n      \n      \n    \n  \n  \n    \n      1\n      TMAX\n      1\n      65.0\n    \n    \n      TMIN\n      1\n      37.0\n    \n    \n      2\n      TMAX\n      1\n      66.0\n    \n    \n      TMIN\n      1\n      45.0\n    \n    \n      3\n      TMAX\n      1\n      68.0\n    \n  \n\n\n\n\n\n\n\nweather3.melt(\n    ignore_index = False,\n    var_name = 'day',\n    value_name = 'temp'\n).reset_index().pivot(\n    index = ['MONTH', 'day'],\n    columns = 'type',\n    values = 'temp'\n).reset_index().rename_axis(columns = {'type': ''}).head()\n\n\n\n\n\n  \n    \n      \n      MONTH\n      day\n      TMAX\n      TMIN\n    \n  \n  \n    \n      0\n      1\n      1\n      65.0\n      37.0\n    \n    \n      1\n      1\n      2\n      62.0\n      38.0\n    \n    \n      2\n      1\n      3\n      60.0\n      42.0\n    \n    \n      3\n      1\n      4\n      72.0\n      43.0\n    \n    \n      4\n      1\n      5\n      61.0\n      40.0"
  },
  {
    "objectID": "slides/week2-tidy.html#merge",
    "href": "slides/week2-tidy.html#merge",
    "title": "Tidy data",
    "section": "Merge",
    "text": "Merge\nMerging resolves the issue of storing observations or variables on one unit type in multiple tables (mess 4). The basic idea is to combine by matching rows."
  },
  {
    "objectID": "slides/week2-tidy.html#merge-1",
    "href": "slides/week2-tidy.html#merge-1",
    "title": "Tidy data",
    "section": "Merge",
    "text": "Merge\nThe code below combines columns in each table by matching rows based on country.\n\npd.merge(undev1, undev2, on = 'country').head(4)\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n      0.655\n      27.2\n      21.6\n      74.7\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n      0.181\n      29.5\n      46.7\n      64.6\n    \n    \n      Algeria\n      43.1\n      73.2\n      5.0\n      27.1\n      2.8\n      0.429\n      21.5\n      14.6\n      67.4\n    \n    \n      Andorra\n      0.1\n      88.0\n      NaN\n      NaN\n      NaN\n      NaN\n      46.4\n      NaN\n      NaN"
  },
  {
    "objectID": "slides/week2-tidy.html#merge-2",
    "href": "slides/week2-tidy.html#merge-2",
    "title": "Tidy data",
    "section": "Merge",
    "text": "Merge\nThere are various rules for exactly how to merge, but the general syntactical procedure to merge dataframes df1 and df2 is this.\n\nSpecify an order: merge(df1, df2) or merge(df2, df1).\nSpecify keys: the shared columns to use for matching rows of df1 with rows of df2.\n\nfor example, merging on date will align rows in df2 with rows of df1 that have the same value for date\n\nSpecify a rule for which rows to return after merging\n\nkeep all rows with key entries in df1, drop non-matching rows in df2 (‘left’ join)\nkeep all rows with key entries in df2 drop non-matching rows in df1 (‘right’ join)\nkeep all rows with key entries in either df1 or df2, inducing missing values (‘outer’ join)\nkeep all rows with key entries in both df1 and df2 (‘inner’ join)"
  },
  {
    "objectID": "slides/week2-tidy.html#next-time",
    "href": "slides/week2-tidy.html#next-time",
    "title": "Tidy data",
    "section": "Next time",
    "text": "Next time\nTransformations of tabular data\n\nSlicing and filtering\nDefining new variables\nVectorized operatioons\nAggregation and grouping"
  },
  {
    "objectID": "slides/week2-transform.html#recap-tidy-data",
    "href": "slides/week2-transform.html#recap-tidy-data",
    "title": "Dataframe Transformations",
    "section": "Recap: tidy data",
    "text": "Recap: tidy data\nThe tidy standard consists in matching semantics and structure.\n\nA dataset is tidy if:\n\nEach variable is a column.\nEach observation is a row.\nEach table contains measurements on only one type of observational unit."
  },
  {
    "objectID": "slides/week2-transform.html#why-tidy",
    "href": "slides/week2-transform.html#why-tidy",
    "title": "Dataframe Transformations",
    "section": "Why tidy?",
    "text": "Why tidy?\n\nWhy use the tidy standard? Wouldn’t any system of organization do just as well?\n\n\nThe tidy standard has three main advantages:\n\nHaving a consistent system of organization makes it easier to focus on analysis and exploration. (True of any system)\nMany software tools are designed to work with tidy data inputs. (Tidy only)\nTransformation of tidy data is especially natural in most computing environments due to vectorized operations. (Tidy only)"
  },
  {
    "objectID": "slides/week2-transform.html#transformations",
    "href": "slides/week2-transform.html#transformations",
    "title": "Dataframe Transformations",
    "section": "Transformations",
    "text": "Transformations\nTransformations of data frames are operations that modify the shape or values of a data frame. These include:\n\nSlicing rows and columns by index\nFiltering rows by logical conditions\nDefining new variables from scratch or by operations on existing variables\nAggregations (min, mean, max, etc.)"
  },
  {
    "objectID": "slides/week2-transform.html#slicing",
    "href": "slides/week2-transform.html#slicing",
    "title": "Dataframe Transformations",
    "section": "Slicing",
    "text": "Slicing\nSlicing refers to retrieving a (usually contiguous) subset (a ‘slice’) of rows/columns from a data frame.\n\nUses:\n\ndata inspection/retrieval\nsubsetting for further analysis/manipulation\ndata display"
  },
  {
    "objectID": "slides/week2-transform.html#data-display",
    "href": "slides/week2-transform.html#data-display",
    "title": "Dataframe Transformations",
    "section": "Data display",
    "text": "Data display\nRecall the UN Development data:\n\n# preview UN data -- note indexed by country\nundev.head(3)\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n      0.655\n      27.2\n      21.6\n      74.7\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n      0.181\n      29.5\n      46.7\n      64.6\n    \n    \n      Algeria\n      43.1\n      73.2\n      5.0\n      27.1\n      2.8\n      0.429\n      21.5\n      14.6\n      67.4\n    \n  \n\n\n\n\n\nAside: .head() is a slicing operation – it returns the ‘top’ slice of rows."
  },
  {
    "objectID": "slides/week2-transform.html#data-inspectionretrieval",
    "href": "slides/week2-transform.html#data-inspectionretrieval",
    "title": "Dataframe Transformations",
    "section": "Data inspection/retrieval",
    "text": "Data inspection/retrieval\nTo inspect the percentage of women in parliament in Mexico, slice accordingly:\n\n\nundev.loc[['Mexico'], ['parliament_pct_women']]\n\n\n\n\n\n  \n    \n      \n      parliament_pct_women\n    \n    \n      country\n      \n    \n  \n  \n    \n      Mexico\n      48.4"
  },
  {
    "objectID": "slides/week2-transform.html#review-.loc-and-.iloc",
    "href": "slides/week2-transform.html#review-.loc-and-.iloc",
    "title": "Dataframe Transformations",
    "section": "Review: .loc and .iloc",
    "text": "Review: .loc and .iloc\nThe primary slicing functions in pandas are\n\n.loc (location) to slice by index\n.iloc (integer location) to slice by position\n\n\n\n# .iloc equivalent of previous slice\nundev.iloc[[111], [6]]\n\n\n\n\n\n  \n    \n      \n      parliament_pct_women\n    \n    \n      country\n      \n    \n  \n  \n    \n      Mexico\n      48.4\n    \n  \n\n\n\n\n\n\nCheck your understanding: which row in the dataframe is the observation for Mexico?\n\n\nIf a single index rather than a list is provided – e.g., Mexico rather than [Mexico], – these functions will return the raw value as a float rather than a dataframe.\n\nundev.loc['Mexico', 'parliament_pct_women']\n\n48.4"
  },
  {
    "objectID": "slides/week2-transform.html#larger-slices",
    "href": "slides/week2-transform.html#larger-slices",
    "title": "Dataframe Transformations",
    "section": "Larger slices",
    "text": "Larger slices\nMore typically, a slice will be a contiguous chunk of rows and columns.\n\nSlicing operations can interpret start:end as shorthand for a range of indices.\n\nundev.loc['Mexico':'Mongolia', ['parliament_pct_women']]\n\n\n\n\n\n  \n    \n      \n      parliament_pct_women\n    \n    \n      country\n      \n    \n  \n  \n    \n      Mexico\n      48.4\n    \n    \n      Micronesia (Federated States of)\n      0.0\n    \n    \n      Moldova (Republic of)\n      25.7\n    \n    \n      Mongolia\n      17.3\n    \n  \n\n\n\n\n\n\nNote: start:end is inclusive of both endpoints with .loc, but not inclusive of the right endpoint with .iloc. Get in the habit of double-checking results."
  },
  {
    "objectID": "slides/week2-transform.html#defining-new-variables",
    "href": "slides/week2-transform.html#defining-new-variables",
    "title": "Dataframe Transformations",
    "section": "Defining new variables",
    "text": "Defining new variables\nVectorization of operations in pandas and numpy make tidy data especially nice to manipulate mathematically. For example:\n\n\nweather2['TRANGE'] = weather2.TMAX - weather2.TMIN\nweather2.loc[0:3, ['TMAX', 'TMIN', 'TRANGE']]\n\n\n\n\n\n  \n    \n      \n      TMAX\n      TMIN\n      TRANGE\n    \n  \n  \n    \n      0\n      65\n      37\n      28\n    \n    \n      1\n      62\n      38\n      24\n    \n    \n      2\n      60\n      42\n      18\n    \n    \n      3\n      72\n      43\n      29\n    \n  \n\n\n\n\n\n\nThis computes \\(t_{min, i} - t_{max, i}\\) for all observations \\(i = 1, \\dots, n\\).\n\n\nCheck your understanding: express this calculation as a linear algebra arithmetic operation."
  },
  {
    "objectID": "slides/week2-transform.html#your-turn",
    "href": "slides/week2-transform.html#your-turn",
    "title": "Dataframe Transformations",
    "section": "Your turn",
    "text": "Your turn\nLet’s take another example – consider this slice of the undev data:\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n    \n    \n      country\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n    \n    \n      Albania\n      2.9\n      61.2\n    \n    \n      Algeria\n      43.1\n      73.2\n    \n  \n\n\n\n\n\nWith your neighbor, write a line of code that calculates the percentage of the population living in rural areas."
  },
  {
    "objectID": "slides/week2-transform.html#filtering",
    "href": "slides/week2-transform.html#filtering",
    "title": "Dataframe Transformations",
    "section": "Filtering",
    "text": "Filtering\nFiltering refers to removing a subset of rows based on one or more conditions. (Think of “filtering out” certain rows.)\n\nFor example, suppose we wanted to retrieve only the countries with populations exceeding 1Bn people:\n\nundev[undev.total_pop > 1000]\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      China\n      1433.8\n      60.3\n      85.0\n      1014.0\n      164.5\n      0.168\n      24.9\n      60.5\n      75.3\n    \n    \n      India\n      1366.4\n      34.5\n      116.8\n      915.6\n      87.1\n      0.488\n      13.5\n      20.5\n      76.1"
  },
  {
    "objectID": "slides/week2-transform.html#filtering-1",
    "href": "slides/week2-transform.html#filtering-1",
    "title": "Dataframe Transformations",
    "section": "Filtering",
    "text": "Filtering\nTechnically, filtering works by slicing according to a long logical vector with one entry per row specifying whether to retain (True) or drop (False).\n\nundev.total_pop > 1000\n\ncountry\nAfghanistan                           False\nAlbania                               False\nAlgeria                               False\nAndorra                               False\nAngola                                False\n                                      ...  \nVenezuela (Bolivarian Republic of)    False\nViet Nam                              False\nYemen                                 False\nZambia                                False\nZimbabwe                              False\nName: total_pop, Length: 189, dtype: bool"
  },
  {
    "objectID": "slides/week2-transform.html#a-small-puzzle",
    "href": "slides/week2-transform.html#a-small-puzzle",
    "title": "Dataframe Transformations",
    "section": "A small puzzle",
    "text": "A small puzzle\nConsider a random filter:\n\nrandom_filter = np.random.binomial(n = 1, p = 0.03, size = undev.shape[0]).astype('bool')\n\nrandom_filter\n\narray([False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False,  True, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False])\n\n\n\n\nHow many rows will undev[random_filter] have?\nHow many rows should this random filtering produce on average?"
  },
  {
    "objectID": "slides/week2-transform.html#logical-comparisons",
    "href": "slides/week2-transform.html#logical-comparisons",
    "title": "Dataframe Transformations",
    "section": "Logical comparisons",
    "text": "Logical comparisons\nAny of the following relations can be used to define filtering conditions\n\n\n\nSymbol\nUsage\nMeaning\n\n\n\n\n==\na == b\nDoes a equal b?\n\n\n<=\na <= b\nIs a less than or equal to b?\n\n\n>=\na >= b\nIs a greater than or equal to b?\n\n\n<\na < b\nIs a less than b?\n\n\n>\na > b\nIs a greater than b?\n\n\n~\n~p\nReturns negation of p\n\n\n|\np | q\np OR q\n\n\n&\np & q\np AND q\n\n\n^\np ^ q\np XOR q (exclusive or)"
  },
  {
    "objectID": "slides/week2-transform.html#aggregation",
    "href": "slides/week2-transform.html#aggregation",
    "title": "Dataframe Transformations",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation refers to any operation that combines many values into fewer values.\n\nCommon aggregation operations include:\n\nsummation \\(\\sum_{i} x_i\\)\naveraging \\(n^{-1} \\sum_i x_i\\)\nextrema \\(\\text{min}_i x_i\\) and \\(\\text{max}_i x_i\\)\nstatistics: median, variance, standard deviation, mean absolute deviation, order statistics, quantiles"
  },
  {
    "objectID": "slides/week2-transform.html#aggregation-vs.-other-transformations",
    "href": "slides/week2-transform.html#aggregation-vs.-other-transformations",
    "title": "Dataframe Transformations",
    "section": "Aggregation vs. other transformations",
    "text": "Aggregation vs. other transformations\nAggregations reduce the number of values, whereas other transformations do not.\n\nA bit more formally:\n\naggregations map larger sets of values to smaller sets of values\ntransformations map sets of values to sets of the same size\n\n\n\nCheck your understanding:\n\nis \\((f*g)(x_i) = \\int f(h)g(x_i - h)dh\\) an aggregation?\nis \\(f(x_1, x_2, \\dots, x_n) = \\left(\\prod_i x_i\\right)^{\\frac{1}{n}}\\) an aggregation?"
  },
  {
    "objectID": "slides/week2-transform.html#aggregation-1",
    "href": "slides/week2-transform.html#aggregation-1",
    "title": "Dataframe Transformations",
    "section": "Aggregation?",
    "text": "Aggregation?\n\nGaussian blur."
  },
  {
    "objectID": "slides/week2-transform.html#example-aggregations",
    "href": "slides/week2-transform.html#example-aggregations",
    "title": "Dataframe Transformations",
    "section": "Example aggregations",
    "text": "Example aggregations\nIn numpy, the most common aggregations are implemented as functions:\n\n\n\nnumpy\nfunction\n\n\n\n\nnp.sum()\n\\(\\sum_i x_i\\)\n\n\nnp.max()\n\\(\\text{max}(x_1, \\dots, x_n)\\)\n\n\nnp.min()\n\\(\\text{min}(x_1, \\dots, x_n)\\)\n\n\nnp.median()\n\\(\\text{median}(x_1, \\dots, x_n)\\)\n\n\nnp.mean()\n\\(n^{-1}\\sum_{i = 1}^n x_i\\)\n\n\nnp.var()\n\\((n - 1)^{-1}\\sum_{i = 1}^n (x_i - \\bar{x})^2\\)\n\n\nnp.std()\n\\(\\sqrt{(n - 1)^{-1}\\sum_{i = 1}^n (x_i - \\bar{x})^2}\\)\n\n\nnp.prod()\n\\(\\prod_i x_i\\)\n\n\nnp.percentile()\n\\(\\hat{F}^{-1}(q)\\)"
  },
  {
    "objectID": "slides/week2-transform.html#argmin-and-argmax",
    "href": "slides/week2-transform.html#argmin-and-argmax",
    "title": "Dataframe Transformations",
    "section": "Argmin and argmax",
    "text": "Argmin and argmax\n\\(\\text{argmax}_D f(x)\\) refers to the value or values in the domain \\(D\\) of \\(f\\) at which the function attains its maximum – the argument in \\(D\\) maximizing \\(f\\).\n\nSimilarly, \\(\\text{argmax}_i x_i\\) refers to the index (or indices, if ties) of the largest value in the set \\(\\{x_i\\}\\).\n\n\nCheck your understanding: what does the following return?\n\nnp.array([1, 5, 10, 2]).argmin()"
  },
  {
    "objectID": "slides/week2-transform.html#argmin-and-argmax-1",
    "href": "slides/week2-transform.html#argmin-and-argmax-1",
    "title": "Dataframe Transformations",
    "section": "Argmin and argmax",
    "text": "Argmin and argmax\nThese index retrieval functions can be handy for slicing rows of interest.\n\nFor example, which country had the largest percentage of women in parliament in the year the UN development data was collected?\n\n\n\nundev.index[undev.parliament_pct_women.argmax()]\n\n'Rwanda'\n\n\n\n\nAnd what were the observations?\n\n\n\nundev.iloc[undev.parliament_pct_women.argmax(), :]\n\ntotal_pop                    12.600\nurban_pct_pop                17.300\npop_under5                    1.800\npop_15to64                    7.200\npop_over65                    0.400\ngender_inequality             0.402\nparliament_pct_women         55.700\nlabor_participation_women    83.900\nlabor_participation_men      83.400\nName: Rwanda, dtype: float64"
  },
  {
    "objectID": "slides/week2-transform.html#dataframe-aggregations",
    "href": "slides/week2-transform.html#dataframe-aggregations",
    "title": "Dataframe Transformations",
    "section": "Dataframe aggregations",
    "text": "Dataframe aggregations\nIn pandas, the numpy aggregation operations are available as dataframe methods that apply the corresponding operation over each column:\n\n# mean of every column\nundev.mean()\n\ntotal_pop                    40.423810\nurban_pct_pop                58.660847\npop_under5                    3.666120\npop_15to64                   27.250820\npop_over65                    3.797814\ngender_inequality             0.344154\nparliament_pct_women         23.093048\nlabor_participation_women    52.139888\nlabor_participation_men      72.470787\ndtype: float64"
  },
  {
    "objectID": "slides/week2-transform.html#row-wise-aggregation",
    "href": "slides/week2-transform.html#row-wise-aggregation",
    "title": "Dataframe Transformations",
    "section": "Row-wise aggregation",
    "text": "Row-wise aggregation\nIn general, supplying the argument axis = 1 will compute rowwise aggregations. For example:\n\n# sum `pop_under5`, `pop_15to64`, and `pop_over65`\nundev.iloc[:, 2:5].sum(axis = 1).head(3)\n\ncountry\nAfghanistan    27.5\nAlbania         2.6\nAlgeria        34.9\ndtype: float64\n\n\n\nThis facilitates, for example:\n\nundev['pop_5to14'] = undev.total_pop - undev.iloc[:, 2:5].sum(axis = 1)"
  },
  {
    "objectID": "slides/week2-transform.html#argminidxmin-and-argmaxidxmax",
    "href": "slides/week2-transform.html#argminidxmin-and-argmaxidxmax",
    "title": "Dataframe Transformations",
    "section": "Argmin/idxmin and argmax/idxmax",
    "text": "Argmin/idxmin and argmax/idxmax\nIn pandas, np.argmin() and np.argmax() are implemented as pd.df.idxmin() and pd.df.idxmax().\n\nundev.idxmax()\n\ntotal_pop                                     China\nurban_pct_pop                Hong Kong, China (SAR)\npop_under5                                    India\npop_15to64                                    China\npop_over65                                    China\ngender_inequality                             Yemen\nparliament_pct_women                         Rwanda\nlabor_participation_women                    Rwanda\nlabor_participation_men                       Qatar\npop_5to14                                     India\ndtype: object"
  },
  {
    "objectID": "slides/week2-transform.html#other-functions",
    "href": "slides/week2-transform.html#other-functions",
    "title": "Dataframe Transformations",
    "section": "Other functions",
    "text": "Other functions\nPandas has a wide array of other aggregation and transformation functions. To show just one example:\n\n## slice weather data\nweather4 = weather1.set_index('DATE').iloc[:, 2:4]\nweather4.head(2)\n\n\n\n\n\n  \n    \n      \n      TMAX\n      TMIN\n    \n    \n      DATE\n      \n      \n    \n  \n  \n    \n      1/1/2021\n      65\n      37\n    \n    \n      1/2/2021\n      62\n      38\n    \n  \n\n\n\n\n\n\n# rolling average\nweather4.rolling(window = 7).mean().head(10)\n\n\n\n\n\n  \n    \n      \n      TMAX\n      TMIN\n    \n    \n      DATE\n      \n      \n    \n  \n  \n    \n      1/1/2021\n      NaN\n      NaN\n    \n    \n      1/2/2021\n      NaN\n      NaN\n    \n    \n      1/3/2021\n      NaN\n      NaN\n    \n    \n      1/4/2021\n      NaN\n      NaN\n    \n    \n      1/5/2021\n      NaN\n      NaN\n    \n    \n      1/6/2021\n      NaN\n      NaN\n    \n    \n      1/7/2021\n      66.285714\n      39.571429\n    \n    \n      1/8/2021\n      68.285714\n      39.428571\n    \n    \n      1/9/2021\n      69.571429\n      39.571429\n    \n    \n      1/10/2021\n      70.571429\n      38.857143"
  },
  {
    "objectID": "slides/week2-transform.html#check-your-understanding",
    "href": "slides/week2-transform.html#check-your-understanding",
    "title": "Dataframe Transformations",
    "section": "Check your understanding",
    "text": "Check your understanding\nInterpret this result:\n\nweather4.rolling(window = 7).mean().idxmax()\n\nTMAX    1/20/2021\nTMIN    3/24/2021\ndtype: object\n\n\n(The weather data is January through March.)"
  },
  {
    "objectID": "slides/week2-transform.html#custom-functions",
    "href": "slides/week2-transform.html#custom-functions",
    "title": "Dataframe Transformations",
    "section": "Custom functions",
    "text": "Custom functions\nSee the documentation for a comprehensive list of transformations and aggregations.\n\nIf pandas doesn’t have a method for an operation you’re wanting to perform, you can implement custom transformations/aggregations with:\n\npd.df.apply() or pd.df.transform() apply a function row-wise or column-wise\npd.df.agg() or pd.df.aggregate()"
  },
  {
    "objectID": "slides/week2-transform.html#custom-functions-1",
    "href": "slides/week2-transform.html#custom-functions-1",
    "title": "Dataframe Transformations",
    "section": "Custom functions",
    "text": "Custom functions\nHere’s an example:\n\n\n\n\n\n\n  \n    \n      \n      1961\n      1962\n      1963\n      1964\n      1965\n      1966\n      1967\n      1968\n      1969\n      1970\n      ...\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Country Name\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Argentina\n      5.427843\n      -0.852022\n      -5.308197\n      10.130298\n      10.569433\n      -0.659726\n      3.191997\n      4.822501\n      9.679526\n      3.045643\n      ...\n      10.125398\n      6.003952\n      -1.026420\n      2.405324\n      -2.512615\n      2.731160\n      -2.080328\n      2.818503\n      -2.565352\n      -2.088015\n    \n    \n      Australia\n      2.485769\n      1.296087\n      6.214630\n      6.978522\n      5.983506\n      2.382458\n      6.302620\n      5.095814\n      7.044329\n      7.172187\n      ...\n      2.067417\n      2.462756\n      3.918163\n      2.584898\n      2.533115\n      2.192647\n      2.770652\n      2.300611\n      2.949286\n      2.160956\n    \n  \n\n2 rows × 59 columns\n\n\n\n\n\n# convert percentages to proportions\ngdp_prop = gdp.transform(lambda x: x/100 + 1)\n\n# compute geometric mean\ngdp_prop.aggregate(\n    lambda x: np.prod(x)**(1/len(x)), \n    axis = 1).head(4)\n\nCountry Name\nArgentina    1.022831\nAustralia    1.034228\nAustria      1.027254\nBurundi      1.023854\ndtype: float64"
  },
  {
    "objectID": "slides/week2-transform.html#your-turn-1",
    "href": "slides/week2-transform.html#your-turn-1",
    "title": "Dataframe Transformations",
    "section": "Your turn",
    "text": "Your turn\nHere’s the country with the highest annualized GDP growth for the period 1961-2019:\n\n\nCountry Name\nBotswana    1.079442\ndtype: float64\n\n\n\nHow did I find this? Suppose that the result on the previous slide were stored as gdp_annualized. Write a line of code that generates the result shown above."
  },
  {
    "objectID": "slides/week2-transform.html#grouped-aggregations",
    "href": "slides/week2-transform.html#grouped-aggregations",
    "title": "Dataframe Transformations",
    "section": "Grouped aggregations",
    "text": "Grouped aggregations\nSuppose we wanted to compute annualized growth by decade for each country.\n\nTo do so, we’d compute the same aggregation (geometric mean) repeatedly for subsets of data values. This is called a grouped aggregation.\n\n\nUsually, one defines a grouping of dataframe rows using columns in the dataset. For example:\n\ngdp_decades.head(4)\n\n\n\n\n\n  \n    \n      \n      Country Name\n      growth\n      decade\n    \n  \n  \n    \n      0\n      Argentina\n      1.054278\n      1960\n    \n    \n      1\n      Australia\n      1.024858\n      1960\n    \n    \n      2\n      Austria\n      1.055380\n      1960\n    \n    \n      3\n      Burundi\n      0.862539\n      1960\n    \n  \n\n\n\n\n\n\nHow should the rows be grouped?"
  },
  {
    "objectID": "slides/week2-transform.html#groupby",
    "href": "slides/week2-transform.html#groupby",
    "title": "Dataframe Transformations",
    "section": ".groupby",
    "text": ".groupby\nIn pandas, df.groupby('COLUMN') defines a grouping of dataframe rows in which each group is a set of rows with the same value of 'COLUMN'.\n\nThere will be exactly as many groups as the number of unique values in 'COLUMN'.\nMultiple columns may be specified to define a grouping, e.g., df.groupby(['COL1', 'COL2'])\nSubsequent operations will be performed group-wise"
  },
  {
    "objectID": "slides/week2-transform.html#annualized-gdp-growth-by-decade",
    "href": "slides/week2-transform.html#annualized-gdp-growth-by-decade",
    "title": "Dataframe Transformations",
    "section": "Annualized GDP growth by decade",
    "text": "Annualized GDP growth by decade\nReturning to our example:\n\ngdp_anngrowth = gdp_decades.groupby(\n    ['Country Name', 'decade']\n    ).aggregate(\n    lambda x: np.prod(x)**(1/len(x))\n    )\n\ngdp_anngrowth\n\n\n\n\n\n  \n    \n      \n      \n      growth\n    \n    \n      Country Name\n      decade\n      \n    \n  \n  \n    \n      Algeria\n      1960\n      1.030579\n    \n    \n      1970\n      1.068009\n    \n    \n      1980\n      1.027661\n    \n    \n      1990\n      1.015431\n    \n    \n      2000\n      1.038750\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      Zimbabwe\n      1970\n      1.038505\n    \n    \n      1980\n      1.051066\n    \n    \n      1990\n      1.027630\n    \n    \n      2000\n      0.944765\n    \n    \n      2010\n      1.055855\n    \n  \n\n714 rows × 1 columns"
  },
  {
    "objectID": "slides/week2-transform.html#your-turn-2",
    "href": "slides/week2-transform.html#your-turn-2",
    "title": "Dataframe Transformations",
    "section": "Your turn",
    "text": "Your turn\nHow do you find the country with the highest annualized GDP growth for each decade?\n\nWrite a line of code that would perform this calculation.\n\ngdp_anngrowth...\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      growth\n    \n    \n      decade\n      \n    \n  \n  \n    \n      1960\n      (Iran, Islamic Rep., 1960)\n    \n    \n      1970\n      (Botswana, 1970)\n    \n    \n      1980\n      (Botswana, 1980)\n    \n    \n      1990\n      (China, 1990)\n    \n    \n      2000\n      (Myanmar, 2000)\n    \n    \n      2010\n      (China, 2010)"
  },
  {
    "objectID": "slides/week2-transform.html#recap",
    "href": "slides/week2-transform.html#recap",
    "title": "Dataframe Transformations",
    "section": "Recap",
    "text": "Recap\n\nIn tidy data, rows and columns correspond to observations and variables.\n\nThis provides a standard dataset structure that facilitates exploration and analysis.\nMany datasets are not stored in this format.\nTransformation operations are a lot easier with tidy data, due in part to the way tools in pandas are designed.\n\nTransformations are operations that modify the shape or values of dataframes. We discussed\n\nslicing\nfiltering\ncreating new variables\naggregations (mean, min, max, argmin, etc.)\ngrouped aggregations\n\nDataframe manipulations will be used throughout the course to tidy up data and perform various inspections and summaries."
  },
  {
    "objectID": "slides/week2-transform.html#up-next",
    "href": "slides/week2-transform.html#up-next",
    "title": "Dataframe Transformations",
    "section": "Up next",
    "text": "Up next\nWe started en media res at this stage of the lifecyle (tidy) so that you could start developing skills that would enable you to jump right into playing with datasets.\n\nNext week, we’ll backtrack to the data collection and assessment stages of a project and discuss:\n\nsampling\nscope of inference\ndata assessment\nmissing data"
  },
  {
    "objectID": "slides/week3-casestudy.html#the-miller-case",
    "href": "slides/week3-casestudy.html#the-miller-case",
    "title": "Case study on sampling and missingness",
    "section": "The Miller case",
    "text": "The Miller case\nOn November 21, 2020, a professor at Williams College, Steven Miller, filed an affidavit alleging that an analysis of phone surveys showed that among registered republican voters in PA:\n\n~40K mail ballots were fraudlently requested;\n~48K mail ballots were not counted.\n\n\n\n“President Donald J. Trump amplified the statement in a tweet, the Chairman of the Federal Elections Commission (FEC) referenced the statement as indicative of fraud, and a conservative group prominently featured it in a legal brief seeking to overturn the Pennsylvania election results.” (Samuel Wolf, Williams Record, 11/25/20)\n\n\n\nThe Miller affidavit was criticized by statisticians as incorrect, irresponsible, and unethical.\n\n\nWe’ll focus on the first claim."
  },
  {
    "objectID": "slides/week3-casestudy.html#the-calculation",
    "href": "slides/week3-casestudy.html#the-calculation",
    "title": "Case study on sampling and missingness",
    "section": "The calculation",
    "text": "The calculation\nOn a purely mathematical level, the calculations were straightforward.\n\n\\(N = 165,412\\) mail ballots were requested by registered republicans but not returned.\nPhone surveys of \\(n = 2250\\) of those voters identified \\(556\\) who claimed not to request ballots.\n\n\n\\[\n\\text{sample proportion} \\times \\text{population size} = \\frac{556}{2250} \\times 165,412 = 40,875\n\\]"
  },
  {
    "objectID": "slides/week3-casestudy.html#the-flawed-assumption",
    "href": "slides/week3-casestudy.html#the-flawed-assumption",
    "title": "Case study on sampling and missingness",
    "section": "The flawed assumption",
    "text": "The flawed assumption\nThe key issue was a single flawed assumption:\n\n\n“The analysis is predicated on the assumption that the responders are a representative sample of the population of registered Republicans in Pennsylvania for whom a mail-in ballot was requested but not counted, and responded accurately to the questions during the phone calls.”” (Miller affidavit)\n\n\n\nEssentially, two critical mistakes were made in the analysis:\n\nFailure to critically assess the sampling design and scope of inference.\nIgnoring missing data.\n\n\n\nMiller is a number theorist, not a trained survey statistician, so on some level his mistakes were understandable, but they did a lot of damage. He issued an apology in short order."
  },
  {
    "objectID": "slides/week3-casestudy.html#the-survey",
    "href": "slides/week3-casestudy.html#the-survey",
    "title": "Case study on sampling and missingness",
    "section": "The survey",
    "text": "The survey\n\nThere were 165,412 unreturned mail ballots requested by registered republicans in PA.\n\n\nThose voters were surveyed by phone by Matt Braynard’s private firm External Affairs on behalf of the Voter Integrity Fund.\n\n\nWe don’t really know how they obtained and selected phone numbers or exactly what the survey procedure was, but here’s what we do know:\n\n~23K individuals were called on Nov. 9-10.\nThe ~2.5K who answered were asked if they were the registered voter or a family member.\nIf they said yes, they were asked if they requested a ballot.\nThose who requested a ballot were asked if they mailed it."
  },
  {
    "objectID": "slides/week3-casestudy.html#potential-sources-of-bias",
    "href": "slides/week3-casestudy.html#potential-sources-of-bias",
    "title": "Case study on sampling and missingness",
    "section": "Potential sources of bias",
    "text": "Potential sources of bias\nThere are several obvious sampling problems.\n\nUndisclosed selection mechanism\nNarrow snapshot in time; 9th and 10th were a Monday and Tuesday, less likely to reach workers.\nUnknown at this time whether mail ballots were ultimately returned and counted or not by this time, so the frame doesn’t align with the population.\n\n\nThere are also some obvious measurement problems.\n\nFamily members could answer on behalf of one another and may give incorrect answers.\nRespondents may not be aware that they requested a ballot, as you don’t have to file an explicit request in Pennsylvania (there’s a checkbox on the voter registration form).\nVoters who believed they did not request a ballot were not asked if they recieved and/or returned one."
  },
  {
    "objectID": "slides/week3-casestudy.html#survey-schematic",
    "href": "slides/week3-casestudy.html#survey-schematic",
    "title": "Case study on sampling and missingness",
    "section": "Survey schematic",
    "text": "Survey schematic"
  },
  {
    "objectID": "slides/week3-casestudy.html#sampling-design",
    "href": "slides/week3-casestudy.html#sampling-design",
    "title": "Case study on sampling and missingness",
    "section": "Sampling design",
    "text": "Sampling design\nPopulation: republicans registered to vote in PA who had unreturned mail ballots issued\n\nSampling frame: unknown; source of phone numbers unspecified.\n\n\nSample: 2684 registered republicans or family members of registered repbulicans who had a mail ballot officially requested in PA and answered survey calls on Nov. 9 or 10.\n\n\nSampling mechanism: nonrandom; depends on availability during calling hours on Monday and Tuesday, language spoken, and willingness to talk.\n\n\nThis is not a representative sample of any meaningful population."
  },
  {
    "objectID": "slides/week3-casestudy.html#missingness",
    "href": "slides/week3-casestudy.html#missingness",
    "title": "Case study on sampling and missingness",
    "section": "Missingness",
    "text": "Missingness\nRespondents hung up at every stage of the survey. This is not at random – individuals who do not believe there were any irregularities in mail ballots are less likely to talk or continue talking.\n\nSo data are MNAR and over-represent people more likely to claim they never requested a ballot."
  },
  {
    "objectID": "slides/week3-casestudy.html#the-analysis",
    "href": "slides/week3-casestudy.html#the-analysis",
    "title": "Case study on sampling and missingness",
    "section": "The analysis",
    "text": "The analysis\nMiller first calculated the proportion of respondents who reported not requesting ballots among those who did not hang up after the first question.\n\\[\n\\left(\\frac{556}{1150 + 556 + 544}\\right) = 0.2471\n\\]\n\nThen he extrapolated that the estimated number of fraudulent requests was:\n\\[\n0.2471 \\times 165,412 = 40,875\n\\]\n\n\nThe two main problems with this are:\n\nnonrandom sampling \\(\\Longrightarrow\\) no scope of inference\nno adjustment for nonresponse (i.e., missing data)"
  },
  {
    "objectID": "slides/week3-casestudy.html#simulation",
    "href": "slides/week3-casestudy.html#simulation",
    "title": "Case study on sampling and missingness",
    "section": "Simulation",
    "text": "Simulation\nIt’s not too tricky to envision sources of bias that would affect the results.\n\nAssume that:\n\nrespondents all know whether they actually requested a ballot\nrespondents tell the truth\nrespondents who didn’t request a ballot are more likely to be reached\nrespondents who did request a ballot are more likely to hang up during the interview\n\n\n\nThen we can show through a simple simulation that an actual fraud rate of under 1% will be estimated at over 20% almost all the time."
  },
  {
    "objectID": "slides/week3-casestudy.html#simulated-population",
    "href": "slides/week3-casestudy.html#simulated-population",
    "title": "Case study on sampling and missingness",
    "section": "Simulated population",
    "text": "Simulated population\nFirst let’s generate a population of 150K voters.\n\n\nnp.random.seed(41021)\n\n# proportion of fraudlent requests\ntrue_prop = 0.009\n\n# generate population of voters who had unreturned mail ballots\nN = 150000\npopulation = pd.DataFrame(data = {'requested': np.ones(N)})\n\n# how many didn't request mail ballots?\nnum_nrequest = round(N*true_prop) - 1\n\n# set the 'requested' indicator to zero for the top chunk of data\npopulation.iloc[0:num_nrequest, 0] = 0"
  },
  {
    "objectID": "slides/week3-casestudy.html#simulated-sample",
    "href": "slides/week3-casestudy.html#simulated-sample",
    "title": "Case study on sampling and missingness",
    "section": "Simulated sample",
    "text": "Simulated sample\nThen let’s introduce sampling weights based on the conditional probability that an individual will talk with the interviewer given whether they requested a ballot or not.\n\n\n# assume respondents tell the truth\np_request = 1 - true_prop\np_nrequest = true_prop\n\n# non-requesters are more likely to talk by this factor\ntalk_factor = 15\n\n# observed response rate\np_talk = 0.09\n\n# conditional response rates\np_talk_request = p_talk/(p_request + talk_factor*p_nrequest) \np_talk_nrequest = talk_factor*p_talk_request\n\n# append conditional response rates as weights\npopulation.loc[population.requested == 1, 'sample_weight'] = p_talk_request\npopulation.loc[population.requested == 0, 'sample_weight'] = p_talk_nrequest\n\n# draw weighted sample\nnp.random.seed(41923)\nsamp_complete = population.sample(n = 2500, replace = False, weights = 'sample_weight')\n\n\n\nThink of the weights as conditional response rates."
  },
  {
    "objectID": "slides/week3-casestudy.html#simulated-missing-mechanism",
    "href": "slides/week3-casestudy.html#simulated-missing-mechanism",
    "title": "Case study on sampling and missingness",
    "section": "Simulated missing mechanism",
    "text": "Simulated missing mechanism\nThen let’s introduce missing values at different rates for respondents who requested a ballot and respondents who didn’t.\n\n\n# requesters are more likely to hang up by this factor\nmissing_factor = 10\n\n# overall nonresponse rate\np_missing = 0.6\n\n# conditional probabilities of missing given request status\np_missing_nrequest = p_missing/(p_nrequest + missing_factor*p_request) \np_missing_request = missing_factor*p_missing_nrequest\n\n# append missingness weights to sample\nsamp_complete.loc[samp_complete.requested == 1, 'missing_weight'] = p_missing_request\nsamp_complete.loc[samp_complete.requested == 0, 'missing_weight'] = p_missing_nrequest\n\n# make a copy of the sample\nsamp_incomplete = samp_complete.copy()\n\n# input missing values at random\nnp.random.seed(41923)\nsamp_incomplete['missing'] = np.random.binomial(n = 1, p = samp_incomplete.missing_weight.values)\nsamp_incomplete.loc[samp_incomplete.missing == 1, 'requested'] = float('nan')"
  },
  {
    "objectID": "slides/week3-casestudy.html#simulated-result",
    "href": "slides/week3-casestudy.html#simulated-result",
    "title": "Case study on sampling and missingness",
    "section": "Simulated result",
    "text": "Simulated result\nIf we then drop all the missing values and calculate the proportion of respondents who didn’t request a ballot, we get:\n\n\n# compute mean after dropping missing values\n1 - samp_incomplete.requested.mean()\n\n0.2395470383275261\n\n\n\n\nSo Miller’s result is expected if the sampling and missing mechanisms introduce bias, even if the true rate of fraudulent requests is under 1% – on the order of 1,000 ballots."
  },
  {
    "objectID": "slides/week3-casestudy.html#in-class-activity",
    "href": "slides/week3-casestudy.html#in-class-activity",
    "title": "Case study on sampling and missingness",
    "section": "In class activity",
    "text": "In class activity\nOpen the notebook and try it for yourself.\n\nSome comments:\n\nI chose these settings specifically to replicate the \\(24\\%\\) estimate.\nI inflated the nonresponse rate relative to the survey to achieve this.\nThe simulation is not an exact model of the actual survey; the survey has other sources of bias.\nYou should try inputting settings that you think are realistic. You’ll still see significant bias."
  },
  {
    "objectID": "slides/week3-casestudy.html#professional-ethics",
    "href": "slides/week3-casestudy.html#professional-ethics",
    "title": "Case study on sampling and missingness",
    "section": "Professional ethics",
    "text": "Professional ethics\nThe American Statistical Association publishes ethical guidelines for statistical practice. The Miller case violated a large number of these, most prominently, that an ethical practitioner:\n\nReports the sources and assessed adequacy of the data, accounts for all data considered in a study, and explains the sample(s) actually used.\nIn publications and reports, conveys the findings in ways that are both honest and meaningful to the user/reader. This includes tables, models, and graphics.\nIn publications or testimony, identifies the ultimate financial sponsor of the study, the stated purpose, and the intended use of the study results.\nWhen reporting analyses of volunteer data or other data that may not be representative of a defined population, includes appropriate disclaimers and, if used, appropriate weighting."
  },
  {
    "objectID": "slides/week3-sampling.html#announcements",
    "href": "slides/week3-sampling.html#announcements",
    "title": "Sampling and missingness",
    "section": "Announcements",
    "text": "Announcements\n\nFirst mini project released: air quality in U.S. cities\nLab 1 (pandas) due Monday 4/17 11:59pm PST\nHW 1 due in one week on Monday 4/24"
  },
  {
    "objectID": "slides/week3-sampling.html#this-week",
    "href": "slides/week3-sampling.html#this-week",
    "title": "Sampling and missingness",
    "section": "This week",
    "text": "This week\nObjective: Enable you to critically assess data quality based on how it was collected.\n\nSampling and statistical bias\n\nSampling terminology\nCommon sampling scenarios\nSampling mechanisms\nStatistical bias\n\nThe missing data problem\n\nTypes of missingness: MCAR, MAR, and MNAR\nPitfalls and simple fixes\n\nCase study: voter fraud\n\nSteven Miller’s analysis of Voter Integrity Fund surveys\nSources of bias\nEthical considerations"
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-terminology",
    "href": "slides/week3-sampling.html#sampling-terminology",
    "title": "Sampling and missingness",
    "section": "Sampling terminology",
    "text": "Sampling terminology\nHere we’ll introduce standard statistical terminology to describe data collection.\n\nAll data are collected somehow. A sampling design is a way of selecting observational units for measurement. It can be construed as a particular relationship between:\n\na population (all entities of interest);\na sampling frame (all entities that are possible to measure); and\na sample (a specific collection of entities)."
  },
  {
    "objectID": "slides/week3-sampling.html#population",
    "href": "slides/week3-sampling.html#population",
    "title": "Sampling and missingness",
    "section": "Population",
    "text": "Population\nLast week, we introduced the terminology observational unit to mean the entity measured for a study – datasets consist of observations made on observational units.\n\nIn less technical terms, all data are data on some kind of thing, such as countries, species, locations, and the like.\n\n\n\n\nA statistical population is the collection of all units of interest. For example:\n\nall countries (GDP data)\nall mammal species (Allison 1976)\nall babies born in the US (babynames data)\nall locations in a region (SB weather data)\nall adult U.S. residents (BRFSS data)"
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-frame",
    "href": "slides/week3-sampling.html#sampling-frame",
    "title": "Sampling and missingness",
    "section": "Sampling frame",
    "text": "Sampling frame\nThere are usually some units in a population that can’t be measured due to practical constraints – for instance, many adult U.S. residents don’t have phones or addresses.\n\n\n\nFor this reason, it is useful to introduce the concept of a sampling frame, which refers to the collection of all units in a population that can be observed for a study. For example:\n\nall countries reporting economic output between 1961 and 2019\nall babies with birth certificates from U.S. hospitals born between 1990 and 2018\nall adult U.S. residents with phone numbers in 2019"
  },
  {
    "objectID": "slides/week3-sampling.html#sample",
    "href": "slides/week3-sampling.html#sample",
    "title": "Sampling and missingness",
    "section": "Sample",
    "text": "Sample\nFinally, it’s rarely feasible to measure every observable unit due to limited data collection resources – for instance, states don’t have the time or money to call every phone number every year.\n\n\n\nA sample is a subcollection of units in the sampling frame actually selected for study. For instance:\n\n234 countries;\n62 mammal species;\n13,684,689 babies born in CA;\n1 weather station location at SB airport;\n418,268 adult U.S. residents."
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-scenarios",
    "href": "slides/week3-sampling.html#sampling-scenarios",
    "title": "Sampling and missingness",
    "section": "Sampling scenarios",
    "text": "Sampling scenarios\nWe can now imagine a few common sampling scenarios by varying the relationship between population, frame, and sample.\n\nDenote an observational unit by \\(U_i\\), and let:\n\\[\\begin{alignat*}{2}\n\\mathcal{U} &= \\{U_i\\}_{i \\in I} &&\\quad(\\text{universe}) \\\\\nP &= \\{U_1, \\dots, U_N\\} \\subseteq \\mathcal{U} &&\\quad(\\text{population}) \\\\\n    F &= \\{U_j: j \\in J \\subset I\\} \\subseteq P &&\\quad(\\text{frame})\\\\\n    S &\\subseteq F &&\\quad(\\text{sample})\n\\end{alignat*}\\]"
  },
  {
    "objectID": "slides/week3-sampling.html#census",
    "href": "slides/week3-sampling.html#census",
    "title": "Sampling and missingness",
    "section": "Census",
    "text": "Census\nThe simplest scenario is a population census, where the entire population is observed.\n\n\n\nFor a census: \\(S = F = P\\)\nAll properties of the population are definitevely known in a census. So there is no need to model census data."
  },
  {
    "objectID": "slides/week3-sampling.html#simple-random-sample",
    "href": "slides/week3-sampling.html#simple-random-sample",
    "title": "Sampling and missingness",
    "section": "Simple random sample",
    "text": "Simple random sample\nThe statistical gold standard for inference, modeling, and prediction is the simple random sample in which units are selected at random from the population.\n\n\n\nFor a simple random sample: \\(S \\subset F = P\\)\nSample properties are reflective of population properties in simple random samples. Population inference is straightforward."
  },
  {
    "objectID": "slides/week3-sampling.html#typical-sample",
    "href": "slides/week3-sampling.html#typical-sample",
    "title": "Sampling and missingness",
    "section": "‘Typical’ sample",
    "text": "‘Typical’ sample\nMore common in practice is a random sample from a sampling frame that overlaps but does not cover the population.\n\n\n\nFor a ‘typical’ sample: \\(S \\subset F \\quad\\text{and}\\quad F \\cap P \\neq \\emptyset\\)\nSample properties are reflective of the frame but not necessarily the study population. Population inference gets more complicated and may not be possible."
  },
  {
    "objectID": "slides/week3-sampling.html#administrative-data",
    "href": "slides/week3-sampling.html#administrative-data",
    "title": "Sampling and missingness",
    "section": "‘Administrative’ data",
    "text": "‘Administrative’ data\nAlso common is administrative data in which all units are selected from a convenient frame that partly covers the population.\n\n\n\nFor administrative data: \\(S = F \\quad\\text{and}\\quad F\\cap P \\neq \\emptyset\\)\nAdministrative data are not really proper samples; they cannot be replicated and they do not represent any broader group. No inference is possible."
  },
  {
    "objectID": "slides/week3-sampling.html#scope-of-inference",
    "href": "slides/week3-sampling.html#scope-of-inference",
    "title": "Sampling and missingness",
    "section": "Scope of inference",
    "text": "Scope of inference\nThe relationships among the population, frame, and sample determine the scope of inference: the extent to which conclusions based on the sample are generalizable.\n\nA good sampling design can ensure that the statistical properties of the sample are expected to match those of the population. If so, it is sound to generalize:\n\nthe sample is said to be representative of the population\nthe scope of inference is broad\n\n\n\nA poor sampling design will produce samples that distort the statistical properties of the population. If so, it is not sound to generalize:\n\nsample statistics are subjet to bias\nthe scope of inference is narrow"
  },
  {
    "objectID": "slides/week3-sampling.html#characterizing-sampling-designs",
    "href": "slides/week3-sampling.html#characterizing-sampling-designs",
    "title": "Sampling and missingness",
    "section": "Characterizing sampling designs",
    "text": "Characterizing sampling designs\nThe sampling scenarios above can be differentiated along two key attributes:\n\nThe overlap between the sampling frame and the population.\n\nframe \\(=\\) population\nframe \\(\\subset\\) population\nframe \\(\\cap\\) population \\(\\neq \\emptyset\\)\n\nThe mechanism of obtaining a sample from the sampling frame.\n\nrandom sampling\nconvenience sampling\n\n\n\nIf you can articulate these two points, you have fully characterized the sampling design."
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-mechanisms",
    "href": "slides/week3-sampling.html#sampling-mechanisms",
    "title": "Sampling and missingness",
    "section": "Sampling mechanisms",
    "text": "Sampling mechanisms\nIn order to describe sampling mechanisms precisely, we need a little terminology.\n\nEach unit has some inclusion probability – the probability of being included in the sample.\n\n\nLet’s suppose that the frame \\(F\\) comprises \\(N\\) units, and denote the inclusion probabilities by:\n\\[\np_i = P(\\text{unit } i \\text{ is included in the sample})\n\\quad i = 1, \\dots, N\n\\]\nThe inclusion probability of each unit depends on the physical procedure of collecting data."
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-mechanisms-1",
    "href": "slides/week3-sampling.html#sampling-mechanisms-1",
    "title": "Sampling and missingness",
    "section": "Sampling mechanisms",
    "text": "Sampling mechanisms\nSampling mechanisms are methods of drawing samples and are categorized into four types based on inclusion probabilities.\n\nin a census every unit is included\n\n\\(p_i = 1\\) for every unit \\(i = 1, \\dots, N\\)\n\nin a random sample every unit is equally likely to be included\n\n\\(p_i = p_j\\) for every pair of units \\(i, j\\)\n\nin a probability sample units have different inclusion probabilities\n\n\\(p_i \\neq p_j\\) for at least one \\(i \\neq j\\)\n\nin a nonrandom sample there is no random mechanism\n\n\\(p_i = 1\\) for \\(i \\in S\\)"
  },
  {
    "objectID": "slides/week3-sampling.html#revisiting-example-datasets-gdp",
    "href": "slides/week3-sampling.html#revisiting-example-datasets-gdp",
    "title": "Sampling and missingness",
    "section": "Revisiting example datasets: GDP",
    "text": "Revisiting example datasets: GDP\nAnnual observations of GDP growth for 234 countries from 1961 - 2018.\n\nPopulation: all countries in existence between 1961-2019.\nFrame: all countries reporting economic output for at least one year between 1961 and 2019.\nSample: equal to frame.\n\n\nSo:\n\nOverlap: frame partly overlaps population.\nMechanism: sample is every country in the sampling frame.\n\n\n\nThis is administrative data with no scope of inference."
  },
  {
    "objectID": "slides/week3-sampling.html#revisiting-example-datasets-brfss-data",
    "href": "slides/week3-sampling.html#revisiting-example-datasets-brfss-data",
    "title": "Sampling and missingness",
    "section": "Revisiting example datasets: BRFSS data",
    "text": "Revisiting example datasets: BRFSS data\nPhone surveys of 418K U.S. residents in 2019.\n\nPopulation: all U.S. residents.\nFrame: all adult U.S. residents with phone numbers.\nSample: 418K adult U.S. residents with phone numbers.\n\n\nSo:\n\nOverlap: frame is a subset of the population.\nMechanism: probability sample.\n\nRandomly selected phone numbers were dialed in each state, so individuals in less populous states or with multiple numbers are more likely to be included\n\n\n\n\nThis is a typical sample with narrow inference to adult residents with phone numbers."
  },
  {
    "objectID": "slides/week3-sampling.html#statistical-bias",
    "href": "slides/week3-sampling.html#statistical-bias",
    "title": "Sampling and missingness",
    "section": "Statistical bias",
    "text": "Statistical bias\nStatistical bias is the average difference between a sample property and a population property across all possible samples under a particular sampling design.\n\nIn less technical terms: the expected error of estimates.\n\n\nTwo possible sources of statistical bias:\n\nAn estimator systematically over- or under-estimates its target population property\n\ne.g., \\(\\frac{1}{n}\\sum_i (x_i - \\bar{x})^2\\) is biased for (underestimates) the population variance\n\nSampling design systematically over- or under-represents certain observational units\n\ne.g., studies conducted on college campuses are biased towards (overrepresent) young adults\n\n\n\n\nThese are distinct from other kinds of bias that we are not discussing:\n\nMeasurement bias: attributes or outcomes are measured unevenly across populations\nExperimenter bias: study design and/or outcomes favor an investigator’s preconceptions"
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-bias",
    "href": "slides/week3-sampling.html#sampling-bias",
    "title": "Sampling and missingness",
    "section": "Sampling bias",
    "text": "Sampling bias\nIn Lab 2 you’ll explore sampling bias arising from sampling mechanisms. Here’s a preview:\n\n\n\n\n\nDistributions of body length by sex (top) and in aggregate (bottom) for a hypothetical population of 5K hawks.\n\n\n\nConsider:\n\nAre males or females generally longer?\nHow will the sample mean shift if disproportionately more males are sampled?\nIf disproportionately more females are sampled?"
  },
  {
    "objectID": "slides/week3-sampling.html#bias-corrections",
    "href": "slides/week3-sampling.html#bias-corrections",
    "title": "Sampling and missingness",
    "section": "Bias corrections",
    "text": "Bias corrections\nIf inclusion probabilities are known or estimable it is possible to apply bias corrections to estimates using inverse probability weighting.\n\nIf\n\n\\(p_i\\) is the probability that individual \\(i\\) is included in the sample \\(S\\)\n\\(Y_i\\) are observations of a variable of interest\n\n\n\nThen a bias-corrected estimate of the population mean is given by the weighted average:\n\\[\n\\sum_{i\\in S} \\left(\\frac{p_i^{-1}}{\\sum_i p_i^{-1}}\\right) Y_i\n\\]"
  },
  {
    "objectID": "slides/week3-sampling.html#bias-correction-example",
    "href": "slides/week3-sampling.html#bias-correction-example",
    "title": "Sampling and missingness",
    "section": "Bias correction example",
    "text": "Bias correction example\nSuppose we obtain a biased sample in which female hawks were 6 times as likely to be selected as males. This yields an overestimate:\n\n\npopulation mean:  54.73771716352954\nsample mean:  56.567779534464016\n\n\n\nBut since we know the exact inclusion probabilities up to a proportionality constant, we can apply inverse probability weighting to adjust for bias:\n\n# specify weights s.t. 6:1 female:male\nweight_df = pd.DataFrame(\n    data = {'sex': np.array(['male', 'female']),\n            'weight': np.array([1, 6])})\n\n# append weights to sample\nsamp_w = pd.merge(samp, weight_df, how = 'left', on = 'sex')\n\n# calculate inverse probability weightings\nsamp_w['correction_factor'] = (1/samp_w.weight)/np.sum(1/samp_w.weight)\n\n# multiply observed values by weightings\nsamp_w['weighted_length'] = samp_w.length*samp_w.correction_factor\n\n# take weighted average\nsamp_w.weighted_length.sum()\n\n54.40928091743469"
  },
  {
    "objectID": "slides/week3-sampling.html#bias-correction-example-1",
    "href": "slides/week3-sampling.html#bias-correction-example-1",
    "title": "Sampling and missingness",
    "section": "Bias correction example",
    "text": "Bias correction example\nHowever, even if we didn’t know the exact inclusion probabilities, we could estimate them from the sample:\n\nsamp.sex.value_counts()\n\nfemale    88\nmale      12\nName: sex, dtype: int64\n\n\n\nAnd use the same approach:\n\n# estimate factor by which F more likely than M\nratio = samp.sex.value_counts().loc['female']/samp.sex.value_counts().loc['male']\n\n# input as weights\nweight_df = pd.DataFrame(data = {'sex': np.array(['male', 'female']), 'weight': np.array([1, ratio])})\n\n# append weights to sample\nsamp_w = pd.merge(samp, weight_df, how = 'left', on = 'sex')\n\n# calculate inverse probability weightings\nsamp_w['correction_factor'] = (1/samp_w.weight)/np.sum(1/samp_w.weight)\n\n# multiply observed values by weightings\nsamp_w['weighted_length'] = samp_w.length*samp_w.correction_factor\n\n# take weighted average\nsamp_w.weighted_length.sum()\n\n54.082235672430265"
  },
  {
    "objectID": "slides/week3-sampling.html#remarks-on-ipw-and-bias-correction",
    "href": "slides/week3-sampling.html#remarks-on-ipw-and-bias-correction",
    "title": "Sampling and missingness",
    "section": "Remarks on IPW and bias correction",
    "text": "Remarks on IPW and bias correction\nInverse probability weighting can be applied to correct a wide range of estimators besides averages.\n\nIt is also applicable to adjust for bias due to missing data.\n\n\nIn principle, the technique is simple, but in practice, there are some common hurdles:\n\nusually inclusion probabilities are not known\nestimating inclusion probabilities can be difficult and messy"
  },
  {
    "objectID": "slides/week3-sampling.html#missingness",
    "href": "slides/week3-sampling.html#missingness",
    "title": "Sampling and missingness",
    "section": "Missingness",
    "text": "Missingness\nMissing data arise when one or more variable measurements fail for a subset of observations.\n\nThis can happen for a variety of reasons, but is very common in pratice due to, for instance:\n\nequipment failure;\nsample contamination or loss;\nrespondents leaving questions blank;\nattrition (dropping out) of study participants.\n\n\n\nMany researchers and data scientists ignore missingness by simply deleting affected observations, but this is bad practice! Missingness needs to be treated carefully."
  },
  {
    "objectID": "slides/week3-sampling.html#missing-representations",
    "href": "slides/week3-sampling.html#missing-representations",
    "title": "Sampling and missingness",
    "section": "Missing representations",
    "text": "Missing representations\nIt is standard practice to record observations with missingness but enter a special symbol (.., -, NA, etcetera) for missing values.\n\nIn python, missing values are mapped to a special float:\n\n\n\nfloat('nan')\n\nnan"
  },
  {
    "objectID": "slides/week3-sampling.html#missing-representations-1",
    "href": "slides/week3-sampling.html#missing-representations-1",
    "title": "Sampling and missingness",
    "section": "Missing representations",
    "text": "Missing representations\nHere is some made-up data with two missing values:\n\n\n\n\n\n\n\n  \n    \n      \n      value\n    \n    \n      obs\n      \n    \n  \n  \n    \n      0\n      -0.9286936933427271\n    \n    \n      1\n      -0.3088381742999848\n    \n    \n      2\n      -\n    \n    \n      3\n      -1.4345064041945543\n    \n    \n      4\n      0.03958917896644836\n    \n    \n      5\n      -\n    \n    \n      6\n      -0.5316890502224456\n    \n    \n      7\n      1.4734842645335422"
  },
  {
    "objectID": "slides/week3-sampling.html#missing-representations-2",
    "href": "slides/week3-sampling.html#missing-representations-2",
    "title": "Sampling and missingness",
    "section": "Missing representations",
    "text": "Missing representations\nIf we read in the file with an na_values argument, pandas will parse the specified characters as NaN:\n\n\nsome_data = pd.read_csv('data/some_data.csv', index_col = 'obs', na_values = '-')\nsome_data\n\n\n\n\n\n  \n    \n      \n      value\n    \n    \n      obs\n      \n    \n  \n  \n    \n      0\n      -0.928694\n    \n    \n      1\n      -0.308838\n    \n    \n      2\n      NaN\n    \n    \n      3\n      -1.434506\n    \n    \n      4\n      0.039589\n    \n    \n      5\n      NaN\n    \n    \n      6\n      -0.531689\n    \n    \n      7\n      1.473484"
  },
  {
    "objectID": "slides/week3-sampling.html#calculations-with-nans",
    "href": "slides/week3-sampling.html#calculations-with-nans",
    "title": "Sampling and missingness",
    "section": "Calculations with NaNs",
    "text": "Calculations with NaNs\nNaNs halt calculations on numpy arrays.\n\n# mean in numpy -- halt\nsome_data.values.mean()\n\nnan\n\n\n\nHowever, the default behavior in pandas is to ignore the NaN’s, which allows the computation to proceed:\n\n\n\n# mean in pandas -- ignore\nsome_data.mean()\n\nvalue   -0.281776\ndtype: float64"
  },
  {
    "objectID": "slides/week3-sampling.html#omitting-missing-values-alters-results",
    "href": "slides/week3-sampling.html#omitting-missing-values-alters-results",
    "title": "Sampling and missingness",
    "section": "Omitting missing values alters results",
    "text": "Omitting missing values alters results\nBut those missing values could have been anything. For example:\n\n\n# one counterfactual scenario\ncomplete_data = some_data.copy()\ncomplete_data.loc[[2, 5], 'value'] = [5, 6] \n\n\n\nNow the mean is:\n\ncomplete_data.mean()\n\nvalue    1.163668\ndtype: float64\n\n\n\n\nSo missing values can dramatically alter results if they are simply omitted from calculations!"
  },
  {
    "objectID": "slides/week3-sampling.html#the-missing-data-problem",
    "href": "slides/week3-sampling.html#the-missing-data-problem",
    "title": "Sampling and missingness",
    "section": "The missing data problem",
    "text": "The missing data problem\nIn a nutshell, the missing data problem is: how should missing values be handled in a data analysis?\n\n\nGetting the software to run is one thing, but this alone does not address the challenges posed by the missing data. Unless the analyst, or the software vendor, provides some way to work around the missing values, the analysis cannot continue because calculations on missing values are not possible. There are many approaches to circumvent this problem. Each of these affects the end result in a different way. (Stef van Buuren, 2018)\n\n\n\nThere’s no universal approach to the missing data problem. The choice of method depends on:\n\nthe analysis objective;\nthe missing data mechanism."
  },
  {
    "objectID": "slides/week3-sampling.html#missing-data-in-pstat100",
    "href": "slides/week3-sampling.html#missing-data-in-pstat100",
    "title": "Sampling and missingness",
    "section": "Missing data in PSTAT100",
    "text": "Missing data in PSTAT100\nWe won’t go too far into this topic in PSTAT 100. Our goal will be awareness-raising, specifically:\n\ncharacterizing types of missingness (missing data mechanisms);\nunderstanding missingness as a potential source of bias;\nbasic do’s and don’t’s when it comes to missingness.\n\n\nIf you are interested in the topic, Stef van Buuren’s Flexible Imputation of Missing Data (the source of one of your readings this week) provides an excellent introduction."
  },
  {
    "objectID": "slides/week3-sampling.html#missing-data-mechanisms",
    "href": "slides/week3-sampling.html#missing-data-mechanisms",
    "title": "Sampling and missingness",
    "section": "Missing data mechanisms",
    "text": "Missing data mechanisms\nMissing data mechanisms (like sampling mechanisms) are characterized by the probabilities that observations go missing.\n\nFor dataset \\(X = \\{x_{ij}\\}\\) comprising\n\n\\(n\\) rows/observations\n\\(p\\) columns/variables\n\n\n\ndenote the probability that a value goes missing as:\n\\[\nq_{ij} = P(x_{ij} \\text{ is missing})\n\\]"
  },
  {
    "objectID": "slides/week3-sampling.html#missing-completely-at-random",
    "href": "slides/week3-sampling.html#missing-completely-at-random",
    "title": "Sampling and missingness",
    "section": "Missing completely at random",
    "text": "Missing completely at random\nData are missing completely at random (MCAR) if the probabilities of missing entries are uniformly equal.\n\n\\[\nq_{ij} = q\n\\quad\\text{for all}\\quad\ni = 1, \\dots, n\n\\quad\\text{and}\\quad\nj = 1, \\dots, p\n\\]\n\n\nThis implies that the cause of missingness is unrelated to the data: missing values can be ignored. This is the easiest scenario to handle."
  },
  {
    "objectID": "slides/week3-sampling.html#missing-at-random",
    "href": "slides/week3-sampling.html#missing-at-random",
    "title": "Sampling and missingness",
    "section": "Missing at random",
    "text": "Missing at random\nData are missing at random (MAR) if the probabilities of missing entries depend on observed data.\n\n\\[\nq_{ij} = f(\\mathbf{x}_i)\n\\]\n\n\nThis implies that information about the cause of missingness is captured within the dataset. As a result:\n\nit is possible to estimate \\(q_{ij}\\)\nbias corrections using inverse probability weighting can be implemented"
  },
  {
    "objectID": "slides/week3-sampling.html#missing-not-at-random",
    "href": "slides/week3-sampling.html#missing-not-at-random",
    "title": "Sampling and missingness",
    "section": "Missing not at random",
    "text": "Missing not at random\nData are missing not at random (MNAR) if the probabilities of missing entries depend on unobserved data.\n\n\\[\nq_{ij} = f(z_i, x_{ij}) \\quad z_i \\text{ unknown}\n\\]\n\n\nThis implies that information about the cause of missingness is unavailable. This is the most complicated scenario."
  },
  {
    "objectID": "slides/week3-sampling.html#assessing-the-missing-data-mechanism",
    "href": "slides/week3-sampling.html#assessing-the-missing-data-mechanism",
    "title": "Sampling and missingness",
    "section": "Assessing the missing data mechanism",
    "text": "Assessing the missing data mechanism\nImportantly, there is no easy diagnostic check to distinguish MCAR, MAR, and MNAR without measuring some of the missing data.\n\nSo in practice, usually one has to make an informed assumption based on knowledge of the data collection process."
  },
  {
    "objectID": "slides/week3-sampling.html#example-gdp-data",
    "href": "slides/week3-sampling.html#example-gdp-data",
    "title": "Sampling and missingness",
    "section": "Example: GDP data",
    "text": "Example: GDP data\nIn the GDP growth data, growth measurements are missing for many countries before a certain year.\n\nWe might be able to hypothesize about why – perhaps a country didn’t exist or didn’t keep reliable records for a period of time.However, the data as they are contain no additional information that might explain the cause of missingness.\n\n\nSo these data are MNAR."
  },
  {
    "objectID": "slides/week3-sampling.html#simple-fixes",
    "href": "slides/week3-sampling.html#simple-fixes",
    "title": "Sampling and missingness",
    "section": "Simple fixes",
    "text": "Simple fixes\nThe easiest approach to missing data is to drop observations with missing values: df.dropna().\n\nImplicitly assumes data are MCAR\nInduces bias if data are MAR or MNAR\n\n\nAnother simple fix is mean imputation, filling in missing values with the mean of the corresponding variable: df.fillna().\n\nOnly a good idea if a very small proportion of values are missing\nInduces bias if data are MAR or MNAR"
  },
  {
    "objectID": "slides/week3-sampling.html#perils-of-mean-imputation",
    "href": "slides/week3-sampling.html#perils-of-mean-imputation",
    "title": "Sampling and missingness",
    "section": "Perils of mean imputation",
    "text": "Perils of mean imputation\n\nImputing too many missing values distorts the distribution of sample values."
  },
  {
    "objectID": "slides/week3-sampling.html#other-common-approaches-to-missingness",
    "href": "slides/week3-sampling.html#other-common-approaches-to-missingness",
    "title": "Sampling and missingness",
    "section": "Other common approaches to missingness",
    "text": "Other common approaches to missingness\nWhen data are MCAR or MAR, one can:\n\nmodel the probability of missingness and apply bias corrections to estimated quantities using inverse probability weighting\nmodel the variables with missing observations as functions of the other variables and perform model-based imputation"
  },
  {
    "objectID": "slides/week3-sampling.html#dos-and-donts",
    "href": "slides/week3-sampling.html#dos-and-donts",
    "title": "Sampling and missingness",
    "section": "Do’s and don’t’s",
    "text": "Do’s and don’t’s\nDo:\n\nAlways check for missing values upon import.\n\nTabulate the proportion of observations with missingness\nTabulate the proportion of values for each variable that are missing\n\nTake time to find out the reasons data are missing.\n\nDetermine which outcomes are coded as missing.\nInvestigate the physical mechanisms involved.\n\nReport missing data if they are present.\n\nDon’t:\n\nRely on software defaults for handling missing values.\nDrop missing values if data are not MCAR."
  },
  {
    "objectID": "slides/week4-graphics.html#this-week-data-visualization",
    "href": "slides/week4-graphics.html#this-week-data-visualization",
    "title": "Statistical graphics",
    "section": "This week: data visualization",
    "text": "This week: data visualization\n\nUses of data visualization\n\nExploration\nPresentation\n\nStatistical graphics\n\nGraphical elements: axes, geometric objects, aesthetic attributes, and text\nBuilding graphics: mapping data to graphical elements\n\nSurvey of common graphics\n\none- and two-variable displays\n\nPrinciples of effective visualization\n\nEffective uses of aesthetics and layout\nCommon blunders"
  },
  {
    "objectID": "slides/week4-graphics.html#figure-credits",
    "href": "slides/week4-graphics.html#figure-credits",
    "title": "Statistical graphics",
    "section": "Figure credits",
    "text": "Figure credits\nMany of the figures from this week’s slides are from Claus Wilke’s Fundamentals of Data Visualization."
  },
  {
    "objectID": "slides/week4-graphics.html#notice-your-reaction",
    "href": "slides/week4-graphics.html#notice-your-reaction",
    "title": "Statistical graphics",
    "section": "Notice your reaction",
    "text": "Notice your reaction\n\n\nThis is great for a paper or technical report, but it takes effort to discern patterns; I’d much rather see a few plots, like achievement vs. year by grade and gender."
  },
  {
    "objectID": "slides/week4-graphics.html#uses-of-graphics",
    "href": "slides/week4-graphics.html#uses-of-graphics",
    "title": "Statistical graphics",
    "section": "Uses of graphics",
    "text": "Uses of graphics\nThere is a broad distinction between:\n\nexploratory graphics, which are intended to be seen only by analysts; and\npresentation graphics, which are intended to be seen by an audience.\n\n\nExploratory graphics are made quickly in large volumes, and usually not formatted too carefully. Think of them like the pages of a sketchbook.\n\n\nPresentation graphics are made slowly with great attention to detail. Think of them as exhibition artworks.\n\n\nThe two are not mutually exclusive: an especially helpful exploratory graphic is often worth developing as a presentation graphic to help an audience understand ‘what the data look like’."
  },
  {
    "objectID": "slides/week4-graphics.html#elements-of-statistical-graphics",
    "href": "slides/week4-graphics.html#elements-of-statistical-graphics",
    "title": "Statistical graphics",
    "section": "Elements of statistical graphics",
    "text": "Elements of statistical graphics\nStatistical graphics are actually quite simple. They consist of the following four elements:\n\nAxes\n\nReferences for all other graphical elements.\n\nGeometric objects\n\nPoints, lines, curves, filled regions, etc.\n\nAesthetic attributes\n\nColor, shape, size, opacity/transparency.\n\nText\n\nLabels, legends, and titles."
  },
  {
    "objectID": "slides/week4-graphics.html#axes",
    "href": "slides/week4-graphics.html#axes",
    "title": "Statistical graphics",
    "section": "Axes",
    "text": "Axes\nWe are all familiar with axes. The word axis literally means axle: an axis is an object that other things turn around.\n\nIn statistical graphics, axes establish positional references for locating any geometric object – line, point, polygon – on the graphic."
  },
  {
    "objectID": "slides/week4-graphics.html#geometric-objects",
    "href": "slides/week4-graphics.html#geometric-objects",
    "title": "Statistical graphics",
    "section": "Geometric objects",
    "text": "Geometric objects\nGeometric objects are the things depicted on a plot, whatever those may be; typically points, lines, polygons, and shapes."
  },
  {
    "objectID": "slides/week4-graphics.html#aesthetic-attributes",
    "href": "slides/week4-graphics.html#aesthetic-attributes",
    "title": "Statistical graphics",
    "section": "Aesthetic attributes",
    "text": "Aesthetic attributes\nFor us, aesthetics will mean qualities of geometric objects, like color or transparency.\n\nPrimary aesthetics in statistical graphics are:\n\nShape (for points)\nColor\nSize\nOpacity/transparency"
  },
  {
    "objectID": "slides/week4-graphics.html#text",
    "href": "slides/week4-graphics.html#text",
    "title": "Statistical graphics",
    "section": "Text",
    "text": "Text\nText is used to label axes, objects, legends, and specify titles.\n\nText may seem innocuous, but it is what creates story – text gives a plot its plot!"
  },
  {
    "objectID": "slides/week4-graphics.html#statistical-graphics-are-mappings",
    "href": "slides/week4-graphics.html#statistical-graphics-are-mappings",
    "title": "Statistical graphics",
    "section": "Statistical graphics are mappings",
    "text": "Statistical graphics are mappings\nStatistical graphics are mappings of dataframe columns and attributes to graphical elements: axes, geometric objects, and aesthetic attributes.\n\nFor a simple example, consider the following time series of Cuba’s population by year:\n\n\n\n\n\n\n\n\n\n\n\nMappings:\n\npopulation \\(\\longrightarrow\\) y coordinate of axis;\nyear \\(\\longrightarrow\\) x coordinate of axis;\nobservations \\(\\longrightarrow\\) line"
  },
  {
    "objectID": "slides/week4-graphics.html#mapping-columns-to-aesthetics",
    "href": "slides/week4-graphics.html#mapping-columns-to-aesthetics",
    "title": "Statistical graphics",
    "section": "Mapping columns to aesthetics",
    "text": "Mapping columns to aesthetics\nNow consider aggregated populations by global region and year:\n\n\n\n\n\n\n\n\n\n\nMappings:\n\npopulation \\(\\longrightarrow\\) y\nyear \\(\\longrightarrow\\) x\nregion \\(\\longrightarrow\\) color\nobservations \\(\\longrightarrow\\) line (groupwise by color)"
  },
  {
    "objectID": "slides/week4-graphics.html#using-aesthetics",
    "href": "slides/week4-graphics.html#using-aesthetics",
    "title": "Statistical graphics",
    "section": "Using aesthetics",
    "text": "Using aesthetics\nThe ability to map variables to the elements of a graphic is essential because it means we can display more than two variables at a time by leveraging aesthetic attributes.\n\nFor example, in lab you’ll begin with this scatterplot:\n\n\n\n\n\n\n\n\n\nEach point represents a country in a particular year. The graphic shows that life expectancy increases with GDP per capita."
  },
  {
    "objectID": "slides/week4-graphics.html#using-aesthetics-1",
    "href": "slides/week4-graphics.html#using-aesthetics-1",
    "title": "Statistical graphics",
    "section": "Using aesthetics",
    "text": "Using aesthetics\nIn the lab you’ll add aesthetic mappings step by step until arriving at this plot:\n\n\n\n\n\n\n\n\nThis figure displays the same x-y relationship as before, but together with time, continental region, and population.\n\n\nVerges on too complex."
  },
  {
    "objectID": "slides/week4-graphics.html#using-graphics-for-discovery",
    "href": "slides/week4-graphics.html#using-graphics-for-discovery",
    "title": "Statistical graphics",
    "section": "Using graphics for discovery",
    "text": "Using graphics for discovery\nFurther incorporating sex shows that GDP per capita is associated with differential life expectancy gaps between men and women:\n\n\n\n\n\n\n\n\nIn other words, on average women outlive men by longer in wealtheir countries.\n\nMaybe just by an additional 2-3 years in wealthier countries\nClear pattern but lots of variation for a given GDP/capita"
  },
  {
    "objectID": "slides/week4-graphics.html#altair",
    "href": "slides/week4-graphics.html#altair",
    "title": "Statistical graphics",
    "section": "Altair",
    "text": "Altair\nAltair, a python library, creates graphics exactly as described above: mapping columns of a dataframe to graphical elements.\n\nIt has a somewhat idiosyncratic syntactical pattern involving a “chart”, “marks”, and “encodings”:\n\n\n\n\n\n\n\n\nAltair syntax\nExample handle\nOperation\n\n\n\n\nChart\nalt.Chart(df)\nCoerces a dataframe df to a chart object\n\n\nMark\nmark_point()\nSpecifies a geometric object\n\n\nEncoding\nencode(x = ..., y = ..., color = ...)\nMaps columns of df to objects and aesthetics"
  },
  {
    "objectID": "slides/week4-graphics.html#basic-use-of-syntax",
    "href": "slides/week4-graphics.html#basic-use-of-syntax",
    "title": "Statistical graphics",
    "section": "Basic use of syntax",
    "text": "Basic use of syntax\nA chart specification, mark(s), and encodings are chained together to make a graphic.\n\n\n\nCode\nalt.Chart( \n    popregion \n).mark_line( \n).encode(\n    x = 'Year:T', y = 'Population', color = 'Region' \n).properties(\n    width = 500, height = 100\n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week4-graphics.html#choice-of-scale",
    "href": "slides/week4-graphics.html#choice-of-scale",
    "title": "Statistical graphics",
    "section": "Choice of scale",
    "text": "Choice of scale\nThe choice of scales for each mapping can either reveal or obscure patterns in data.\n\nWhen population is mapped onto a logarithmic rather than linear scale, rates of increase become evident in less populous regions:\n\n\n\n\nCode\nalt.Chart( \n    popregion\n).mark_line( \n).encode( \n    x = 'Year:T',\n    y = alt.Y('Population', scale = alt.Scale(type = 'log')), # change axis scale\n    color = 'Region'\n).properties(\n    width = 350, height = 100 \n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n)\n\n\n\n\n\n\n\n\n\nNote: scale is adjusted at the encoding level by alt.Y(...); every encoding channel has an analogous function, e.g., alt.X(...), alt.Color(...), alt.Shape(...), etc., with optional scale arguments."
  },
  {
    "objectID": "slides/week4-graphics.html#common-statistical-graphics",
    "href": "slides/week4-graphics.html#common-statistical-graphics",
    "title": "Statistical graphics",
    "section": "Common statistical graphics",
    "text": "Common statistical graphics\nBroadly, the most common statistical graphics can be divided according to the number of variables that form their primary display. The uses listed below are not exclusive, just some of the most common.\n\nOne-variable graphics are used to visualize distributions.\nTwo-variable graphics are used to visualize relationships.\nThree-variable graphics are used to visualize spatial data, matrices, and a collection of other data types.\n\n\n\nMost graphics you’ll encounter are grouped one- or two-variable graphics with superpositions of geometric objects differentiating observed from inferred values – e.g., scatterplots with points color-coded by another (grouping) variable and trend lines."
  },
  {
    "objectID": "slides/week4-graphics.html#single-variable-graphics",
    "href": "slides/week4-graphics.html#single-variable-graphics",
    "title": "Statistical graphics",
    "section": "Single-variable graphics",
    "text": "Single-variable graphics\nSingle-variable graphics usually display the distribution of values of a single variable.\n\n\n\n\nCommon single-variable graphics\n\n\n\n\nHistograms and smoothed density plots show shape but depend on arbitrary binning/smoothing parameters.\n\n\nCDF and quantile plots show the distribution exactly but are harder to interpret."
  },
  {
    "objectID": "slides/week4-graphics.html#histograms",
    "href": "slides/week4-graphics.html#histograms",
    "title": "Statistical graphics",
    "section": "Histograms",
    "text": "Histograms\nHistograms show the relative frequencies of values of a single variable.\n\n\nCode\nalt.Chart(\n    popcountry.loc[\"1970\"]\n).mark_bar().encode(\n    x = alt.X('log(Population)', \n            bin = alt.Bin(maxbins = 50)),\n    y = 'count()'\n).properties(\n    height = 150,\n    title = 'National populations in 1970'\n).properties(\n    width = 500, height = 300 \n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n).configure_title(\n    fontSize = 16\n)\n\n\n\n\n\n\n\n\nThe main advantage of the histogram is it shows the shape of a distribution."
  },
  {
    "objectID": "slides/week4-graphics.html#bin-widths",
    "href": "slides/week4-graphics.html#bin-widths",
    "title": "Statistical graphics",
    "section": "Bin widths",
    "text": "Bin widths\nThe main downside is that the shape depends on bin width, which is an arbitrary parameter.\n\n\n\n\nCode\nalt.Chart(\n    popcountry.loc[\"1970\"]\n).mark_bar().encode(\n    x = alt.X('log(Population)', \n            bin = alt.Bin(maxbins = 10)),\n    y = 'count()'\n).properties(\n    height = 150,\n    title = 'National populations in 1970'\n).properties(\n    width = 350, height = 100 \n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n).configure_title(\n    fontSize = 16\n)\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(\n    popcountry.loc[\"1970\"]\n).mark_bar().encode(\n    x = alt.X('log(Population)', \n            bin = alt.Bin(maxbins = 50)),\n    y = 'count()'\n).properties(\n    height = 150,\n    title = 'National populations in 1970'\n).properties(\n    width = 350, height = 100 \n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n).configure_title(\n    fontSize = 16\n)\n\n\n\n\n\n\n\n\n\n\nAlways experiment with multiple bin widths to ensure you don’t overlook any important details such as outliers, multiple modes, etc.\n\nBinning at left is too coarse, obscures outlying values\nBinning at right is good"
  },
  {
    "objectID": "slides/week4-graphics.html#density-plots",
    "href": "slides/week4-graphics.html#density-plots",
    "title": "Statistical graphics",
    "section": "Density plots",
    "text": "Density plots\nDenisty plots are smoothed histograms – we’ll discuss further next week. They also require some arbitrary choices that affect the appearance.\n\nDensity plots with different smoothing kernels and bandwidths"
  },
  {
    "objectID": "slides/week4-graphics.html#more-single-variable-graphics",
    "href": "slides/week4-graphics.html#more-single-variable-graphics",
    "title": "Statistical graphics",
    "section": "More single-variable graphics",
    "text": "More single-variable graphics\nGrouped single-variable graphics allow visualization of multiple distributions.\n\n\n\n\nGrouped single-variable graphics"
  },
  {
    "objectID": "slides/week4-graphics.html#boxplots",
    "href": "slides/week4-graphics.html#boxplots",
    "title": "Statistical graphics",
    "section": "Boxplots",
    "text": "Boxplots\nBoxplots display data quantiles and outliers, conveying skewness and range.\n\n\nDue to their compactness, they are useful for comparing multiple distributions."
  },
  {
    "objectID": "slides/week4-graphics.html#many-boxplots",
    "href": "slides/week4-graphics.html#many-boxplots",
    "title": "Statistical graphics",
    "section": "Many boxplots",
    "text": "Many boxplots\nSingle-variable graphics are not necessarily limited to univariate data; one might want to compare distributions using the same single-variable displays shown groupwise.\n\n\n\nCode\nalt.Chart(\n    popcountry.reset_index()\n).mark_boxplot(\n    outliers = True, size = 7\n).encode(\n    x = 'Year:T', \n    y = alt.Y('Population', scale = alt.Scale(type = 'log')) \n).properties(\n    width = 600\n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n).configure_title(\n    fontSize = 16\n)"
  },
  {
    "objectID": "slides/week4-graphics.html#multiple-histograms",
    "href": "slides/week4-graphics.html#multiple-histograms",
    "title": "Statistical graphics",
    "section": "Multiple histograms",
    "text": "Multiple histograms\nHistograms aren’t well-suited to comparing distributions. Do not stack histograms.\n\n\n\n\n\nStacked histograms\n\n\n\n\n\n\nOverlaid histograms\n\n\n\n\n\nStacked histograms do not preserve the shape of distributions (except whichever one is on the bottom).\n\n\nOverlaid histograms are visually messy due to color blending."
  },
  {
    "objectID": "slides/week4-graphics.html#multiple-histograms-1",
    "href": "slides/week4-graphics.html#multiple-histograms-1",
    "title": "Statistical graphics",
    "section": "Multiple histograms",
    "text": "Multiple histograms\nHere’s a creative solution, but one that will only work for comparing two distributions."
  },
  {
    "objectID": "slides/week4-graphics.html#alternatives-to-multiple-histograms",
    "href": "slides/week4-graphics.html#alternatives-to-multiple-histograms",
    "title": "Statistical graphics",
    "section": "Alternatives to multiple histograms",
    "text": "Alternatives to multiple histograms\nDensity plots are better alternatives to stacked histograms for a small-ish number of distributions.\n\nOverlapping densities"
  },
  {
    "objectID": "slides/week4-graphics.html#visualizing-many-distributions",
    "href": "slides/week4-graphics.html#visualizing-many-distributions",
    "title": "Statistical graphics",
    "section": "Visualizing many distributions",
    "text": "Visualizing many distributions\nRidge plots are good options for comparing a large number of distributions at once.\n\nRidge plot: temperatures in Nebraska in 2016."
  },
  {
    "objectID": "slides/week4-graphics.html#visualizing-many-distributions-1",
    "href": "slides/week4-graphics.html#visualizing-many-distributions-1",
    "title": "Statistical graphics",
    "section": "Visualizing many distributions",
    "text": "Visualizing many distributions\nRidge plots are good options for comparing a large number of distributions at once.\n\nAnother ridge plot: movie lengths by year."
  },
  {
    "objectID": "slides/week4-graphics.html#two-variable-graphics",
    "href": "slides/week4-graphics.html#two-variable-graphics",
    "title": "Statistical graphics",
    "section": "Two-variable graphics",
    "text": "Two-variable graphics\nTwo-variable graphics are all about displaying relationships, usually with scatter or lines.\n\n\n\n\nBasic two-variable scatterplots"
  },
  {
    "objectID": "slides/week4-graphics.html#scatterplots",
    "href": "slides/week4-graphics.html#scatterplots",
    "title": "Statistical graphics",
    "section": "Scatterplots",
    "text": "Scatterplots\nScatterplots display relationships between two variables.\n\n\nCode\nalt.Chart(\n    lifegdp.loc[2015]\n).mark_point().encode(\n    x = alt.X('GDP per capita', scale = alt.Scale(type = 'log')),\n    y = alt.Y('All', scale = alt.Scale(zero = False), \n    title = 'Life expectancy at birth')\n).properties(\n    width = 600\n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n).configure_title(\n    fontSize = 16\n)\n\n\n\n\n\n\n\n\nYou’ll make extensive use of scatter and bubble plots for displaying this relationship in lab 4."
  },
  {
    "objectID": "slides/week4-graphics.html#alternatives-to-bar-plots",
    "href": "slides/week4-graphics.html#alternatives-to-bar-plots",
    "title": "Statistical graphics",
    "section": "Alternatives to bar plots",
    "text": "Alternatives to bar plots\nBar plots usually depict amount or magnitude.\n\n\nCode\nalt.Chart(\n    popregion\n).mark_bar(\n).encode(\n    x = 'Year:T',\n    y = alt.Y('Population:Q'),\n    color = 'Region'\n).properties(\n    width = 600\n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n).configure_title(\n    fontSize = 16\n)"
  },
  {
    "objectID": "slides/week4-graphics.html#alternatives-to-bar-plots-1",
    "href": "slides/week4-graphics.html#alternatives-to-bar-plots-1",
    "title": "Statistical graphics",
    "section": "Alternatives to bar plots",
    "text": "Alternatives to bar plots\n\nThere is almost always a better alternative to a bar chart\n\nWith reference to the last example, which are we more interested in:\n\nPopulation growth by region?\nRegional share of global population?\n\n\nAs an aside, this depends on what story the plot is intended to tell and how it fits into the broader data analysis."
  },
  {
    "objectID": "slides/week4-graphics.html#alternatives-to-bar-plots-2",
    "href": "slides/week4-graphics.html#alternatives-to-bar-plots-2",
    "title": "Statistical graphics",
    "section": "Alternatives to bar plots",
    "text": "Alternatives to bar plots\nIf it’s population growth by region, the line plot from earlier is cleaner.\n\n\n\nCode\nalt.Chart( \n    popregion\n).mark_line( \n).encode( \n    x = 'Year:T',\n    y = alt.Y('Population', scale = alt.Scale(type = 'log')), # change axis scale\n    color = 'Region'\n).properties(\n    width = 400, height = 300 \n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week4-graphics.html#alternatives-to-bar-plots-3",
    "href": "slides/week4-graphics.html#alternatives-to-bar-plots-3",
    "title": "Statistical graphics",
    "section": "Alternatives to bar plots",
    "text": "Alternatives to bar plots\nIf it’s the relative share of the globabl population in each region over time, an area chart is cleaner.\n\n\nCode\nalt.Chart(\n    popregion\n).mark_area(\n).encode(\n    x = \"Year:T\",\n    y = alt.Y(\"Population:Q\", \n        stack = \"normalize\",\n        scale = alt.Scale(type = 'sqrt'),\n        title = \"Proportion of global population\"),\n    color = \"Region:N\"\n).properties(\n    width = 600\n).configure_axis(\n    labelFontSize = 16, titleFontSize = 16\n).configure_legend(\n    labelFontSize = 16, titleFontSize = 16\n).configure_title(\n    fontSize = 16\n)"
  },
  {
    "objectID": "slides/week4-graphics.html#but-if-you-insist-on-bars",
    "href": "slides/week4-graphics.html#but-if-you-insist-on-bars",
    "title": "Statistical graphics",
    "section": "But if you insist on bars…",
    "text": "But if you insist on bars…\n… there are some rules of thumb to keep in mind for bar plots:\n\norient axes so labels are legible\ndon’t stack; use side-by-side bars instead\nstart bars at zero, so that their height is proportional to the quantity of interest\narrange bar order sensibly; if the categories are ordered, arrange by order, and otherwise, sort by bar height"
  },
  {
    "objectID": "slides/week4-graphics.html#axis-orientation-for-barplots",
    "href": "slides/week4-graphics.html#axis-orientation-for-barplots",
    "title": "Statistical graphics",
    "section": "Axis orientation for barplots",
    "text": "Axis orientation for barplots\nIf you have to tilt your head, there’s a better orientation available.\n\n\n\n\n\nAwkward axis orientation.\n\n\n\n\n\n\nGood axis orientation."
  },
  {
    "objectID": "slides/week4-graphics.html#ordering-of-bars",
    "href": "slides/week4-graphics.html#ordering-of-bars",
    "title": "Statistical graphics",
    "section": "Ordering of bars",
    "text": "Ordering of bars\nFor categorical bar plots, order bars by height.\n\n\n\n\n\nMessy order\n\n\n\n\n\n\nOrdered by bar height"
  },
  {
    "objectID": "slides/week4-graphics.html#ordering-of-bars-1",
    "href": "slides/week4-graphics.html#ordering-of-bars-1",
    "title": "Statistical graphics",
    "section": "Ordering of bars",
    "text": "Ordering of bars\nBut don’t order by height if the categories themselves are ordered.\n\n\n\n\n\nOrdered categories\n\n\n\n\n\n\nJumbled order"
  },
  {
    "objectID": "slides/week4-graphics.html#group-dont-stack",
    "href": "slides/week4-graphics.html#group-dont-stack",
    "title": "Statistical graphics",
    "section": "Group don’t stack",
    "text": "Group don’t stack\nStacked bars are not an effective means of comparing distributions – group and use side-by-side bars instead.\n\nGrouped bar plot"
  },
  {
    "objectID": "slides/week4-graphics.html#always-start-at-zero",
    "href": "slides/week4-graphics.html#always-start-at-zero",
    "title": "Statistical graphics",
    "section": "Always start at zero",
    "text": "Always start at zero\nBar height should be proportional to the quantity of interest.\n\n\n\n\n\nBars start near the minimum observed value.\n\n\n\n\n\n\nBars start at zero."
  },
  {
    "objectID": "slides/week4-graphics.html#but-dont-take-up-all-the-space",
    "href": "slides/week4-graphics.html#but-dont-take-up-all-the-space",
    "title": "Statistical graphics",
    "section": "But don’t take up all the space",
    "text": "But don’t take up all the space\nIf your bars occupy almost the entire plot, there’s probably a better alternative. Try dots.\n\n\n\n\n\nBars do a poor job of conveying differences in life expectancy\n\n\n\n\n\n\nDots make this clearer, since the axis can start away from zero"
  },
  {
    "objectID": "slides/week4-graphics.html#other-common-visuzlizations",
    "href": "slides/week4-graphics.html#other-common-visuzlizations",
    "title": "Statistical graphics",
    "section": "Other common visuzlizations",
    "text": "Other common visuzlizations\nSmoothing scatterplots helps to visualize trends. Next week we’ll discuss this in detail.\n\nScatterplot smoothing"
  },
  {
    "objectID": "slides/week4-graphics.html#other-common-visualizations",
    "href": "slides/week4-graphics.html#other-common-visualizations",
    "title": "Statistical graphics",
    "section": "Other common visualizations",
    "text": "Other common visualizations\nHeatmaps are a common choice for displaying amounts in two-way groupings or for visualizing matrices.\n\nHeatmap"
  },
  {
    "objectID": "slides/week4-graphics.html#small-but-important-choices",
    "href": "slides/week4-graphics.html#small-but-important-choices",
    "title": "Statistical graphics",
    "section": "Small but important choices",
    "text": "Small but important choices\nHow to order the countries? Depends on what feature you wish to emphasize.\n\nCountries reordered by internet use in 2016.\nAre you more interested in present internet use, or early/late adoption?"
  },
  {
    "objectID": "slides/week4-graphics.html#other-common-visualizations-1",
    "href": "slides/week4-graphics.html#other-common-visualizations-1",
    "title": "Statistical graphics",
    "section": "Other common visualizations",
    "text": "Other common visualizations\nChloropleth maps are the most common display of spatial data.\n\nChloropleth map"
  },
  {
    "objectID": "slides/week4-graphics.html#what-makes-visualizations-effective",
    "href": "slides/week4-graphics.html#what-makes-visualizations-effective",
    "title": "Statistical graphics",
    "section": "What makes visualizations effective?",
    "text": "What makes visualizations effective?\n\nNovel. Novel visuals don’t need to elicit superlative reactions, but they should (if only subtly) surprise and spark interest to some extent.\nInformative. Informative visuals make information apparent. In a way they are unambiguous.\nEfficient. Efficient visuals have an accessible message. They use space economically but without becoming overly complicated.\nPleasant. Visuals should be nice to look at!\n\n\nNext time we’ll discuss principles of visualizaiton"
  },
  {
    "objectID": "slides/week4-principles.html#principles-of-effective-design",
    "href": "slides/week4-principles.html#principles-of-effective-design",
    "title": "Figure design",
    "section": "Principles of effective design",
    "text": "Principles of effective design\nA good figure should:\n\nconvey a clear message or story\navoid excessive complexity\nlook nice\nbe well-labeled and appropriately sized\nstand alone with a short caption\n\n\nHere we’ll mostly look at lots of examples."
  },
  {
    "objectID": "slides/week4-principles.html#on-color",
    "href": "slides/week4-principles.html#on-color",
    "title": "Figure design",
    "section": "On color",
    "text": "On color\nColor is one of the most frequently used aesthetics and is easy to misuse.\n\nchoice of color scale should match the data\nuse of color should take account of colorblindness\ncolor can only encode a limited amount of information"
  },
  {
    "objectID": "slides/week4-principles.html#color-scales",
    "href": "slides/week4-principles.html#color-scales",
    "title": "Figure design",
    "section": "Color scales",
    "text": "Color scales\nThere are three types of color scales.\n\nQualitative scales are non-monotonic sets of colors.\nSequential scales are monotonic sets of colors spanning a color gradient.\nDiverging scales are sequential scales centered at a neutral color."
  },
  {
    "objectID": "slides/week4-principles.html#qualitative-scales",
    "href": "slides/week4-principles.html#qualitative-scales",
    "title": "Figure design",
    "section": "Qualitative scales",
    "text": "Qualitative scales\nQualitative scales are non-monotonic sets of colors.\n\n\nUseful for displaying categorical variables with few levels."
  },
  {
    "objectID": "slides/week4-principles.html#sequential-scales",
    "href": "slides/week4-principles.html#sequential-scales",
    "title": "Figure design",
    "section": "Sequential scales",
    "text": "Sequential scales\nSequential scales are monotonic sets of colors spanning a color gradient.\n\n\nUseful for continuous variables."
  },
  {
    "objectID": "slides/week4-principles.html#sequential-scales-1",
    "href": "slides/week4-principles.html#sequential-scales-1",
    "title": "Figure design",
    "section": "Sequential scales",
    "text": "Sequential scales\n\nExample sequential color scale"
  },
  {
    "objectID": "slides/week4-principles.html#diverging-scales",
    "href": "slides/week4-principles.html#diverging-scales",
    "title": "Figure design",
    "section": "Diverging scales",
    "text": "Diverging scales\nDiverging scales are sequential scales centered at a neutral color.\n\n\nUseful for continuous variables with a ‘natural’ center."
  },
  {
    "objectID": "slides/week4-principles.html#diverging-scales-1",
    "href": "slides/week4-principles.html#diverging-scales-1",
    "title": "Figure design",
    "section": "Diverging scales",
    "text": "Diverging scales"
  },
  {
    "objectID": "slides/week4-principles.html#use-of-color",
    "href": "slides/week4-principles.html#use-of-color",
    "title": "Figure design",
    "section": "Use of color",
    "text": "Use of color\nCommon mistakes:\n\nEncoding too much information\nPoor choice of scale\nNot accounting for colorblindness"
  },
  {
    "objectID": "slides/week4-principles.html#tmi",
    "href": "slides/week4-principles.html#tmi",
    "title": "Figure design",
    "section": "TMI",
    "text": "TMI"
  },
  {
    "objectID": "slides/week4-principles.html#better",
    "href": "slides/week4-principles.html#better",
    "title": "Figure design",
    "section": "Better",
    "text": "Better\n\n\nAvoid encoding more than 5 categories using color"
  },
  {
    "objectID": "slides/week4-principles.html#inappropriate-scales",
    "href": "slides/week4-principles.html#inappropriate-scales",
    "title": "Figure design",
    "section": "Inappropriate scales",
    "text": "Inappropriate scales\nThe color scale doesn’t match the data well, since the rainbow scale emphasizes arbitrary data values. In addition, colors here are too intense."
  },
  {
    "objectID": "slides/week4-principles.html#better-1",
    "href": "slides/week4-principles.html#better-1",
    "title": "Figure design",
    "section": "Better",
    "text": "Better\n\n\nA diverging scale is appropriate here because 50% is a natural midpoint in context."
  },
  {
    "objectID": "slides/week4-principles.html#color-blindness",
    "href": "slides/week4-principles.html#color-blindness",
    "title": "Figure design",
    "section": "Color blindness",
    "text": "Color blindness\nColor vision deficiency (CVD) or colorblindness refers to difficulty distinguishing specific colors.\n\nred-green CVD: protanomaly and deuteranomaly\nblue-yellow CVD: tritanomaly"
  },
  {
    "objectID": "slides/week4-principles.html#cvd-friendly-scales",
    "href": "slides/week4-principles.html#cvd-friendly-scales",
    "title": "Figure design",
    "section": "CVD-friendly scales",
    "text": "CVD-friendly scales\nSome color scales still retain visible contrast for different types of color vision deficiency (CVD).\nHere is a simulation (for those without CVD).\n\nColor scale shown for different types of colorblindness using CVD simulator"
  },
  {
    "objectID": "slides/week4-principles.html#cvd-unfriendly-scales",
    "href": "slides/week4-principles.html#cvd-unfriendly-scales",
    "title": "Figure design",
    "section": "CVD-unfriendly scales",
    "text": "CVD-unfriendly scales\nOther scales get muddled.\n\n\nWhen in doubt, use a CVD simulator to check figures"
  },
  {
    "objectID": "slides/week4-principles.html#redundancy",
    "href": "slides/week4-principles.html#redundancy",
    "title": "Figure design",
    "section": "Redundancy",
    "text": "Redundancy\nWhen possible, use ‘redundant coding’ – map the same variable to color and one other aesthetic.\n\n\n\n\n\n\n\n\n\n\nRedundancy provides a failsafe against any circumstance that might compromise the effectiveness of color:\n\nprinters or black-and-white printing\nprojectors, displays, and lighting conditions\nCVD"
  },
  {
    "objectID": "slides/week4-principles.html#redundancy-1",
    "href": "slides/week4-principles.html#redundancy-1",
    "title": "Effective figure design",
    "section": "Redundancy",
    "text": "Redundancy\nWhen possible, use ‘redundant coding’ – map the same variable to color and one other aesthetic."
  },
  {
    "objectID": "slides/week4-principles.html#faceting",
    "href": "slides/week4-principles.html#faceting",
    "title": "Figure design",
    "section": "Faceting",
    "text": "Faceting\nYou’ve already made a faceted plot.\n\n\nNotice the redundant use of color!"
  },
  {
    "objectID": "slides/week4-principles.html#faceting-1",
    "href": "slides/week4-principles.html#faceting-1",
    "title": "Figure design",
    "section": "Faceting",
    "text": "Faceting\nFacets are another way to encode categorical variables when side-by-side comparisons are of interest.\n\nThe most common blunders with faceting are:\n\nFree axis scales are misleading\nFacet layout isn’t conducive to comparison of interest"
  },
  {
    "objectID": "slides/week4-principles.html#many-facets",
    "href": "slides/week4-principles.html#many-facets",
    "title": "Figure design",
    "section": "Many facets",
    "text": "Many facets\nOften a big panel of scatterplots can be a useful exploratory graphic.\n\n\n\n\n\nMovie ratings from IMDB\n\n\n\nThe figure shows a lot:\n\nTimespan of data 1906-2005\nMore observations (movies) in later years\nHigher vote counts in later years\nHigher rating variance among movies with fewer votes\nLong term reversal of voting/rating trend"
  },
  {
    "objectID": "slides/week4-principles.html#use-fixed-axis-scales",
    "href": "slides/week4-principles.html#use-fixed-axis-scales",
    "title": "Figure design",
    "section": "Use fixed axis scales",
    "text": "Use fixed axis scales\n\nExample of facets with different y axes\nSuggests, misleadingly, that Education declined by the same amount as social science and history."
  },
  {
    "objectID": "slides/week4-principles.html#use-fixed-axis-scales-1",
    "href": "slides/week4-principles.html#use-fixed-axis-scales-1",
    "title": "Figure design",
    "section": "Use fixed axis scales",
    "text": "Use fixed axis scales\n\nSame as before, with common fixed axis scales."
  },
  {
    "objectID": "slides/week4-principles.html#labels-and-legends",
    "href": "slides/week4-principles.html#labels-and-legends",
    "title": "Figure design",
    "section": "Labels and legends",
    "text": "Labels and legends\nThe most common blunders with regard to labels are:\n\nUse of dataframe column names as labels\nObscure or uninterpretable labels\nToo small or too big\n\n\nFor sizing, it’s important to pay attention to the balance of labels, whitespace, and graphical elements."
  },
  {
    "objectID": "slides/week4-principles.html#sizing",
    "href": "slides/week4-principles.html#sizing",
    "title": "Figure design",
    "section": "Sizing",
    "text": "Sizing\nUsually figure defaults look fine on your IDE but render too small when graphics are exported.\n\nThese will be illegible in slide presentations, reports, etc."
  },
  {
    "objectID": "slides/week4-principles.html#sizing-1",
    "href": "slides/week4-principles.html#sizing-1",
    "title": "Figure design",
    "section": "Sizing",
    "text": "Sizing\nThese labels are legible, but still too small – they take up a minimum of space in the figure.\n\nUnbalanced text/graphic/whitespace"
  },
  {
    "objectID": "slides/week4-principles.html#sizing-2",
    "href": "slides/week4-principles.html#sizing-2",
    "title": "Figure design",
    "section": "Sizing",
    "text": "Sizing\nUse larger labels than you think you’ll need.\n\nBalanced\nNote also the mark size is increased a bit."
  },
  {
    "objectID": "slides/week4-principles.html#sizing-3",
    "href": "slides/week4-principles.html#sizing-3",
    "title": "Figure design",
    "section": "Sizing",
    "text": "Sizing\nDon’t overdo it.\n\nUnbalanced again"
  },
  {
    "objectID": "slides/week4-principles.html#sizing-4",
    "href": "slides/week4-principles.html#sizing-4",
    "title": "Figure design",
    "section": "Sizing",
    "text": "Sizing\nIf the figure will be reproduced in a scaled-down size, increase all sizes in proportion."
  },
  {
    "objectID": "slides/week4-principles.html#critiques",
    "href": "slides/week4-principles.html#critiques",
    "title": "Figure design",
    "section": "Critiques",
    "text": "Critiques\nSeries from NYC Life Expectancy Dropped 4.6 Years in 2020\n\n\n\n\nPositive:\n\neffective use of labels\neffective use of highlighting\nwell-proportioned\nclean axes\n\nNegative:\n\nCOVID spike looks minimal, contrary to story?\nthe most striking feature of the plot is the time trend and variance stabilization"
  },
  {
    "objectID": "slides/week4-principles.html#tidy-graphics",
    "href": "slides/week4-principles.html#tidy-graphics",
    "title": "Figure design",
    "section": "Tidy graphics?",
    "text": "Tidy graphics?\nGraphics should avoid conflating data semantics.\n\nobservational units should be clearly distinguished\ndifferent types of observational units should be shown on different graphics\n\n\nIn addition, they should avoid conflating observed from inferred quantities.\n\naggregated values should be clearly distinguished from individual observations\npredictions, inferred trends, or uncertainty should be shown using a different graphical element than observed data\nunless comparing estimates and observations is the point, make separate graphics"
  },
  {
    "objectID": "slides/week4-principles.html#example",
    "href": "slides/week4-principles.html#example",
    "title": "Effective figure design",
    "section": "Example",
    "text": "Example\nThe starting plot in lab 3 is actually a bad plot because all years are shown together – so observationational units (countries) are not clearly distinguished."
  },
  {
    "objectID": "slides/week4-principles.html#exploration-or-presentation",
    "href": "slides/week4-principles.html#exploration-or-presentation",
    "title": "Figure design",
    "section": "Exploration or presentation?",
    "text": "Exploration or presentation?\nIn data exploration, it’s more important to generate lots of figures quickly than put a lot of care into details.\n\ndo not need to be scrupulous about labels, sizing, color scales, proportionality, etc.\ndo need to attend to axis scales and appropriate choice of graphical display (e.g., boxplots vs. densities)\nshould keep plots simple; don’t try to visualize too much information at once\n\n\nIn developing presentation graphics, details matter.\n\nconsider all visualization principles, especially sizing, color, etc.\noptimize for communication"
  },
  {
    "objectID": "slides/week4-principles.html#presenting-graphics",
    "href": "slides/week4-principles.html#presenting-graphics",
    "title": "Figure design",
    "section": "Presenting graphics",
    "text": "Presenting graphics\nHere is my approach to presenting a graphic. I use this for both written and oral presentations.\n\nDescribe clearly each graphical element and what each represents.\n\nStart with the highest-level graphical elements (axes) and work progressively to the lowest-level elements (aesthetics)\n\nDescribe clearly what is shown visually in the graphic without interpreting any patterns.\nFinally, say what the graphic shows."
  },
  {
    "objectID": "slides/week4-principles.html#practice",
    "href": "slides/week4-principles.html#practice",
    "title": "Figure design",
    "section": "Practice",
    "text": "Practice"
  },
  {
    "objectID": "slides/week4-principles.html#practice-1",
    "href": "slides/week4-principles.html#practice-1",
    "title": "Figure design",
    "section": "Practice",
    "text": "Practice"
  },
  {
    "objectID": "slides/week4-principles.html#another-approach-redundancy",
    "href": "slides/week4-principles.html#another-approach-redundancy",
    "title": "Figure design",
    "section": "Another approach: redundancy",
    "text": "Another approach: redundancy\nWhen possible, use ‘redundant coding’ – map the same variable to color and one other aesthetic."
  },
  {
    "objectID": "slides/week4-principles.html#what-about-this",
    "href": "slides/week4-principles.html#what-about-this",
    "title": "Figure design",
    "section": "What about this?",
    "text": "What about this?\nOne axis is fixed, one is free.\n\nA figure from HW2\nThe variable of interest, Gap, is still comparable across facets. So only one axis needs to be fixed.\n\n\nWhat would it look like if all axis scales were fixed? Would comparisons be easier or harder?"
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda-soln.html",
    "href": "hw/hw2-seda/hw2-seda-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"hw2-seda.ipynb\")"
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda-soln.html#assignment-objectives",
    "href": "hw/hw2-seda/hw2-seda-soln.html#assignment-objectives",
    "title": "PSTAT100",
    "section": "Assignment objectives",
    "text": "Assignment objectives\nIn this assignment, you’ll explore achievement gaps in California school districts in 2018, reproducing the findings described in the article above on a more local scale and with the most recent SEDA data. You’ll practice the following:\n\nreview of data documentation\nassessment of sampling design and scope of inference\ndata tidying operations\n\nslicing and filtering\nmerging multiple data frames\npivoting tables\nrenaming and reordering variables\n\nconstructing exploratory graphics and visualizing trends\ndata aggregations\nnarrative summary of exploratory analysis"
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda-soln.html#test-score-data",
    "href": "hw/hw2-seda/hw2-seda-soln.html#test-score-data",
    "title": "PSTAT100",
    "section": "Test score data",
    "text": "Test score data\nThe first few rows of the test data are shown below. The columns are:\n\n\n\n\n\n\n\nColumn name\nMeaning\n\n\n\n\nsedalea\nDistrict ID\n\n\ngrade\nGrade level\n\n\nstateabb\nState abbreviation\n\n\nsedaleaname\nDistrict name\n\n\nsubject\nTest subject\n\n\ncs_mn_...\nEstimated mean test score\n\n\ncs_mnse_...\nStandard error for estimated mean test score\n\n\ntotgyb_...\nNumber of individual tests used to estimate the mean score\n\n\n\n\n# import seda data\nca_main = pd.read_csv('data/ca-main.csv')\nca_cov = pd.read_csv('data/ca-cov.csv')\n\n# preview test score data\nca_main.head(3)\n\n\n\n\n\n  \n    \n      \n      sedalea\n      grade\n      stateabb\n      sedaleaname\n      subject\n      cs_mn_all\n      cs_mnse_all\n      totgyb_all\n      cs_mn_asn\n      cs_mnse_asn\n      ...\n      totgyb_whg\n      cs_mn_wht\n      cs_mnse_wht\n      totgyb_wht\n      cs_mn_wmg\n      cs_mnse_wmg\n      totgyb_wmg\n      cs_mn_wng\n      cs_mnse_wng\n      totgyb_wng\n    \n  \n  \n    \n      0\n      600001\n      4\n      CA\n      ACTON-AGUA DULCE UNIFIED                      ...\n      mth\n      -0.367007\n      0.108543\n      86.0\n      NaN\n      NaN\n      ...\n      79.0\n      -0.208654\n      0.165783\n      35.0\n      -0.089003\n      0.518066\n      38.0\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      600001\n      4\n      CA\n      ACTON-AGUA DULCE UNIFIED                      ...\n      rla\n      0.005685\n      0.117471\n      85.0\n      NaN\n      NaN\n      ...\n      78.0\n      0.259587\n      0.189614\n      35.0\n      0.526942\n      0.602989\n      38.0\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      600001\n      6\n      CA\n      ACTON-AGUA DULCE UNIFIED                      ...\n      rla\n      -0.000040\n      0.092172\n      114.0\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3 rows × 59 columns\n\n\n\nThe test score means for each district are named cs_mn_... with an abbreviation indicating subgroup (such as mean score for all cs_mean_all, for boys cs_mean_mal, for white students cs_mn_wht, and so on). Notice that these are generally small-ish: decimal numbers between -0.5 and 0.5.\nThese means are estimated from a number of individual student tests and standardized relative to national averages. They represent the number of standard deviations by which a district mean differs from the national average. So, for instance, the value cs_mn_all = 0.1 indicates that the district average is estimated to be 0.1 standard deviations greater than the national average on the corresponding test and at the corresponding grade level.\n\n\nQuestion 1: Interpreting test score values\nInterpret the average math test score for all 4th grade students in Acton-Agua Dulce Unified School District (the first row of the dataset shown above).\nType your answer here, replacing this text.\nSOLUTION In Acton-Agua Dulce Unified School District, the mean math test score for fourth graders in 2018 is estimated to be 0.37 standard deviations below the national average."
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda-soln.html#covariate-data",
    "href": "hw/hw2-seda/hw2-seda-soln.html#covariate-data",
    "title": "PSTAT100",
    "section": "Covariate data",
    "text": "Covariate data\nThe first few rows of the covariate data are shown below. The column information is as follows:\n\n\n\nColumn name\nMeaning\n\n\n\n\nsedalea\nDistrict ID\n\n\ngrade\nGrade level\n\n\nsedaleanm\nDistrict name\n\n\nurban\nIndicator: is the district in an urban locale?\n\n\nsuburb\nIndicator: is the district in a suburban locale?\n\n\ntown\nIndicator: is the district in a town locale?\n\n\nrural\nIndicator: is the district in a rural locale?\n\n\nlocale\nDescription of district locale\n\n\nRemaining variables\nDemographic and socioeconomic measures\n\n\n\n\nca_cov.head(3)\n\n\n\n\n\n  \n    \n      \n      sedalea\n      grade\n      sedaleanm\n      urban\n      suburb\n      town\n      rural\n      locale\n      perind\n      perasn\n      ...\n      snapall\n      snapblk\n      snaphsp\n      snapwht\n      single_momall\n      single_momblk\n      single_momhsp\n      single_momwht\n      seswhtblk\n      seswhthsp\n    \n  \n  \n    \n      0\n      600001\n      4.0\n      ACTON-AGUA DULCE UNIFIED                      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      Rural, Distant\n      0.003893\n      0.045901\n      ...\n      0.035165\n      0.20293\n      0.0819\n      0.032362\n      0.084385\n      0.349636\n      0.198482\n      0.061653\n      1.839339\n      0.692566\n    \n    \n      1\n      600001\n      5.0\n      ACTON-AGUA DULCE UNIFIED                      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      Rural, Distant\n      0.003788\n      0.046652\n      ...\n      0.035165\n      0.20293\n      0.0819\n      0.032362\n      0.084385\n      0.349636\n      0.198482\n      0.061653\n      1.839339\n      0.692566\n    \n    \n      2\n      600001\n      6.0\n      ACTON-AGUA DULCE UNIFIED                      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      Rural, Distant\n      0.003218\n      0.043657\n      ...\n      0.035165\n      0.20293\n      0.0819\n      0.032362\n      0.084385\n      0.349636\n      0.198482\n      0.061653\n      1.839339\n      0.692566\n    \n  \n\n3 rows × 60 columns\n\n\n\nYou will only be working with a handful of the demographic and socioeconomic measures, so you can put off getting acquainted with those until selecting a subset of variables.\n\n\nQuestion 2: Data semantics\nIn the non-public data, observational units are students – test scores are measured for each student. However, in the SEDA data you’ve imported, scores are aggregated to the district level by grade. Let’s regard estimated test score means for each grade as distinct variables, so that an observation consists in a set of estimated means for different grade levels and groups. In this view, what are the observational units in the test score dataset? Are they the same or different for the covariate dataset?\nType your answer here, replacing this text.\nSOLUTION The observational units are school districts. They are the same for each dataset.\n\n\n\nQuestion 3: Sample sizes\nHow many observational units are in each dataset? Count the number of units in the test dataset and the number of units in the covariate dataset separately. Store the values as ca_cov_units and ca_main_units, respectively.\n(Hint: use .nunique().)\n\nca_cov_units = ca_cov.sedalea.nunique() # SOLUTION\nca_main_units =ca_main.sedalea.nunique() # SOLUTION\n\nprint('units in covariate data: ', ca_cov_units)\nprint('units in test score data: ', ca_main_units)\n\nunits in covariate data:  913\nunits in test score data:  872\n\n\n\ngrader.check(\"q3\")\n\n\n\n\nQuestion 4: Sample characteristics and scope of inference\nAnswer the questions below about the sampling design in a short paragraph. You do not need to dig through any data documentation in order to resolve these questions.\n\n\nWhat is the relevant population for the datasets you’ve imported?\n\n\nAbout what proportion (to within 0.1) of the population is captured in the sample? (Hint: have a look at this website.)\n\n\nConsidering that the sampling frame is not identified clearly, what kind of dataset do you suspect this is (e.g., administrative, data from a ‘typical sample’, census, etc.)?\n\n\n\nIn light of your description of the sample characteristics, what is the scope of inference for this dataset?\n\n\nType your answer here, replacing this text.\nSOLUTION\nThe population of interest is all school districts in California in 2018. There are ~1000 school districts in total in 2022-2023, and although districts change a little from year to year, the total number doesn’t change too much in adjacent years. So this sample from 2018 likely covers about 90% of the population. The dataset is best described as administrative data: it is not a census, because the entire population is not included; and it is not a ‘typical’ sample or a random sample because there is no random selection mechanism from a well-defined frame. This data does not support inference and should be used for summary purposes only – conclusions should not be generalized beyond the sample. However, this actually isn’t much of a limitation here because the sample covers so much of the population."
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda-soln.html#gender-gaps-and-socioeconomic-factors",
    "href": "hw/hw2-seda/hw2-seda-soln.html#gender-gaps-and-socioeconomic-factors",
    "title": "PSTAT100",
    "section": "Gender gaps and socioeconomic factors",
    "text": "Gender gaps and socioeconomic factors\nThe cell below generates a panel of scatterplots showing the relationship between estimated gender gap and socioeconomic factors for all grade levels by test subject. The plot suggests that the reading gap favors girls consistently across the socioeconomic spectrum – in a typical district girls seem to outperform boys by 0.25 standard deviations of the national average. By contrast, the math gap appears to depend on socioeconomic factors – boys only seem to outperform girls under better socioeconomic conditions.\n\n# plot gap against socioeconomic variables by subject for all grades\nfig1 = alt.Chart(plot_df).mark_circle(opacity = 0.1).encode(\n    y = 'Gap',\n    x = alt.X('Measure', scale = alt.Scale(zero = False), title = ''),\n    color = 'Gap type'\n).properties(\n    width = 100,\n    height = 100\n).facet(\n    column = alt.Column('Socioeconomic variable')\n).resolve_scale(x = 'independent')\n\nfig1\n\n\n\n\n\n\n\n\nQuestion 13: Relationships by grade level\nDoes the pattern shown in the plot above persist within each grade level? Modify the plot above to show these relationships by grade level: generate a panel of scatterplots of gap against socioeconomic measures by subject, where each column of the panel corresponds to one socioeconomic variable and each row corresponds to one grade level; the result should by a 5x5 panel. Resize the width and height of each facet so that the panel is of reasonable size. Keep a fixed axis scale for the variable of interest, but allow the axis scales for socioeconomic variables to vary independently. Store the plot as fig2; display the figure and provide an answer to the question of interest in the text cell.\n(Hint: you may find it useful to have a look at the altair documentation on compound charts, and lab 3, for examples to follow.)\nType your answer here, replacing this text.\n\n# plotting codes here\n# BEGIN SOLUTION\nfig2 = alt.Chart(plot_df).mark_circle(opacity = 0.2).encode(\n    y = 'Gap',\n    x = alt.X('Measure', scale = alt.Scale(zero = False), title = ''),\n    color = 'Gap type'\n).properties(\n    width = 75,\n    height = 75\n).facet(\n    column = alt.Column('Socioeconomic variable'),\n    row = alt.Row('Grade')\n).resolve_scale(x = 'independent')\n\n# END SOLUTION\n\n# display\nfig2 # SOLUTION\n\n\n\n\n\n\nSOLUTION Yes, the patterns observed across all grade levels are similar to the patterns within grade level.\n\n\n\n\nQuestion 14: Association with grade level\nDo gaps shift across grade levels? It’s not so easy to tell from the last figure. Construct a 2x5 panel of scatterplots showing estimated achievement gap against each of the 5 socioeconomic variables, with one row per test subject. Display grade level using a color gradient. Store the plot as fig3; display the figure and answer the question of interest in a short sentence or two in the text cell provided.\nType your answer here, replacing this text.\n\n# plotting codes here\n# BEGIN SOLUTION\nfig3 = alt.Chart(plot_df).mark_circle(opacity = 0.2).encode(\n    y = 'Gap',\n    x = alt.X('Measure', scale = alt.Scale(zero = False), title = ''),\n    color = 'Grade'\n).properties(\n    width = 75,\n    height = 75\n).facet(\n    column = alt.Column('Socioeconomic variable'),\n    row = alt.Row('Gap type')\n).resolve_scale(x = 'independent')\n\n# END SOLUTION\n\n# display\nfig3 # SOLUTION\n\n\n\n\n\n\nSOLUTION Yes – the scatter shifts from dark to light as the estimated gap decreases for both subjects and all socioeconomic variables, indicating that as grade level increases, the gap increasingly favors girls in both math and reading and language.\n\nWhile the magnitude of the achievement gaps seems to depend very slightly on grade level (figure 3), the form of relationship between achievement gap and socioeconomic factors does not differ from grade to grade (figure 2).\nGiven that the relationships between achievement gaps and socioeconomic factors don’t change drastically across grade levels, it is reasonable to look at the average relationship between estimated achievement gap and median income after aggregating across grade.\n\n\nQuestion 15: Aggregation across grade levels\nCompute the mean estimated achievement gap in each subject across grade levels by district using District ID and retain the district-level socioeconomic variables. Store the resulting data frame as seda_data_agg.\nNote: best practice here would be to aggregate just the test scores by district and then re-merge the result with the district-level socioeconomic variables. However, since the district-level socioeconomic variables do not differ by grade within a district, averaging them across grade levels by district together with the test scores will simply return their unique values; so the aggregation can be applied across all columns for a fast-and-loose way to obtain the desired result.\n\n# aggregate across grades\n# BEGIN SOLUTION\nseda_data_agg = seda_data.groupby(\n    ['District ID']\n    ).mean(\n    numeric_only = True\n    ).reset_index(\n    ).drop(columns = 'Grade')\n# END SOLUTION\n\n# print first few rows\nseda_data_agg.head() # SOLUTION\n\n\n\n\n\n  \n    \n      \n      District ID\n      log(Median income)\n      Poverty rate\n      Unemployment rate\n      SNAP rate\n      Socioeconomic index\n      Math gap\n      Reading gap\n    \n  \n  \n    \n      0\n      600001\n      11.392048\n      0.091894\n      0.048886\n      0.035165\n      1.237209\n      -0.562855\n      -0.785321\n    \n    \n      1\n      600006\n      11.607236\n      0.041418\n      0.048269\n      0.028006\n      1.912972\n      0.061163\n      -0.242572\n    \n    \n      2\n      600011\n      10.704570\n      0.159981\n      0.066333\n      0.102054\n      -0.478127\n      -0.015417\n      -0.191400\n    \n    \n      3\n      600012\n      10.589787\n      0.179102\n      0.059158\n      0.074903\n      -0.096379\n      NaN\n      NaN\n    \n    \n      4\n      600013\n      11.399662\n      0.060338\n      0.045533\n      0.035016\n      1.398133\n      0.054454\n      -0.312638\n    \n  \n\n\n\n\n\ngrader.check(\"q15\")\n\n\n\nQuestion 16: Melt aggregated data for plotting\nSimilar to working with the disaggregated data, it will be helpful for plotting to melt the two gap variables into a single column. Follow the example above at the beginning of this section to melt only the test score gap columns (not the district-level variables – we will not create scatterplot panels as before). Name the new columns Subject and Average estimated gap; store the resulting data frame as agg_plot_df and print the first four rows.\n\n# format for plotting\n# BEGIN SOLUTION\nagg_plot_df = seda_data_agg.melt(\n    id_vars = seda_data_agg.columns[0:6],\n    value_vars = ['Math gap', 'Reading gap'],\n    var_name = 'Subject',\n    value_name = 'Average estimated gap'\n)\n# END SOLUTION\n\n# print four rows\nagg_plot_df.head(4) # SOLUTION\n\n\n\n\n\n  \n    \n      \n      District ID\n      log(Median income)\n      Poverty rate\n      Unemployment rate\n      SNAP rate\n      Socioeconomic index\n      Subject\n      Average estimated gap\n    \n  \n  \n    \n      0\n      600001\n      11.392048\n      0.091894\n      0.048886\n      0.035165\n      1.237209\n      Math gap\n      -0.562855\n    \n    \n      1\n      600006\n      11.607236\n      0.041418\n      0.048269\n      0.028006\n      1.912972\n      Math gap\n      0.061163\n    \n    \n      2\n      600011\n      10.704570\n      0.159981\n      0.066333\n      0.102054\n      -0.478127\n      Math gap\n      -0.015417\n    \n    \n      3\n      600012\n      10.589787\n      0.179102\n      0.059158\n      0.074903\n      -0.096379\n      Math gap\n      NaN\n    \n  \n\n\n\n\n\ngrader.check(\"q16\")\n\n\n\n\nQuestion 17: District average gaps\nConstruct a scatterplot of the average estimated gap against log(Median income) by subject for each district and add trend lines (see lab 4). Store the plot as fig4. Describe and interpret the plot in a few sentences.\nType your answer here, replacing this text.\n\n# scatterplot\n# BEGIN SOLUTION\nbase = alt.Chart(agg_plot_df).mark_point(opacity = 0.5).encode(\n    y = 'Average estimated gap',\n    x = alt.X('log(Median income)', scale = alt.Scale(zero = False)),\n    color = 'Subject'\n)\n# END SOLUTION\n\n# trend line\ntrend = base.transform_regression('log(Median income)', 'Average estimated gap', groupby = ['Subject']).mark_line() # SOLUTION\n\n# combine layers\nfig4 = base + trend # SOLUTION\n\n# display\nfig4 # SOLUTION\n\n\n\n\n\n\nSOLUTION Figure 4 shows average estimated achievement gaps – the difference between boys’ scores and girls’ scores – on math and reading tests across grade levels for 872 school districts in California against the median district income. Linear fits help visualize trends. The reading achievement gap favors girls and appears uncorrelated with median income; the math achievement gap increasingly favors boys in more affluent districts.\n\nNow let’s try to capture this pattern in tabular form. The cell below adds an Income bracket variable by cutting the median income into 8 contiguous intervals using pd.cut(), and tabulates the average socioeconomic measures and estimated gaps across districts by income bracket. Notice that with respect to the gaps, this displays the pattern that is shown visually in the figures above.\n\nseda_data_agg['Income bracket'] = pd.cut(np.e**seda_data_agg['log(Median income)'], 8)\nseda_data_agg.groupby('Income bracket').mean().drop(columns = ['District ID', 'log(Median income)'])\n\n\n\n\n\n  \n    \n      \n      Poverty rate\n      Unemployment rate\n      SNAP rate\n      Socioeconomic index\n      Math gap\n      Reading gap\n    \n    \n      Income bracket\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      (21980.176, 46455.372]\n      0.194870\n      0.072689\n      0.155061\n      -0.651999\n      -0.070284\n      -0.309743\n    \n    \n      (46455.372, 70736.321]\n      0.134078\n      0.063788\n      0.095303\n      0.291085\n      -0.034061\n      -0.315545\n    \n    \n      (70736.321, 95017.269]\n      0.088713\n      0.052785\n      0.048242\n      1.110433\n      0.004239\n      -0.302114\n    \n    \n      (95017.269, 119298.218]\n      0.064131\n      0.046848\n      0.030548\n      1.640159\n      0.050006\n      -0.287117\n    \n    \n      (119298.218, 143579.167]\n      0.050315\n      0.044343\n      0.011023\n      2.167272\n      0.090138\n      -0.289529\n    \n    \n      (143579.167, 167860.115]\n      0.043896\n      0.042379\n      0.008451\n      2.382258\n      0.084683\n      -0.335975\n    \n    \n      (167860.115, 192141.064]\n      0.040552\n      0.040120\n      0.010159\n      2.652906\n      0.175793\n      -0.232306\n    \n    \n      (192141.064, 216422.013]\n      0.047097\n      0.054055\n      0.002555\n      2.588499\n      0.267301\n      -0.299798\n    \n  \n\n\n\n\n\n\nQuestion 18: Proportion of districts with a math gap\nWhat proportion of districts in each income bracket have an average estimated math achievement gap favoring boys? Answer this question by performing the following steps:\n\nAppend an indicator variable Math gap favoring boys to seda_data_agg that records whether the average estimated math gap favors boys by more than 0.1 standard deviations relative to the national average.\nCompute the proportion of districts in each income bracket for which the indicator is true: group by bracket and take the mean. Store this as income_bracket_boys_favored\n\n\n# define indicator\nseda_data_agg['Math gap favoring boys'] = seda_data_agg['Math gap'] > 0.1 # SOLUTION\n\n# proportion of districts with gap favoring boys, by income bracket\n# BEGIN SOLUTION\nincome_bracket_boys_favored = seda_data_agg.groupby(\n    ['Income bracket']\n    ).mean(\n    ).reset_index(\n    ).loc[:, ['Income bracket', 'Math gap favoring boys']] \n# END SOLUTION\n\n# print result\nincome_bracket_boys_favored # SOLUTION\n\n\n\n\n\n  \n    \n      \n      Income bracket\n      Math gap favoring boys\n    \n  \n  \n    \n      0\n      (21980.176, 46455.372]\n      0.036585\n    \n    \n      1\n      (46455.372, 70736.321]\n      0.061224\n    \n    \n      2\n      (70736.321, 95017.269]\n      0.084337\n    \n    \n      3\n      (95017.269, 119298.218]\n      0.232143\n    \n    \n      4\n      (119298.218, 143579.167]\n      0.388889\n    \n    \n      5\n      (143579.167, 167860.115]\n      0.444444\n    \n    \n      6\n      (167860.115, 192141.064]\n      0.500000\n    \n    \n      7\n      (192141.064, 216422.013]\n      1.000000\n    \n  \n\n\n\n\n\ngrader.check(\"q18\")\n\n\n\nQuestion 19: Statewide averages\nTo wrap up the exploration, calculate a few statewide averages to get a sense of how some of the patterns above compare with the state as a whole.\n\n\nCompute the statewide average estimated achievement gaps. Store the result as state_avg.\n\n\nCompute the proportion of districts in the state with a math gap favoring boys. Store this result as math_boys_proportion\n\n\nCompute the proportion of districts in the state with a math gap favoring girls. You will need to define a new indicator within seda_data_agg to perform this calculation.\n\n\n\n# statewide average\nstate_avg = seda_data_agg.loc[:, ['Reading gap', 'Math gap']].mean() # SOLUTION\n\n# proportion of districts in the state with a math gap favoring boys\nmath_boys_proportion = seda_data_agg['Math gap favoring boys'].mean() # SOLUTION\n\n# proportion of districts in the state with a math gap favoring girls\nseda_data_agg['Math gap favoring girls'] = seda_data_agg['Math gap'] < -0.1 # SOLUTION\nmath_girls_proportion = seda_data_agg['Math gap favoring girls'].mean() # SOLUTION\n\n\ngrader.check(\"q19\")"
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda.html",
    "href": "hw/hw2-seda/hw2-seda.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"hw2-seda.ipynb\")"
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda.html#assignment-objectives",
    "href": "hw/hw2-seda/hw2-seda.html#assignment-objectives",
    "title": "PSTAT100",
    "section": "Assignment objectives",
    "text": "Assignment objectives\nIn this assignment, you’ll explore achievement gaps in California school districts in 2018, reproducing the findings described in the article above on a more local scale and with the most recent SEDA data. You’ll practice the following:\n\nreview of data documentation\nassessment of sampling design and scope of inference\ndata tidying operations\n\nslicing and filtering\nmerging multiple data frames\npivoting tables\nrenaming and reordering variables\n\nconstructing exploratory graphics and visualizing trends\ndata aggregations\nnarrative summary of exploratory analysis"
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda.html#test-score-data",
    "href": "hw/hw2-seda/hw2-seda.html#test-score-data",
    "title": "PSTAT100",
    "section": "Test score data",
    "text": "Test score data\nThe first few rows of the test data are shown below. The columns are:\n\n\n\n\n\n\n\nColumn name\nMeaning\n\n\n\n\nsedalea\nDistrict ID\n\n\ngrade\nGrade level\n\n\nstateabb\nState abbreviation\n\n\nsedaleaname\nDistrict name\n\n\nsubject\nTest subject\n\n\ncs_mn_...\nEstimated mean test score\n\n\ncs_mnse_...\nStandard error for estimated mean test score\n\n\ntotgyb_...\nNumber of individual tests used to estimate the mean score\n\n\n\n\n# import seda data\nca_main = pd.read_csv('data/ca-main.csv')\nca_cov = pd.read_csv('data/ca-cov.csv')\n\n# preview test score data\nca_main.head(3)\n\nThe test score means for each district are named cs_mn_... with an abbreviation indicating subgroup (such as mean score for all cs_mean_all, for boys cs_mean_mal, for white students cs_mn_wht, and so on). Notice that these are generally small-ish: decimal numbers between -0.5 and 0.5.\nThese means are estimated from a number of individual student tests and standardized relative to national averages. They represent the number of standard deviations by which a district mean differs from the national average. So, for instance, the value cs_mn_all = 0.1 indicates that the district average is estimated to be 0.1 standard deviations greater than the national average on the corresponding test and at the corresponding grade level.\n\n\nQuestion 1: Interpreting test score values\nInterpret the average math test score for all 4th grade students in Acton-Agua Dulce Unified School District (the first row of the dataset shown above).\nType your answer here, replacing this text."
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda.html#covariate-data",
    "href": "hw/hw2-seda/hw2-seda.html#covariate-data",
    "title": "PSTAT100",
    "section": "Covariate data",
    "text": "Covariate data\nThe first few rows of the covariate data are shown below. The column information is as follows:\n\n\n\nColumn name\nMeaning\n\n\n\n\nsedalea\nDistrict ID\n\n\ngrade\nGrade level\n\n\nsedaleanm\nDistrict name\n\n\nurban\nIndicator: is the district in an urban locale?\n\n\nsuburb\nIndicator: is the district in a suburban locale?\n\n\ntown\nIndicator: is the district in a town locale?\n\n\nrural\nIndicator: is the district in a rural locale?\n\n\nlocale\nDescription of district locale\n\n\nRemaining variables\nDemographic and socioeconomic measures\n\n\n\n\nca_cov.head(3)\n\nYou will only be working with a handful of the demographic and socioeconomic measures, so you can put off getting acquainted with those until selecting a subset of variables.\n\n\nQuestion 2: Data semantics\nIn the non-public data, observational units are students – test scores are measured for each student. However, in the SEDA data you’ve imported, scores are aggregated to the district level by grade. Let’s regard estimated test score means for each grade as distinct variables, so that an observation consists in a set of estimated means for different grade levels and groups. In this view, what are the observational units in the test score dataset? Are they the same or different for the covariate dataset?\nType your answer here, replacing this text.\n\n\n\nQuestion 3: Sample sizes\nHow many observational units are in each dataset? Count the number of units in the test dataset and the number of units in the covariate dataset separately. Store the values as ca_cov_units and ca_main_units, respectively.\n(Hint: use .nunique().)\n\nca_cov_units = ...\nca_main_units = ...\n\nprint('units in covariate data: ', ca_cov_units)\nprint('units in test score data: ', ca_main_units)\n\n\ngrader.check(\"q3\")\n\n\n\n\nQuestion 4: Sample characteristics and scope of inference\nAnswer the questions below about the sampling design in a short paragraph. You do not need to dig through any data documentation in order to resolve these questions.\n\n\nWhat is the relevant population for the datasets you’ve imported?\n\n\nAbout what proportion (to within 0.1) of the population is captured in the sample? (Hint: have a look at this website.)\n\n\nConsidering that the sampling frame is not identified clearly, what kind of dataset do you suspect this is (e.g., administrative, data from a ‘typical sample’, census, etc.)?\n\n\n\nIn light of your description of the sample characteristics, what is the scope of inference for this dataset?\n\n\nType your answer here, replacing this text."
  },
  {
    "objectID": "hw/hw2-seda/hw2-seda.html#gender-gaps-and-socioeconomic-factors",
    "href": "hw/hw2-seda/hw2-seda.html#gender-gaps-and-socioeconomic-factors",
    "title": "PSTAT100",
    "section": "Gender gaps and socioeconomic factors",
    "text": "Gender gaps and socioeconomic factors\nThe cell below generates a panel of scatterplots showing the relationship between estimated gender gap and socioeconomic factors for all grade levels by test subject. The plot suggests that the reading gap favors girls consistently across the socioeconomic spectrum – in a typical district girls seem to outperform boys by 0.25 standard deviations of the national average. By contrast, the math gap appears to depend on socioeconomic factors – boys only seem to outperform girls under better socioeconomic conditions.\n\n# plot gap against socioeconomic variables by subject for all grades\nfig1 = alt.Chart(plot_df).mark_circle(opacity = 0.1).encode(\n    y = 'Gap',\n    x = alt.X('Measure', scale = alt.Scale(zero = False), title = ''),\n    color = 'Gap type'\n).properties(\n    width = 100,\n    height = 100\n).facet(\n    column = alt.Column('Socioeconomic variable')\n).resolve_scale(x = 'independent')\n\nfig1\n\n\n\nQuestion 13: Relationships by grade level\nDoes the pattern shown in the plot above persist within each grade level? Modify the plot above to show these relationships by grade level: generate a panel of scatterplots of gap against socioeconomic measures by subject, where each column of the panel corresponds to one socioeconomic variable and each row corresponds to one grade level; the result should by a 5x5 panel. Resize the width and height of each facet so that the panel is of reasonable size. Keep a fixed axis scale for the variable of interest, but allow the axis scales for socioeconomic variables to vary independently. Store the plot as fig2; display the figure and provide an answer to the question of interest in the text cell.\n(Hint: you may find it useful to have a look at the altair documentation on compound charts, and lab 3, for examples to follow.)\nType your answer here, replacing this text.\n\n# plotting codes here\n...\n\n# display\n...\n\n\n\n\n\nQuestion 14: Association with grade level\nDo gaps shift across grade levels? It’s not so easy to tell from the last figure. Construct a 2x5 panel of scatterplots showing estimated achievement gap against each of the 5 socioeconomic variables, with one row per test subject. Display grade level using a color gradient. Store the plot as fig3; display the figure and answer the question of interest in a short sentence or two in the text cell provided.\nType your answer here, replacing this text.\n\n# plotting codes here\n...\n\n# display\n...\n\n\nWhile the magnitude of the achievement gaps seems to depend very slightly on grade level (figure 3), the form of relationship between achievement gap and socioeconomic factors does not differ from grade to grade (figure 2).\nGiven that the relationships between achievement gaps and socioeconomic factors don’t change drastically across grade levels, it is reasonable to look at the average relationship between estimated achievement gap and median income after aggregating across grade.\n\n\nQuestion 15: Aggregation across grade levels\nCompute the mean estimated achievement gap in each subject across grade levels by district using District ID and retain the district-level socioeconomic variables. Store the resulting data frame as seda_data_agg.\nNote: best practice here would be to aggregate just the test scores by district and then re-merge the result with the district-level socioeconomic variables. However, since the district-level socioeconomic variables do not differ by grade within a district, averaging them across grade levels by district together with the test scores will simply return their unique values; so the aggregation can be applied across all columns for a fast-and-loose way to obtain the desired result.\n\n# aggregate across grades\n...\n\n# print first few rows\n...\n\n\ngrader.check(\"q15\")\n\n\n\nQuestion 16: Melt aggregated data for plotting\nSimilar to working with the disaggregated data, it will be helpful for plotting to melt the two gap variables into a single column. Follow the example above at the beginning of this section to melt only the test score gap columns (not the district-level variables – we will not create scatterplot panels as before). Name the new columns Subject and Average estimated gap; store the resulting data frame as agg_plot_df and print the first four rows.\n\n# format for plotting\n...\n\n# print four rows\n...\n\n\ngrader.check(\"q16\")\n\n\n\n\nQuestion 17: District average gaps\nConstruct a scatterplot of the average estimated gap against log(Median income) by subject for each district and add trend lines (see lab 4). Store the plot as fig4. Describe and interpret the plot in a few sentences.\nType your answer here, replacing this text.\n\n# scatterplot\n...\n\n# trend line\ntrend = ...\n\n# combine layers\nfig4 = ...\n\n# display\n...\n\n\nNow let’s try to capture this pattern in tabular form. The cell below adds an Income bracket variable by cutting the median income into 8 contiguous intervals using pd.cut(), and tabulates the average socioeconomic measures and estimated gaps across districts by income bracket. Notice that with respect to the gaps, this displays the pattern that is shown visually in the figures above.\n\nseda_data_agg['Income bracket'] = pd.cut(np.e**seda_data_agg['log(Median income)'], 8)\nseda_data_agg.groupby('Income bracket').mean().drop(columns = ['District ID', 'log(Median income)'])\n\n\n\nQuestion 18: Proportion of districts with a math gap\nWhat proportion of districts in each income bracket have an average estimated math achievement gap favoring boys? Answer this question by performing the following steps:\n\nAppend an indicator variable Math gap favoring boys to seda_data_agg that records whether the average estimated math gap favors boys by more than 0.1 standard deviations relative to the national average.\nCompute the proportion of districts in each income bracket for which the indicator is true: group by bracket and take the mean. Store this as income_bracket_boys_favored\n\n\n# define indicator\nseda_data_agg['Math gap favoring boys'] = ...\n\n# proportion of districts with gap favoring boys, by income bracket\n...\n\n# print result\n...\n\n\ngrader.check(\"q18\")\n\n\n\nQuestion 19: Statewide averages\nTo wrap up the exploration, calculate a few statewide averages to get a sense of how some of the patterns above compare with the state as a whole.\n\n\nCompute the statewide average estimated achievement gaps. Store the result as state_avg.\n\n\nCompute the proportion of districts in the state with a math gap favoring boys. Store this result as math_boys_proportion\n\n\nCompute the proportion of districts in the state with a math gap favoring girls. You will need to define a new indicator within seda_data_agg to perform this calculation.\n\n\n\n# statewide average\nstate_avg = ...\n\n# proportion of districts in the state with a math gap favoring boys\nmath_boys_proportion = ...\n\n# proportion of districts in the state with a math gap favoring girls\nseda_data_agg['Math gap favoring girls'] = ...\nmath_girls_proportion = ...\n\n\ngrader.check(\"q19\")"
  },
  {
    "objectID": "slides/week4-principles.html#an-untidy-plot",
    "href": "slides/week4-principles.html#an-untidy-plot",
    "title": "Figure design",
    "section": "An untidy plot",
    "text": "An untidy plot\nThe starting plot in lab 3 is actually a bad plot because all years are shown together – so observationational units (countries) are not clearly distinguished."
  },
  {
    "objectID": "slides/week4-principles.html#critiques-1",
    "href": "slides/week4-principles.html#critiques-1",
    "title": "Figure design",
    "section": "Critiques",
    "text": "Critiques\nSeries from NYC Life Expectancy Dropped 4.6 Years in 2020\n\n\n\n\nPositive:\n\nsame as before\n\nNegative:\n\ndoesn’t convey proportional change in decrease efficiently, but that’s what the caption emphasizes\n‘overall’ looks like a fourth group"
  },
  {
    "objectID": "slides/week4-principles.html#critiques-2",
    "href": "slides/week4-principles.html#critiques-2",
    "title": "Figure design",
    "section": "Critiques",
    "text": "Critiques\nSeries from NYC Life Expectancy Dropped 4.6 Years in 2020\n\n\n\n\nPositive:\n\nexemplary use of color scale/palette\nline shading shows missing data clearly\neffective use of labels\n\nNegative:\n\nno clear story\nlacking a baseline comparison"
  },
  {
    "objectID": "slides/week4-principles.html#critiques-3",
    "href": "slides/week4-principles.html#critiques-3",
    "title": "Figure design",
    "section": "Critiques",
    "text": "Critiques\nSeries from NYC Life Expectancy Dropped 4.6 Years in 2020\n\n\n\n\nPositive:\n\nclear story\n\nNegative:\n\nawkward/distracting to include time, since no history for COVID\nnot the most efficient display of the captioned message\n\nRemark:\n\nit would be more interesting to see the time courses after 2020"
  },
  {
    "objectID": "slides/week4-principles.html#more-critiques",
    "href": "slides/week4-principles.html#more-critiques",
    "title": "Figure design",
    "section": "More critiques",
    "text": "More critiques\n\n\n\n\nPositive:\n\nclear labels\nunambiguous\n\nNegative:\n\nbars take up all of the plot here\nmany words seem equivalent\n\nSuggestions:\n\nfind an alternative to the bar plot\nconsider emphasizing comparisons between word clusters rather than individual words"
  },
  {
    "objectID": "slides/week4-principles.html#more-critiques-1",
    "href": "slides/week4-principles.html#more-critiques-1",
    "title": "Effective figure design",
    "section": "More critiques",
    "text": "More critiques"
  },
  {
    "objectID": "slides/week4-principles.html#practice-2",
    "href": "slides/week4-principles.html#practice-2",
    "title": "Figure design",
    "section": "Practice",
    "text": "Practice"
  },
  {
    "objectID": "slides/week4-principles.html#a-tidy-plot",
    "href": "slides/week4-principles.html#a-tidy-plot",
    "title": "Figure design",
    "section": "A tidy plot",
    "text": "A tidy plot\n\nThis is tidy, because within facets:\n\neach bubble represents a country\nany two bubbles represent distinct countries"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab4-smoothing.ipynb\")"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html#filter-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html#filter-transform",
    "title": "PSTAT100",
    "section": "Filter transform",
    "text": "Filter transform\nLast week you saw a way to make histograms. As a quick refresher, to make a histogram of life expectancies across the globe in 2010, one can filter the data and then plot using the following commands:\n\n# filter\ndata2010 = data[data.Year == 2010]\n\n# plot\nalt.Chart(data2010).mark_bar().encode(\n    x = alt.X('Life Expectancy', \n              bin = alt.Bin(step = 2), \n              title = 'Life Expectancy at Birth'),\n    y = 'count()'\n)\n\n\n\n\n\n\nHowever, the filtering step can be handled within the plotting commands using .transform_filter().\nThis uses a helper command to specify the filtering condition – in the above example, the filtering condition is that Year is equal to 2010. A filtering condition is referred to in Altair as a ‘field predicate’. In the above example: * filtering field: Year * field predicate: equals 2010\nThere are different helpers for different types of field predicates – you can find a complete list in the documentation.\nHere is how to use .transform_filter() to make the same histogram shown above, but skipping the step of storing a subset of the data under a separate name:\n\n# filter and plot\nalt.Chart(data).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).mark_bar().encode(\n    x = alt.X('Life Expectancy', \n              bin = alt.Bin(step = 2), \n              title = 'Life Expectancy at Birth'),\n    y = 'count()'\n)\n\n\n\n\n\n\n\n\nQuestion 1: Filter transform\nConstruct a histogram of life expectancies across the globe in 2019 using a filter transform as shown above to filter the appropriate rows of the dataset. Use a bin size of three (not two) years.\n\n# filter and plot\nalt.Chart(data).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2019) # SOLUTION\n).mark_bar().encode(\n    x = alt.X('Life Expectancy', bin = alt.Bin(step = 3), title = 'Life Expectancy at Birth'), # SOLUTION\n    y = 'count()' # SOLUTION\n)"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html#bin-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html#bin-transform",
    "title": "PSTAT100",
    "section": "Bin transform",
    "text": "Bin transform\nThe codes above provide a sleek way to construct the histogram that handles binning via arguments to alt.X(...). However, binning actually involves an operation: creating a new variable that is a discretization of an existing variable into contiguous intervals of a specified width.\nTo illustrate, have a look at how the histogram could be constructed ‘manually’ by the following operations. 1. Bin life expectancies 2. Count values in each bin 3. Make a bar plot of counts against bin centers.\nHere’s step 1:\n\n# bin life expectancies into 20 contiguous intervals\ndata2010['Bin'] = pd.cut(data2010[\"Life Expectancy\"], bins = 20)\ndata2010.head()\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Year\n      Life Expectancy\n      Male Life Expectancy\n      Female Life Expectancy\n      GDP per capita\n      region\n      sub-region\n      Population\n      Bin\n    \n  \n  \n    \n      2\n      Afghanistan\n      2010\n      59.9\n      59.6\n      60.3\n      543.303042\n      Asia\n      Southern Asia\n      29185507.0\n      (59.57, 62.14]\n    \n    \n      5\n      Albania\n      2010\n      76.2\n      74.2\n      78.3\n      4094.350334\n      Europe\n      Southern Europe\n      2913021.0\n      (74.99, 77.56]\n    \n    \n      9\n      Algeria\n      2010\n      75.9\n      75.0\n      76.8\n      4479.341720\n      Africa\n      Northern Africa\n      35977455.0\n      (74.99, 77.56]\n    \n    \n      13\n      Angola\n      2010\n      58.1\n      55.8\n      60.5\n      3587.883798\n      Africa\n      Sub-Saharan Africa\n      23356246.0\n      (57.0, 59.57]\n    \n    \n      17\n      Antigua and Barbuda\n      2010\n      75.9\n      73.6\n      78.2\n      13049.257050\n      Americas\n      Latin America and the Caribbean\n      88028.0\n      (74.99, 77.56]\n    \n  \n\n\n\n\nHere’s step 2:\n\n# count values in each bin and store midpoints\nhistdata = data2010.loc[:, ['Life Expectancy', 'Bin']].groupby('Bin').count()\nhistdata['Bin midpoint'] = histdata.index.values.categories.mid.values\nhistdata\n\n\n\n\n\n  \n    \n      \n      Life Expectancy\n      Bin midpoint\n    \n    \n      Bin\n      \n      \n    \n  \n  \n    \n      (31.249, 33.87]\n      1\n      32.5595\n    \n    \n      (33.87, 36.44]\n      0\n      35.1550\n    \n    \n      (36.44, 39.01]\n      0\n      37.7250\n    \n    \n      (39.01, 41.58]\n      0\n      40.2950\n    \n    \n      (41.58, 44.15]\n      0\n      42.8650\n    \n    \n      (44.15, 46.72]\n      0\n      45.4350\n    \n    \n      (46.72, 49.29]\n      3\n      48.0050\n    \n    \n      (49.29, 51.86]\n      1\n      50.5750\n    \n    \n      (51.86, 54.43]\n      1\n      53.1450\n    \n    \n      (54.43, 57.0]\n      6\n      55.7150\n    \n    \n      (57.0, 59.57]\n      11\n      58.2850\n    \n    \n      (59.57, 62.14]\n      11\n      60.8550\n    \n    \n      (62.14, 64.71]\n      8\n      63.4250\n    \n    \n      (64.71, 67.28]\n      9\n      65.9950\n    \n    \n      (67.28, 69.85]\n      12\n      68.5650\n    \n    \n      (69.85, 72.42]\n      16\n      71.1350\n    \n    \n      (72.42, 74.99]\n      24\n      73.7050\n    \n    \n      (74.99, 77.56]\n      24\n      76.2750\n    \n    \n      (77.56, 80.13]\n      10\n      78.8450\n    \n    \n      (80.13, 82.7]\n      20\n      81.4150\n    \n  \n\n\n\n\nAnd finally, step 3:\n\n# plot histogram\nalt.Chart(histdata).mark_bar(width = 10).encode(\n    x = 'Bin midpoint',\n    y = alt.Y('Life Expectancy', title = 'Count')\n)\n\n\n\n\n\n\nThese operations can be articulated as a transform in Altair using .bin_transform():\n\n# filter, bin, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).transform_bin(\n    'Life Expectancy at Birth', # name to give binned variable\n    field = 'Life Expectancy', # variable to bin\n    bin = alt.Bin(step = 2) # binning parameters\n).mark_bar(size = 10).encode(\n    x = 'Life Expectancy at Birth:Q', \n    y = 'count()'\n)\n\n\n\n\n\n\nThe plotting codes are a little more verbose, but they’re much more efficient than performing the manipulations separately in pandas.\n\n\nQuestion 2: Bin transform\nFollow the example above and make a histogram of life expectancies across the globe in 2019 using an explicit bin transform to create bins spanning three years.\n\n# filter, bin, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2019)\n).transform_bin(\n    'Life Expectancy at Birth', # SOLUTION\n    field = 'Life Expectancy', # SOLUTION\n    bin = alt.Bin(step = 3) # SOLUTION\n).mark_bar(size = 10).encode(\n    x = 'Life Expectancy at Birth:Q', # SOLUTION\n    y = 'count()'\n)"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html#aggregate-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html#aggregate-transform",
    "title": "PSTAT100",
    "section": "Aggregate transform",
    "text": "Aggregate transform\nNow, the counting of observations in each bin (implemented via y = count()) is also an under-the-hood operation in constructing the histogram. You already saw how this was done ‘manually’ in the example above before introducing the bin transform.\nGrouped counting is a form of aggregation in the sense discussed in lecture: it produces output that has fewer values than the input by combining multiple values (in this case rows) into one value (in this case a count of the number of rows).\nThis operation can also be made explicit using .transform_aggregate(). This makes use of Altair’s aggregation shorthands for common aggregation functions; see the documentation on Altair encodings for a full list of shorthands.\nHere is how .transform_aggregate() would be used to perform the counting:\n\n# filter, bin, count, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).transform_bin(\n    'Life Expectancy at Birth', \n    field = 'Life Expectancy',\n    bin = alt.Bin(step = 2) \n).transform_aggregate(\n    Count = 'count()', # altair shorthand operation -- see docs for full list\n    groupby = ['Life Expectancy at Birth'] # grouping variable(s)\n).mark_bar(size = 10).encode(\n    x = 'Life Expectancy at Birth:Q', \n    y = 'Count:Q'\n)"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html#calculate-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html#calculate-transform",
    "title": "PSTAT100",
    "section": "Calculate transform",
    "text": "Calculate transform\nBy default, Altair’s histograms are displayed on the count scale rather than the density scale.\nThe count scale means that the y-axis shows counts of observations in each bin.\nBy contrast, on the density scale, the y-axis would show proportions of total bar area (so that the area of plotted bars sums to 1).\nIt might seem like a silly distinction – after all, the two scales differ simply by a proportionality constant (the sample size times the bin width) – but as you will see shortly, the density scale is more useful for statistical thinking about the distribution of values and for direct comparisons of distributions approximated from samples of different sizes.\nThe scale conversion can be done using .transform_calculate(), which computes derived variables using arithmetic operations. In this case, one only needs to divide the count by the total number of observations.\n\n# filter, bin, count, convert scale, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).transform_bin(\n    'Life Expectancy at Birth', \n    field = 'Life Expectancy', \n    bin = alt.Bin(step = 2)\n).transform_aggregate(\n    Count = 'count()',\n    groupby = ['Life Expectancy at Birth']\n).transform_calculate(\n    Density = 'datum.Count/(2*157)' # divide counts by sample size x binwidth\n).mark_bar(size = 10).encode(\n    x = 'Life Expectancy at Birth:Q', \n    y = 'Density:Q'\n)\n\n\n\n\n\n\n\nQuestion 3: Density scale histogram\nFollow the example above and convert your histogram from Question 2 (with the year 2019, the step size of 3, and the usage of .transform_bin(...)) to the density scale.\n\n\nFirst, calculate the sample size and store the value as sample_size. Store the desired step size as bin_width.\n\n\nConvert your histogram from Question 2 (with the year 2019, the step size of 3, and the usage of .transform_bin(...)) to the density scale. First calculate the count explicitly using .transform_aggregate(...) and then convert to a proportion using .transform_calculate(...). Multiply sample_size with bin_width to obtain the scaling constant and hardcode it into your implementation.\n\n\n\n# find scaling factor\nsample_size = np.sum(data.Year == 2019) # SOLUTION\nbin_width = 3 # SOLUTION\nprint('scaling factor = ', sample_size*bin_width)\n\n# construct histogram\nalt.Chart(data).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2019)\n).transform_bin(\n    'Life Expectancy at Birth', # SOLUTION\n    field = 'Life Expectancy', # SOLUTION\n    bin = alt.Bin(step = 3) # SOLUTION\n).transform_aggregate(\n    Count = 'count()', # SOLUTION\n    groupby = ['Life Expectancy at Birth'] # SOLUTION\n).transform_calculate(\n    # use sample_size*bin_width to rescale - you will need to hardcode this value\n    Density = 'datum.Count/(459)' # SOLUTION\n).mark_bar(size = 20).encode(\n    x = 'Life Expectancy at Birth:Q', # SOLUTIOON\n    y = 'Density:Q' # SOLUTION\n)\n\nscaling factor =  459\n\n\n\n\n\n\n\n\ngrader.check(\"q3\")"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html#comparing-distributions",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html#comparing-distributions",
    "title": "PSTAT100",
    "section": "Comparing distributions",
    "text": "Comparing distributions\nThe visual advantage of a kernel density estimate for discerning shape is even more apparent when comparing distributions.\nA major task in exploratory analysis is understanding how the distribution of a variable of interest changes depending on other variables – for example, you have already seen in the last lab that life expectancy seems to change over time. We can explore this phenomenon from a different angle by comparing distributions in different years.\nMultiple density estimates can be displayed on the same plot by passing a grouping variable (or set of variables) to .transform_density(...). For example, the cell below computes density estimates of life expectancies for each of two years.\n\nalt.Chart(data).transform_filter(\n    alt.FieldOneOfPredicate(field = 'Year',\n                            oneOf = [2010, 2019])\n).transform_density(\n    density = 'Life Expectancy',\n    groupby = ['Year'],\n    as_ = ['Life Expectancy at Birth', 'Estimated Density'],\n    bandwidth = 1.8, \n    extent = [25, 90],\n    steps = 1000 \n).mark_line().encode(\n    x = 'Life Expectancy at Birth:Q',\n    y = 'Estimated Density:Q',\n    color = 'Year:N'\n)\n\n\n\n\n\n\nOften the area beneath each density estimate is filled in. This can be done by simply appending a .mark_area() call at the end of the plot.\n\np = alt.Chart(data).transform_filter(\n    alt.FieldOneOfPredicate(field = 'Year',\n                            oneOf = [2010, 2019])\n).transform_density(\n    density = 'Life Expectancy',\n    groupby = ['Year'],\n    as_ = ['Life Expectancy at Birth', 'Estimated Density'],\n    bandwidth = 1.8, \n    extent = [25, 90],\n    steps = 1000 \n).mark_line().encode(\n    x = 'Life Expectancy at Birth:Q',\n    y = 'Estimated Density:Q',\n    color = 'Year:N'\n)\n\np + p.mark_area(opacity = 0.1)\n\n\n\n\n\n\nNotice that this makes it much easier to compare the distributions between years – you can see a pronounced rightward shift of the smooth for 2019 compared with 2010.\nWe could make the same comparison based on the histograms, but the shift is a lot harder to make out. Overlaid histograms should be avoided.\n\nalt.Chart(data).transform_filter(\n    alt.FieldOneOfPredicate(field = 'Year',\n                            oneOf = [2010, 2019])\n).mark_bar(opacity = 0.5).encode(\n    x = alt.X('Life Expectancy', bin = alt.Bin(maxbins = 30), title = 'Life Expectancy at Birth'),\n    y = alt.Y('count()', stack = None),\n    color = 'Year:N'\n)\n\n\n\n\n\n\n\n\nQuestion 5: Multiple density estimates\nFollow the example above to construct a plot showing separate density estimates of life expectancy for each region in the 2010. You can choose whether you prefer to fill in the area beneath the smooth curves, or not. Be sure to play with the bandwidth parameter and choose a value that seems sensible to you.\n\n# construct density estimates\np = alt.Chart(data).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2010) # SOLUTION\n).transform_density(\n    density = 'Life Expectancy', # SOLUTION\n    groupby = ['region'], # SOLUTION\n    as_ = ['Life Expectancy at Birth', 'Estimated density'], # SOLUTION\n    bandwidth = 3, # SOLUTION\n    extent = [25, 90], # SOLUTION\n    steps = 1000 # SOLUTION\n).mark_line(\n).encode(\n    x = 'Life Expectancy at Birth:Q', # SOLUTION\n    y = 'Estimated density:Q', # SOLUTION\n    color = 'region:N' # SOLUTION\n)\n\n# add shaded area underneath curves\np + p.mark_area(opacity = 0.1) # SOLUTION\n\n\n\n\n\n\n\n\n\n\nQuestion 6: Interpretation\nDo the distributions of life expectancies seem to differ by region? If so, what is one difference that you notice? Answer in 1-2 sentences.\nType your answer here, replacing this text.\nSOLUTION\nYes – most distributions are bimodal, with two peaks, but focusing on the largest peaks, they are ordered from lowest to highest as: Africa, Oceania, Asia, Americas, Europe.\n\n\n\nQuestion 7: Outlier\nNotice that little peak way off to the left in the distribution of life expectancies in the Americas. That’s an outlier.\n\n\nWhich country is it? Check by filtering data appropriately and using .sort_values(...) to find the lowest life expectancy in the Americas. Save the outlying observation as a one-row dataframe called lowest_Americas and print the row.\n\n\nWhat was the life expectancy for that country in other years? Filter the data to examine the life expectancy in the country you identified as the outlier in all y. Save the resulting data frame as outlier_country.\n\n\nWhat Happened in 2010? Can you explain why the life expectancy was so low in that country for that particular year?(Hint: if you don’t remember, Google the country name and year in question.)\n\n\nType your answer here, replacing this text.\n\n# examine outlier\nlowest_Americas = data[\n    (data.region == 'Americas') & (data.Year == 2010) # SOLUTION\n    ].sort_values(\n    by = 'Life Expectancy', axis = 0, ascending = True # SOLUTION\n    ).head(1)\nlowest_Americas\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Year\n      Life Expectancy\n      Male Life Expectancy\n      Female Life Expectancy\n      GDP per capita\n      region\n      sub-region\n      Population\n    \n  \n  \n    \n      246\n      Haiti\n      2010\n      31.3\n      28.0\n      35.4\n      1172.098543\n      Americas\n      Latin America and the Caribbean\n      9949322.0\n    \n  \n\n\n\n\n\n# show all obsrvations for country of interest\noutlier_country = data[data['Country Name'] == 'Haiti'] #SOLUTION\noutlier_country\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Year\n      Life Expectancy\n      Male Life Expectancy\n      Female Life Expectancy\n      GDP per capita\n      region\n      sub-region\n      Population\n    \n  \n  \n    \n      244\n      Haiti\n      2019\n      64.1\n      63.3\n      64.8\n      1272.490925\n      Americas\n      Latin America and the Caribbean\n      11263077.0\n    \n    \n      245\n      Haiti\n      2015\n      62.6\n      62.1\n      63.1\n      1389.119520\n      Americas\n      Latin America and the Caribbean\n      10695542.0\n    \n    \n      246\n      Haiti\n      2010\n      31.3\n      28.0\n      35.4\n      1172.098543\n      Americas\n      Latin America and the Caribbean\n      9949322.0\n    \n    \n      247\n      Haiti\n      2000\n      57.0\n      57.0\n      57.2\n      811.533974\n      Americas\n      Latin America and the Caribbean\n      8463806.0\n    \n  \n\n\n\n\nSolution\nThere was an earthquake that killed over a hundred thousand people (over 1% of the population).\n\ngrader.check(\"q7\")"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html#loess",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html#loess",
    "title": "PSTAT100",
    "section": "LOESS",
    "text": "LOESS\nLocally weighted scatterplot smoothing (LOESS) is a flexible smoothing technique for visualizing trends in scatterplots. The technical details are a little involved but quite similar conceptually to kernel density estimation; we’ll just look at the implementation for now.\nTo illustrate, consider the scatterplots you made in lab 3 showing the relationship between life expectancy and GDP per capita. The plot for 2010 looked like this:\n\n# log transform gdp explicitly\ndata_mod1['log(GDP per capita)'] = np.log(data_mod1['GDP per capita'])\n\n# scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2000)\n).mark_circle(opacity = 0.5).encode(\n    x = alt.X('log(GDP per capita)', scale = alt.Scale(zero = False)),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False)),\n    size = alt.Size('Population', scale = alt.Scale(type = 'sqrt'))\n)\n\n# show\nscatter\n\n\n\n\n\n\nTo add a LOESS curve, simply append .transform_loess() to the base plot:\n\n# compute smooth\nsmooth = scatter.transform_loess(\n    on = 'log(GDP per capita)', # x variable\n    loess = 'Life Expectancy', # y variable\n    bandwidth = 0.25 # how smooth?\n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\n\n\n\n\n\nJust as with kernel density estimates, LOESS curves have a bandwidth parameter that controls how smooth or wiggly the curve is. In Altair, the LOESS bandwidth is a unitless parameter between 0 and 1.\n\n\nQuestion 8: LOESS bandwidth selection\nTinker with the bandwidth parameter to see its effect in the cell below. Then choose a value that produces a smoothing you find appropriate for indicating the general trend shown in the scatter.\n\n# compute smooth\nsmooth = scatter.transform_loess(\n    on = 'log(GDP per capita)', \n    loess = 'All', \n    bandwidth = 0.75 # SOLUTION\n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\n\n\n\n\n\n\nLOESS curves can also be computed groupwise. For instance, to display separate curves for each region, one need only pass a groupby = ... argument to .transform_loess():\n\n# scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2000)\n).mark_circle(opacity = 0.5).encode(\n    x = alt.X('log(GDP per capita)', scale = alt.Scale(zero = False)),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False)),\n    size = alt.Size('Population', scale = alt.Scale(type = 'sqrt')),\n    color = 'region'\n)\n\n# compute smooth\nsmooth = scatter.transform_loess(\n    groupby = ['region'], # add groupby\n    on = 'log(GDP per capita)', \n    loess = 'Life Expectancy', \n    bandwidth = 0.8 \n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\n\n\n\n\n\nThe curves are a little jagged because there aren’t very many countries in each region.\n\ndata_mod1[data_mod1.Year == 2000].groupby('region').count().iloc[:, [0]]\n\n\n\n\n\n  \n    \n      \n      Country Name\n    \n    \n      region\n      \n    \n  \n  \n    \n      Africa\n      45\n    \n    \n      Americas\n      27\n    \n    \n      Asia\n      38\n    \n    \n      Europe\n      35\n    \n    \n      Oceania\n      9"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing-soln.html#regression",
    "href": "labs/lab4-smoothing/lab4-smoothing-soln.html#regression",
    "title": "PSTAT100",
    "section": "Regression",
    "text": "Regression\nYou will be learning more about linear regression later in the course, but we can introduce regression lines now as a visualization technique. As with LOESS, you don’t need to concern yourself with the mathematical details (yet). From this perspective, regression is a form of linear smoothing – a regression smooth is a straight line. By contrast, LOESS smooths have curvature – they are not straight lines.\nIn the example above, the LOESS curves don’t have much curvature. So it may be a cleaner choice visually to show linear smooths. This can be done using .transform_regression(...) with a similar argument structure.\n\n# scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2000)\n).mark_circle(opacity = 0.5).encode(\n    x = alt.X('log(GDP per capita)', scale = alt.Scale(zero = False)),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False)),\n    size = alt.Size('Population', scale = alt.Scale(type = 'sqrt')),\n    color = 'region'\n)\n\n# compute smooth\nsmooth = scatter.transform_regression(\n    groupby = ['region'],\n    on = 'log(GDP per capita)', \n    regression = 'Life Expectancy'\n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\n\n\n\n\n\n\n\nQuestion 9: Simple regression line\nBased on the example immediately above, construct a scatterplot of life expectancy against log GDP per capita in 2010 with points sized according to population (and no distinction between regions). Layer a single linear smooth on the scatterplot using .transform_regression(...).\n(Hint: remove the color aesthetic and grouping from the previous plot.)\n\n# construct scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2010) # SOLUTION\n).mark_circle(opacity = 0.5).encode(\n    x = alt.X('log(GDP per capita)', scale = alt.Scale(zero = False)), # SOLUTION\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False)), # SOLUTION\n    size = alt.Size('Population', scale = alt.Scale(type = 'sqrt')) # SOLUTION\n)\n\n# construct smooth\nsmooth = scatter.transform_regression(\n    on = 'log(GDP per capita)', # SOLUTION\n    regression = 'Life Expectancy' # SOLUTION\n).mark_line(color = 'black')\n\n# layer\nscatter + smooth # SOLUTION"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html",
    "href": "labs/lab4-smoothing/lab4-smoothing.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab4-smoothing.ipynb\")"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html#filter-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing.html#filter-transform",
    "title": "PSTAT100",
    "section": "Filter transform",
    "text": "Filter transform\nLast week you saw a way to make histograms. As a quick refresher, to make a histogram of life expectancies across the globe in 2010, one can filter the data and then plot using the following commands:\n\n# filter\ndata2010 = data[data.Year == 2010]\n\n# plot\nalt.Chart(data2010).mark_bar().encode(\n    x = alt.X('Life Expectancy', \n              bin = alt.Bin(step = 2), \n              title = 'Life Expectancy at Birth'),\n    y = 'count()'\n)\n\nHowever, the filtering step can be handled within the plotting commands using .transform_filter().\nThis uses a helper command to specify the filtering condition – in the above example, the filtering condition is that Year is equal to 2010. A filtering condition is referred to in Altair as a ‘field predicate’. In the above example: * filtering field: Year * field predicate: equals 2010\nThere are different helpers for different types of field predicates – you can find a complete list in the documentation.\nHere is how to use .transform_filter() to make the same histogram shown above, but skipping the step of storing a subset of the data under a separate name:\n\n# filter and plot\nalt.Chart(data).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).mark_bar().encode(\n    x = alt.X('Life Expectancy', \n              bin = alt.Bin(step = 2), \n              title = 'Life Expectancy at Birth'),\n    y = 'count()'\n)\n\n\n\nQuestion 1: Filter transform\nConstruct a histogram of life expectancies across the globe in 2019 using a filter transform as shown above to filter the appropriate rows of the dataset. Use a bin size of three (not two) years.\n\n# filter and plot\nalt.Chart(data).transform_filter(\n    ...\n).mark_bar().encode(\n    x = ...\n    y = ...\n)"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html#bin-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing.html#bin-transform",
    "title": "PSTAT100",
    "section": "Bin transform",
    "text": "Bin transform\nThe codes above provide a sleek way to construct the histogram that handles binning via arguments to alt.X(...). However, binning actually involves an operation: creating a new variable that is a discretization of an existing variable into contiguous intervals of a specified width.\nTo illustrate, have a look at how the histogram could be constructed ‘manually’ by the following operations. 1. Bin life expectancies 2. Count values in each bin 3. Make a bar plot of counts against bin centers.\nHere’s step 1:\n\n# bin life expectancies into 20 contiguous intervals\ndata2010['Bin'] = pd.cut(data2010[\"Life Expectancy\"], bins = 20)\ndata2010.head()\n\nHere’s step 2:\n\n# count values in each bin and store midpoints\nhistdata = data2010.loc[:, ['Life Expectancy', 'Bin']].groupby('Bin').count()\nhistdata['Bin midpoint'] = histdata.index.values.categories.mid.values\nhistdata\n\nAnd finally, step 3:\n\n# plot histogram\nalt.Chart(histdata).mark_bar(width = 10).encode(\n    x = 'Bin midpoint',\n    y = alt.Y('Life Expectancy', title = 'Count')\n)\n\nThese operations can be articulated as a transform in Altair using .bin_transform():\n\n# filter, bin, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).transform_bin(\n    'Life Expectancy at Birth', # name to give binned variable\n    field = 'Life Expectancy', # variable to bin\n    bin = alt.Bin(step = 2) # binning parameters\n).mark_bar(size = 10).encode(\n    x = 'Life Expectancy at Birth:Q', \n    y = 'count()'\n)\n\nThe plotting codes are a little more verbose, but they’re much more efficient than performing the manipulations separately in pandas.\n\n\nQuestion 2: Bin transform\nFollow the example above and make a histogram of life expectancies across the globe in 2019 using an explicit bin transform to create bins spanning three years.\n\n# filter, bin, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2019)\n).transform_bin(\n    ...\n    field = ...\n    bin = ...\n).mark_bar(size = 10).encode(\n    x = ...\n    y = 'count()'\n)"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html#aggregate-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing.html#aggregate-transform",
    "title": "PSTAT100",
    "section": "Aggregate transform",
    "text": "Aggregate transform\nNow, the counting of observations in each bin (implemented via y = count()) is also an under-the-hood operation in constructing the histogram. You already saw how this was done ‘manually’ in the example above before introducing the bin transform.\nGrouped counting is a form of aggregation in the sense discussed in lecture: it produces output that has fewer values than the input by combining multiple values (in this case rows) into one value (in this case a count of the number of rows).\nThis operation can also be made explicit using .transform_aggregate(). This makes use of Altair’s aggregation shorthands for common aggregation functions; see the documentation on Altair encodings for a full list of shorthands.\nHere is how .transform_aggregate() would be used to perform the counting:\n\n# filter, bin, count, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).transform_bin(\n    'Life Expectancy at Birth', \n    field = 'Life Expectancy',\n    bin = alt.Bin(step = 2) \n).transform_aggregate(\n    Count = 'count()', # altair shorthand operation -- see docs for full list\n    groupby = ['Life Expectancy at Birth'] # grouping variable(s)\n).mark_bar(size = 10).encode(\n    x = 'Life Expectancy at Birth:Q', \n    y = 'Count:Q'\n)"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html#calculate-transform",
    "href": "labs/lab4-smoothing/lab4-smoothing.html#calculate-transform",
    "title": "PSTAT100",
    "section": "Calculate transform",
    "text": "Calculate transform\nBy default, Altair’s histograms are displayed on the count scale rather than the density scale.\nThe count scale means that the y-axis shows counts of observations in each bin.\nBy contrast, on the density scale, the y-axis would show proportions of total bar area (so that the area of plotted bars sums to 1).\nIt might seem like a silly distinction – after all, the two scales differ simply by a proportionality constant (the sample size times the bin width) – but as you will see shortly, the density scale is more useful for statistical thinking about the distribution of values and for direct comparisons of distributions approximated from samples of different sizes.\nThe scale conversion can be done using .transform_calculate(), which computes derived variables using arithmetic operations. In this case, one only needs to divide the count by the total number of observations.\n\n# filter, bin, count, convert scale, and plot\nalt.Chart(\n    data\n).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2010)\n).transform_bin(\n    'Life Expectancy at Birth', \n    field = 'Life Expectancy', \n    bin = alt.Bin(step = 2)\n).transform_aggregate(\n    Count = 'count()',\n    groupby = ['Life Expectancy at Birth']\n).transform_calculate(\n    Density = 'datum.Count/(2*157)' # divide counts by sample size x binwidth\n).mark_bar(size = 10).encode(\n    x = 'Life Expectancy at Birth:Q', \n    y = 'Density:Q'\n)\n\n\nQuestion 3: Density scale histogram\nFollow the example above and convert your histogram from Question 2 (with the year 2019, the step size of 3, and the usage of .transform_bin(...)) to the density scale.\n\n\nFirst, calculate the sample size and store the value as sample_size. Store the desired step size as bin_width.\n\n\nConvert your histogram from Question 2 (with the year 2019, the step size of 3, and the usage of .transform_bin(...)) to the density scale. First calculate the count explicitly using .transform_aggregate(...) and then convert to a proportion using .transform_calculate(...). Multiply sample_size with bin_width to obtain the scaling constant and hardcode it into your implementation.\n\n\n\n# find scaling factor\nsample_size = ...\nbin_width = ...\nprint('scaling factor = ', sample_size*bin_width)\n\n# construct histogram\nalt.Chart(data).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', \n                            equal = 2019)\n).transform_bin(\n    ...\n    field = ...\n    bin = ...\n).transform_aggregate(\n    Count = ...\n    groupby = ...\n).transform_calculate(\n    # use sample_size*bin_width to rescale - you will need to hardcode this value\n    Density = ...\n).mark_bar(size = 20).encode(\n    x = 'Life Expectancy at Birth:Q', # SOLUTIOON\n    y = ...\n)\n\n\ngrader.check(\"q3\")"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html#comparing-distributions",
    "href": "labs/lab4-smoothing/lab4-smoothing.html#comparing-distributions",
    "title": "PSTAT100",
    "section": "Comparing distributions",
    "text": "Comparing distributions\nThe visual advantage of a kernel density estimate for discerning shape is even more apparent when comparing distributions.\nA major task in exploratory analysis is understanding how the distribution of a variable of interest changes depending on other variables – for example, you have already seen in the last lab that life expectancy seems to change over time. We can explore this phenomenon from a different angle by comparing distributions in different years.\nMultiple density estimates can be displayed on the same plot by passing a grouping variable (or set of variables) to .transform_density(...). For example, the cell below computes density estimates of life expectancies for each of two years.\n\nalt.Chart(data).transform_filter(\n    alt.FieldOneOfPredicate(field = 'Year',\n                            oneOf = [2010, 2019])\n).transform_density(\n    density = 'Life Expectancy',\n    groupby = ['Year'],\n    as_ = ['Life Expectancy at Birth', 'Estimated Density'],\n    bandwidth = 1.8, \n    extent = [25, 90],\n    steps = 1000 \n).mark_line().encode(\n    x = 'Life Expectancy at Birth:Q',\n    y = 'Estimated Density:Q',\n    color = 'Year:N'\n)\n\nOften the area beneath each density estimate is filled in. This can be done by simply appending a .mark_area() call at the end of the plot.\n\np = alt.Chart(data).transform_filter(\n    alt.FieldOneOfPredicate(field = 'Year',\n                            oneOf = [2010, 2019])\n).transform_density(\n    density = 'Life Expectancy',\n    groupby = ['Year'],\n    as_ = ['Life Expectancy at Birth', 'Estimated Density'],\n    bandwidth = 1.8, \n    extent = [25, 90],\n    steps = 1000 \n).mark_line().encode(\n    x = 'Life Expectancy at Birth:Q',\n    y = 'Estimated Density:Q',\n    color = 'Year:N'\n)\n\np + p.mark_area(opacity = 0.1)\n\nNotice that this makes it much easier to compare the distributions between years – you can see a pronounced rightward shift of the smooth for 2019 compared with 2010.\nWe could make the same comparison based on the histograms, but the shift is a lot harder to make out. Overlaid histograms should be avoided.\n\nalt.Chart(data).transform_filter(\n    alt.FieldOneOfPredicate(field = 'Year',\n                            oneOf = [2010, 2019])\n).mark_bar(opacity = 0.5).encode(\n    x = alt.X('Life Expectancy', bin = alt.Bin(maxbins = 30), title = 'Life Expectancy at Birth'),\n    y = alt.Y('count()', stack = None),\n    color = 'Year:N'\n)\n\n\n\nQuestion 5: Multiple density estimates\nFollow the example above to construct a plot showing separate density estimates of life expectancy for each region in the 2010. You can choose whether you prefer to fill in the area beneath the smooth curves, or not. Be sure to play with the bandwidth parameter and choose a value that seems sensible to you.\n\n# construct density estimates\np = alt.Chart(data).transform_filter(\n    ...\n).transform_density(\n    density = ...\n    groupby = ...\n    as_ = ...\n    bandwidth = ...\n    extent = ...\n    steps = ...\n).mark_line(\n).encode(\n    x = ...\n    y = ...\n    color = ...\n)\n\n# add shaded area underneath curves\n...\n\n\n\n\n\nQuestion 6: Interpretation\nDo the distributions of life expectancies seem to differ by region? If so, what is one difference that you notice? Answer in 1-2 sentences.\nType your answer here, replacing this text.\n\n\n\nQuestion 7: Outlier\nNotice that little peak way off to the left in the distribution of life expectancies in the Americas. That’s an outlier.\n\n\nWhich country is it? Check by filtering data appropriately and using .sort_values(...) to find the lowest life expectancy in the Americas. Save the outlying observation as a one-row dataframe called lowest_Americas and print the row.\n\n\nWhat was the life expectancy for that country in other years? Filter the data to examine the life expectancy in the country you identified as the outlier in all y. Save the resulting data frame as outlier_country.\n\n\nWhat Happened in 2010? Can you explain why the life expectancy was so low in that country for that particular year?(Hint: if you don’t remember, Google the country name and year in question.)\n\n\nType your answer here, replacing this text.\n\n# examine outlier\nlowest_Americas = data[\n    ...\n    ].sort_values(\n    by = ...\n    ).head(1)\nlowest_Americas\n\n\n# show all obsrvations for country of interest\noutlier_country = ...\noutlier_country\n\n\ngrader.check(\"q7\")"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html#loess",
    "href": "labs/lab4-smoothing/lab4-smoothing.html#loess",
    "title": "PSTAT100",
    "section": "LOESS",
    "text": "LOESS\nLocally weighted scatterplot smoothing (LOESS) is a flexible smoothing technique for visualizing trends in scatterplots. The technical details are a little involved but quite similar conceptually to kernel density estimation; we’ll just look at the implementation for now.\nTo illustrate, consider the scatterplots you made in lab 3 showing the relationship between life expectancy and GDP per capita. The plot for 2010 looked like this:\n\n# log transform gdp explicitly\ndata_mod1['log(GDP per capita)'] = np.log(data_mod1['GDP per capita'])\n\n# scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2000)\n).mark_circle(opacity = 0.5).encode(\n    x = alt.X('log(GDP per capita)', scale = alt.Scale(zero = False)),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False)),\n    size = alt.Size('Population', scale = alt.Scale(type = 'sqrt'))\n)\n\n# show\nscatter\n\nTo add a LOESS curve, simply append .transform_loess() to the base plot:\n\n# compute smooth\nsmooth = scatter.transform_loess(\n    on = 'log(GDP per capita)', # x variable\n    loess = 'Life Expectancy', # y variable\n    bandwidth = 0.25 # how smooth?\n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\nJust as with kernel density estimates, LOESS curves have a bandwidth parameter that controls how smooth or wiggly the curve is. In Altair, the LOESS bandwidth is a unitless parameter between 0 and 1.\n\n\nQuestion 8: LOESS bandwidth selection\nTinker with the bandwidth parameter to see its effect in the cell below. Then choose a value that produces a smoothing you find appropriate for indicating the general trend shown in the scatter.\n\n# compute smooth\nsmooth = scatter.transform_loess(\n    on = 'log(GDP per capita)', \n    loess = 'All', \n    bandwidth = ...\n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\n\nLOESS curves can also be computed groupwise. For instance, to display separate curves for each region, one need only pass a groupby = ... argument to .transform_loess():\n\n# scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2000)\n).mark_circle(opacity = 0.5).encode(\n    x = alt.X('log(GDP per capita)', scale = alt.Scale(zero = False)),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False)),\n    size = alt.Size('Population', scale = alt.Scale(type = 'sqrt')),\n    color = 'region'\n)\n\n# compute smooth\nsmooth = scatter.transform_loess(\n    groupby = ['region'], # add groupby\n    on = 'log(GDP per capita)', \n    loess = 'Life Expectancy', \n    bandwidth = 0.8 \n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\nThe curves are a little jagged because there aren’t very many countries in each region.\n\ndata_mod1[data_mod1.Year == 2000].groupby('region').count().iloc[:, [0]]"
  },
  {
    "objectID": "labs/lab4-smoothing/lab4-smoothing.html#regression",
    "href": "labs/lab4-smoothing/lab4-smoothing.html#regression",
    "title": "PSTAT100",
    "section": "Regression",
    "text": "Regression\nYou will be learning more about linear regression later in the course, but we can introduce regression lines now as a visualization technique. As with LOESS, you don’t need to concern yourself with the mathematical details (yet). From this perspective, regression is a form of linear smoothing – a regression smooth is a straight line. By contrast, LOESS smooths have curvature – they are not straight lines.\nIn the example above, the LOESS curves don’t have much curvature. So it may be a cleaner choice visually to show linear smooths. This can be done using .transform_regression(...) with a similar argument structure.\n\n# scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    alt.FieldEqualPredicate(field = 'Year', equal = 2000)\n).mark_circle(opacity = 0.5).encode(\n    x = alt.X('log(GDP per capita)', scale = alt.Scale(zero = False)),\n    y = alt.Y('Life Expectancy', title = 'Life Expectancy at Birth', scale = alt.Scale(zero = False)),\n    size = alt.Size('Population', scale = alt.Scale(type = 'sqrt')),\n    color = 'region'\n)\n\n# compute smooth\nsmooth = scatter.transform_regression(\n    groupby = ['region'],\n    on = 'log(GDP per capita)', \n    regression = 'Life Expectancy'\n).mark_line(color = 'black')\n\n# add as a layer to the scatterplot\nscatter + smooth\n\n\n\nQuestion 9: Simple regression line\nBased on the example immediately above, construct a scatterplot of life expectancy against log GDP per capita in 2010 with points sized according to population (and no distinction between regions). Layer a single linear smooth on the scatterplot using .transform_regression(...).\n(Hint: remove the color aesthetic and grouping from the previous plot.)\n\n# construct scatterplot\nscatter = alt.Chart(data_mod1).transform_filter(\n    ...\n).mark_circle(opacity = 0.5).encode(\n    x = ...\n    y = ...\n    size = ...\n)\n\n# construct smooth\nsmooth = scatter.transform_regression(\n    on = ...\n    regression = ...\n).mark_line(color = 'black')\n\n# layer\n..."
  },
  {
    "objectID": "slides/week5-smoothing.html#announcements",
    "href": "slides/week5-smoothing.html#announcements",
    "title": "Exploratory analysis and density estimation",
    "section": "Announcements",
    "text": "Announcements\n\nNo assignment posted this week\nNo lab next week"
  },
  {
    "objectID": "slides/week5-smoothing.html#this-week-smoothing",
    "href": "slides/week5-smoothing.html#this-week-smoothing",
    "title": "Kernel Density Estimation",
    "section": "This week: smoothing",
    "text": "This week: smoothing\n\nWhat is exploratory data analysis (EDA)?\n\nThe role of data: information or evidence?\nExploratory vs. confirmatory analysis\nEssential exploratory questions: variation and co-variation\n\nSmoothing\n\nWhy smooth?\nKernel density estimation (KDE)\n\nConnections with probability\n\nWhat’s a probability distribution again?\nEDA and conditional distributions"
  },
  {
    "objectID": "slides/week5-smoothing.html#eda",
    "href": "slides/week5-smoothing.html#eda",
    "title": "Exploratory analysis and density estimation",
    "section": "EDA",
    "text": "EDA\nThe term and spirit of exploratory data analysis (EDA) is attributed to John Tukey, whose philosophically-leaning work in statistics in the 1960’s and 1970’s stressed the need for more data-driven methods.\n\n\nFor a long time I have thought I was a statistician, interested in inferences from the particular to the general … All in all, I have come to feel that my central interest is in data analysis [which] is a larger and more varied field than inference. (Tukey, 1962)\n\n\n\nEDA is an initial stage of non-inferential and largely model-free analysis aiming at understanding the structure, patterns, and particularities present in a dataset."
  },
  {
    "objectID": "slides/week5-smoothing.html#data-as-evidence",
    "href": "slides/week5-smoothing.html#data-as-evidence",
    "title": "Exploratory analysis and density estimation",
    "section": "Data as evidence",
    "text": "Data as evidence\nExperimental data usually serve the role of evidence for or against prespecified hypotheses.\n\n\\[\n\\text{hypothesis} \\longrightarrow \\text{data} \\longrightarrow \\text{inference}\n\\]\n\n\nFor example, in vaccine efficacy trials, trial data are collected precisely to affirm or refute the hypothesis of no effect:\n\n\n\nVaccine group\nPlacebo group\n\n\n\n\n11 cases\n185 cases\n\n\n\n\\(\\hat{P}(\\text{case is in the vaccine group}) = 0.056 \\quad\\Longrightarrow\\quad \\text{evidence of effect}\\)"
  },
  {
    "objectID": "slides/week5-smoothing.html#data-as-information",
    "href": "slides/week5-smoothing.html#data-as-information",
    "title": "Exploratory analysis and density estimation",
    "section": "Data as information",
    "text": "Data as information\nBy contrast, observational data more often serve the role of information about some phenomenon.\n\nFor example, a secondary trial target might is to assess saftey by gathering observational data on side effects; for this there is no hypothesis."
  },
  {
    "objectID": "slides/week5-smoothing.html#eda-then-cda",
    "href": "slides/week5-smoothing.html#eda-then-cda",
    "title": "Exploratory analysis and density estimation",
    "section": "EDA then CDA",
    "text": "EDA then CDA\nThe picture that most practitioners have of modern data science is that EDA precedes confirmatory data analysis (CDA).\n\nEDA is used to generate hypotheses or formulate a model based on patterns in the data\nCDA, consisting of model specification and estimation, is used for inference and/or prediction\n\n\nAside: Historically, statistics has focused on CDA – and therefore a lot of your PSTAT coursework does, too."
  },
  {
    "objectID": "slides/week5-smoothing.html#essential-exploratory-questions",
    "href": "slides/week5-smoothing.html#essential-exploratory-questions",
    "title": "Exploratory analysis and density estimation",
    "section": "Essential exploratory questions",
    "text": "Essential exploratory questions\n\nWhen you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make.\n\n\nThere are two basic kinds of questions that are always useful:\n\nWhat type of variation occurs within variables?\nWhat type of covariation occurs between variables?"
  },
  {
    "objectID": "slides/week5-smoothing.html#variation-in-one-variable",
    "href": "slides/week5-smoothing.html#variation-in-one-variable",
    "title": "Exploratory analysis and density estimation",
    "section": "Variation in one variable",
    "text": "Variation in one variable\nVariation in data is the tendency of values to change from measurement to measurement.\n\nFor example, the following observations from your mini project data are around 8 \\(\\mu g/m^3\\), but each one is different.\n\n\n\n\n\n\n  \n    \n      \n      PM25_mean\n      City\n      State\n      Year\n    \n  \n  \n    \n      0\n      8.6\n      Aberdeen\n      SD\n      2000\n    \n    \n      1\n      8.6\n      Aberdeen\n      SD\n      2001\n    \n    \n      2\n      7.9\n      Aberdeen\n      SD\n      2002\n    \n    \n      3\n      8.4\n      Aberdeen\n      SD\n      2003\n    \n  \n\n\n\n\n\n\nWhat does it mean to ask what ‘type’ of variation there is in this data?"
  },
  {
    "objectID": "slides/week5-smoothing.html#questions-about-variation",
    "href": "slides/week5-smoothing.html#questions-about-variation",
    "title": "Exploratory analysis and density estimation",
    "section": "Questions about variation",
    "text": "Questions about variation\nThere aren’t exact types of variation, but here are some useful questions:\n\n(Common) Which values are most common?\n(Rare) Which values are rare?\n(Spread) How spread out are the values and how are they spread out?\n(Shape) Are values spread out evenly or irregularly?\n\n\nThese questions often lead the way to more focused ones."
  },
  {
    "objectID": "slides/week5-smoothing.html#air-quality",
    "href": "slides/week5-smoothing.html#air-quality",
    "title": "Exploratory analysis and density estimation",
    "section": "Air quality",
    "text": "Air quality\nThe following histogram shows the distribution of PM 2.5 concentrations across all 200 cities and 20 years.\n\n\n\n\n\n\n\n\nIt shows several statistical properties of the data related to variation:\n\nThe common values have the highest bars – values between roughly 6 and 14.\nValues under 4 and over 18 are rare, accounting for under 5% of the data.\nValues are concentrated between 4 and 18, but are spread from 2 to 52.\nThe shape is pretty even but a little more spread out to the right (“right skew”)."
  },
  {
    "objectID": "slides/week5-smoothing.html#question-refinement",
    "href": "slides/week5-smoothing.html#question-refinement",
    "title": "Exploratory analysis and density estimation",
    "section": "Question refinement",
    "text": "Question refinement\nNew question: The national standard is 12 micrograms per cubic meter. Over 1,000 measurements exceeded this. Was it just a few cities, or more widespread?\n\n\n\n\n\n\n\n\n\n\nMany cities exceeded the standard at some point in time: over 70% of the cities in the dataset. So it was more widespread, but these were the worst:\n\n\n\n\nCity                 State\nHanford-Corcoran      CA      20\nVisalia-Porterville   CA      20\nFresno                CA      19\nBakersfield           CA      18\nName: Years exceeding standard, dtype: int64"
  },
  {
    "objectID": "slides/week5-smoothing.html#further-questions",
    "href": "slides/week5-smoothing.html#further-questions",
    "title": "Exploratory analysis and density estimation",
    "section": "Further questions",
    "text": "Further questions\nHow many cities exceed the benchmark each year? Does this change from year to year?\n\n\n\n\n\n\n\n\n\n\nThere are a lot of years and it’s hard to see anything with all the overlapping bars.\n\nRemember the rule? Don’t stack histograms.\nUse density plots instead."
  },
  {
    "objectID": "slides/week5-smoothing.html#further-questions-1",
    "href": "slides/week5-smoothing.html#further-questions-1",
    "title": "Exploratory analysis and density estimation",
    "section": "Further questions",
    "text": "Further questions\nVisually, it’s a lot easier to distinguish overlapping lines than overlapping bars. Smoothing out the histogram produces this:\n\n\n\n\n\n\n\n\n\n\nThis shows that both the variation in PM 2.5 and the typical values are diminishing over time.\n\nsuggests fewer cities exceed the EPA standard (12 \\(\\mu g/m^3\\)) over time\na few outliers in some early year\nnot the best presentation graphic, but useful exploratory graphic"
  },
  {
    "objectID": "slides/week5-smoothing.html#further-questions-2",
    "href": "slides/week5-smoothing.html#further-questions-2",
    "title": "Exploratory analysis and density estimation",
    "section": "Further questions",
    "text": "Further questions\n\n\nThis gets the message across better.\n\n\n\n\n\n\n\n\nAnd here are those outlying values:\n\n\n\n\n\n\n  \n    \n      \n      PM25_mean\n      City\n      State\n      Year\n    \n  \n  \n    \n      1984\n      51.2\n      Fairbanks\n      AK\n      2004\n    \n    \n      1985\n      31.3\n      Fairbanks\n      AK\n      2005"
  },
  {
    "objectID": "slides/week5-smoothing.html#density-estimates",
    "href": "slides/week5-smoothing.html#density-estimates",
    "title": "Exploratory analysis and density estimation",
    "section": "Density estimates",
    "text": "Density estimates\nAll of the above has amounted to exploration of the distribution of PM 2.5 values across years and cities.\n\nDensity estimates provide smooth approximations of distributions:\n\n\n\n\n\n\n\n\n\nThese are useful tools for answering questions about variation. Relative to the histogram:\n\nEasier to see the shape, spread, and typical values quickly.\nEasier to compare multiple distributions."
  },
  {
    "objectID": "slides/week5-smoothing.html#histograms-and-probability-densities",
    "href": "slides/week5-smoothing.html#histograms-and-probability-densities",
    "title": "Exploratory analysis and density estimation",
    "section": "Histograms and probability densities",
    "text": "Histograms and probability densities\nFrom 120A, a probability density/mass function has two properties:\n\nNonnegative: \\(f(x) \\geq 0\\) for every \\(x \\in \\mathbb{R}\\).\nSums/integrates to one: \\(\\int_\\mathbb{R} f(x) dx = 1\\) or \\(\\sum_{x \\in \\mathbb{R}} f(x) = 1\\)\n\n\nHistograms are almost proper density functions: they satisfy (1) but not (2)."
  },
  {
    "objectID": "slides/week5-smoothing.html#count-scale-histograms",
    "href": "slides/week5-smoothing.html#count-scale-histograms",
    "title": "Exploratory analysis and density estimation",
    "section": "Count scale histograms",
    "text": "Count scale histograms\nWhen the bar height is a count of the number of observations in each bin, the histogram is on the count scale.\n\nMore precisely, if the values are \\(x_1, \\dots, x_n\\), then the height of the bar for the \\(j\\)th bin \\(B_j = (a_j, b_j]\\) is:\n\\[\n\\text{height}_j = \\sum_{i = 1}^n \\mathbf{1}\\{x_i \\in B_j\\}\n\\]\n\n\n\n\nBad for comparisons: the bar heights incomparable in scale whenever the bin widths and/or sample sizes differ."
  },
  {
    "objectID": "slides/week5-smoothing.html#density-scale-histograms",
    "href": "slides/week5-smoothing.html#density-scale-histograms",
    "title": "Exploratory analysis and density estimation",
    "section": "Density scale histograms",
    "text": "Density scale histograms\nA fix that ensures comparability of scale for any two histograms is to normalize heights by bin width \\(b\\) and sample size \\(n\\).\n\\[\n\\text{height}_j = \\color{red}{\\frac{1}{nb}} \\sum_{i = 1}^n \\mathbf{1}\\{x_i \\in B_j\\} \\quad\\text{where}\\quad b = b_j - a_j\n\\]\n\nNow the area under the histogram is \\(\\sum_j b \\times \\text{height}_j = 1\\), so we call this a density scale histogram, because it is a valid probability density."
  },
  {
    "objectID": "slides/week5-smoothing.html#smoothing",
    "href": "slides/week5-smoothing.html#smoothing",
    "title": "Exploratory analysis and density estimation",
    "section": "Smoothing",
    "text": "Smoothing\nKernel density estimates are local smoothings of the density scale histogram.\nThis can be seen by comparing the type of smooth curve we saw earlier with the density scale histogram."
  },
  {
    "objectID": "slides/week5-smoothing.html#how-kde-works",
    "href": "slides/week5-smoothing.html#how-kde-works",
    "title": "Exploratory analysis and density estimation",
    "section": "How KDE works",
    "text": "How KDE works\nTechnically, KDE is a convolution filtering. We can try to understand it in more intuitive terms by developing the idea constructively from the density histogram in two steps.\n\nDo locally adaptive binning\nReplace counting by weighted aggregation"
  },
  {
    "objectID": "slides/week5-smoothing.html#a-preliminary-indicator-functions",
    "href": "slides/week5-smoothing.html#a-preliminary-indicator-functions",
    "title": "Exploratory analysis and density estimation",
    "section": "A preliminary: indicator functions",
    "text": "A preliminary: indicator functions\nIn what follows we’re going to express the histogram mathematically as a function of data values.\n\nThis will require the use of indicator functions, which are simply functions that are 1 or 0 depending on a condition. They are denoted like this:\n\\[\n\\mathbf{1}\\{\\text{condition}\\} = \\begin{cases} 1 &\\text{ if condition is true} \\\\ 0 &\\text{ if condition is false} \\end{cases}\n\\]\n\n\nThe sum of an indicator gives a count of how many times the condition is met:\n\\[\n\\sum_i \\mathbf{1}\\{x_i > 0\\} = \\#\\text{ of values that are positive}\n\\]"
  },
  {
    "objectID": "slides/week5-smoothing.html#the-histogram-as-a-step-function",
    "href": "slides/week5-smoothing.html#the-histogram-as-a-step-function",
    "title": "Exploratory analysis and density estimation",
    "section": "The histogram as a step function",
    "text": "The histogram as a step function\nThe value (height) of the density scale histogram at an arbitrary point \\(\\color{red}{x}\\) is\n\\[\n\\text{hist}(\\color{red}{x}) = \\frac{1}{nb} \\sum_{i = 1}^n \\sum_{j} \\mathbf{1}\\{\\color{red}{x} \\in B_j\\} \\mathbf{1}\\{x_i \\in B_j\\}\n\\]\n\nHere’s what those indicators do:\n\\[\n\\mathbf{1}\\{\\color{red}{x} \\in B_j\\} \\quad \\text{finds the right bin}\\;,\\quad\n\\mathbf{1}\\{x_i \\in B_j\\} \\quad \\text{picks out the data points in the bin}\n\\]"
  },
  {
    "objectID": "slides/week5-smoothing.html#a-local-histogram",
    "href": "slides/week5-smoothing.html#a-local-histogram",
    "title": "Exploratory analysis and density estimation",
    "section": "A ‘local’ histogram",
    "text": "A ‘local’ histogram\nOne could do a ‘moving window’ binning by allowing the height at \\(\\color{red}{x}\\) to be a normalization of the count in a neighborhood of \\(x\\) of width \\(b\\) rather than in one of a fixed set of bins:\n\\[\n\\text{hist}_\\text{local}(\\color{red}{x}) = \\frac{1}{nb} \\sum_{i = 1}^n \\mathbf{1}\\left\\{|x_i - \\color{red}{x}| < \\frac{b}{2}\\right\\}\n\\]\n\nLet’s call this a local histogram, because the height at any point \\(\\color{red}{x}\\) is determined relative to the exact location of \\(\\color{red}{x}\\)."
  },
  {
    "objectID": "slides/week5-smoothing.html#local-vs.-fixed-binning",
    "href": "slides/week5-smoothing.html#local-vs.-fixed-binning",
    "title": "Kernel Density Estimation",
    "section": "Local vs. fixed binning",
    "text": "Local vs. fixed binning"
  },
  {
    "objectID": "slides/week5-smoothing.html#drawing-a-local-histogram",
    "href": "slides/week5-smoothing.html#drawing-a-local-histogram",
    "title": "Exploratory analysis and density estimation",
    "section": "Drawing a local histogram",
    "text": "Drawing a local histogram\n\\[\n\\text{hist}_\\text{local}(\\color{red}{x}) = \\frac{1}{nb} \\sum_{i = 1}^n \\mathbf{1}\\left\\{|x_i - \\color{red}{x}| < \\frac{b}{2}\\right\\}\n\\]"
  },
  {
    "objectID": "slides/week5-smoothing.html#pm-2.5-data",
    "href": "slides/week5-smoothing.html#pm-2.5-data",
    "title": "Exploratory analysis and density estimation",
    "section": "PM 2.5 data",
    "text": "PM 2.5 data\nHere’s what that would look like with \\(b = 1\\) for the air quality data:"
  },
  {
    "objectID": "slides/week5-smoothing.html#pm-2.5-data-1",
    "href": "slides/week5-smoothing.html#pm-2.5-data-1",
    "title": "Exploratory analysis and density estimation",
    "section": "PM 2.5 data",
    "text": "PM 2.5 data\nZooming in reveals that this is still a step function:"
  },
  {
    "objectID": "slides/week5-smoothing.html#the-kernel-function",
    "href": "slides/week5-smoothing.html#the-kernel-function",
    "title": "Exploratory analysis and density estimation",
    "section": "The kernel function",
    "text": "The kernel function\nThe local histogram is in fact a density estimate with a uniform ‘kernel’:\n\\[\n\\text{hist}_\\text{local}(\\color{red}{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\underbrace{\\frac{1}{b}\\mathbf{1}\\left\\{|x_i - \\color{red}{x}| < \\frac{b}{2}\\right\\}}_\\text{kernel function}\n\\]\n\nuniform because the kernel function is constant about \\(x\\)\nwhen \\(x_1, \\dots, x_n\\) are a random sample, this is an estimate of the population denisty"
  },
  {
    "objectID": "slides/week5-smoothing.html#gaussian-kde",
    "href": "slides/week5-smoothing.html#gaussian-kde",
    "title": "Exploratory analysis and density estimation",
    "section": "Gaussian KDE",
    "text": "Gaussian KDE\nReplacing the uniform kernel with a Gaussian kernel yields a smooth density estimate:\n\\[\n\\hat{f}(\\color{red}{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\frac{1}{b}\\varphi\\left(\\frac{x_i - \\color{red}{x}}{b}\\right)\n\\]\n\n\\(\\varphi\\) is the standard Gaussian density \\(\\varphi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{- \\frac{z^2}{2}\\right\\}\\)\n\\(b\\) is the smoothing ‘bandwidth’\n\n\nIn effect, the KDE curve at any point \\(\\color{red}{x}\\) is a weighted aggregation of all the data with weights proportional to their distance from \\(\\color{red}{x}\\)."
  },
  {
    "objectID": "slides/week5-smoothing.html#smoothing-bandwidth",
    "href": "slides/week5-smoothing.html#smoothing-bandwidth",
    "title": "Exploratory analysis and density estimation",
    "section": "Smoothing bandwidth",
    "text": "Smoothing bandwidth\nThe bandwidth parameter \\(b\\) controls how wiggly the KDE curve is.\n\n\n\n\n\n\n\n\nThe choice of smoothing bandwidth can change the visual impression.\n\ntoo much smoothing can obscure outliers and multiple modes\ntoo little smoothing can misleadingly overemphasize sample artefacts"
  },
  {
    "objectID": "slides/week5-smoothing.html#other-kernels",
    "href": "slides/week5-smoothing.html#other-kernels",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Other kernels",
    "text": "Other kernels\nNow let’s try varying the kernel. The ‘local histogram’ introduced last lecture is a KDE with a uniform kernel.\n\n\nCode\n# compute density estimate\nkde = sm.nonparametric.KDEUnivariate(sust.Env_Domain)\nkde.fit(kernel = 'uni', fft = False, bw = 0.02)\n\n# arrange as dataframe\nkde_df = pd.DataFrame({'Environmental sustainability index': kde.support, 'Density': kde.density})\n\n# plot\nsmooth2 = alt.Chart(\n    kde_df\n).mark_line(color = 'black').encode(\n    x = 'Environmental sustainability index:Q',\n    y = alt.Y('Density:Q', scale = alt.Scale(domain = [0, 12]))\n)\n\n(hist + smooth2 + smooth2.mark_area(opacity = 0.3)).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#kde-in-higher-dimensions",
    "href": "slides/week5-smoothing.html#kde-in-higher-dimensions",
    "title": "Exploratory analysis and density estimation",
    "section": "KDE in higher dimensions",
    "text": "KDE in higher dimensions\nUsually for multivariate data it’s easier to work with conditional distributions, but KDE can be generalized to estimating joint densities in \\(p\\) dimensions:\n\\[\n\\hat{f}_K (\\mathbf{x}) = \\frac{1}{n} \\sum_i |\\mathbf{B}|^{-1/2} K \\left(\\mathbf{B^{-1/2}}(\\mathbf{x} - \\mathbf{x}_i)\\right)\n\\]\n\n\\(\\mathbf{x}\\in\\mathbb{R}^p\\) is a \\(p\\)-dimensional vector\n\\(K:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) is a nonnegative kernel function\n\\(B\\) is a \\(p\\times p\\) matrix of bandwidth parameters\n\n\nThe usual approach is to decorrelate the variates and apply a product kernel \\(K(\\mathbf{z}) = K_1(z_1)K_2(z_2)\\cdots K_p(z_p)\\) with separate bandwidths for each dimension."
  },
  {
    "objectID": "slides/week5-smoothing.html#this-week-eda-and-smoothing",
    "href": "slides/week5-smoothing.html#this-week-eda-and-smoothing",
    "title": "Exploratory analysis and density estimation",
    "section": "This week: EDA and smoothing",
    "text": "This week: EDA and smoothing\n\nWhat is exploratory data analysis (EDA)?\n\nThe role of data: information or evidence?\nExploratory vs. confirmatory analysis\nEssential exploratory questions: variation and co-variation\n\nSmoothing\n\nKernel density estimation (KDE)\nLOESS"
  },
  {
    "objectID": "slides/week5-smoothing.html#bivariate-example",
    "href": "slides/week5-smoothing.html#bivariate-example",
    "title": "Exploratory analysis and density estimation",
    "section": "Bivariate example",
    "text": "Bivariate example\n\n\n\n\n\n\nBivariate histogram shown as a raster plot\n\n\n\n\n\n\n\nContours of density estimate\n\n\n\n\n\nDoes race time seem correlated with runner’s age?"
  },
  {
    "objectID": "content.html#week-5",
    "href": "content.html#week-5",
    "title": "Materials",
    "section": "Week 5",
    "text": "Week 5\nReadings:\n\nLDS 11.2 Smoothing and aggregating data\nScott, D.W. (2012). Multivariate Density Estimation and Visualization. In: Gentle, J., Härdle, W., Mori, Y. (eds) Handbook of Computational Statistics. [link to chapter]\n\nMonday: Exploratory analysis and density estimation [slides]\nLab sections: Smoothing [html] [notebook]\nWednesday: Multivariate KDE, mixture models, and scatterplot smoothing [slides] [activity html] [activity notebook]"
  },
  {
    "objectID": "slides/week5-activity.html#outline",
    "href": "slides/week5-activity.html#outline",
    "title": "Kernel smoothing",
    "section": "Outline",
    "text": "Outline\n\nMore on density estimation\n\nnon-Gaussian kernels\nmultivariate KDE\nmixture models\n\nScatterplot smoothing\n\nKernel smoothing\nLOESS"
  },
  {
    "objectID": "slides/week5-activity.html#sustainability-data",
    "href": "slides/week5-activity.html#sustainability-data",
    "title": "Kernel smoothing",
    "section": "Sustainability data",
    "text": "Sustainability data\nWe’ll work with sustainability index data for US cities to explore density estimation further.\n\n\nCode\nsust = pd.read_csv('data/sustainability.csv')\nsust.iloc[:, 1:5].head(2)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      Econ_Domain\n      Social_Domain\n      Env_Domain\n    \n  \n  \n    \n      0\n      Aberdeen, SD Micro Area\n      0.565264\n      0.591259\n      0.444472\n    \n    \n      1\n      Aberdeen, WA Micro Area\n      0.427671\n      0.520744\n      0.429274\n    \n  \n\n\n\n\n\n933 distinct cities\nindices for sustainability in enviornmental, social, and eonomic domains\ncomposite sustainability index"
  },
  {
    "objectID": "slides/week5-activity.html#environmental-sustainability-index",
    "href": "slides/week5-activity.html#environmental-sustainability-index",
    "title": "Kernel smoothing",
    "section": "Environmental sustainability index",
    "text": "Environmental sustainability index\nLet’s use the environmental sustainability index (Env_Domain) initially. The distribution of environmental sustainability across cities is shown below.\n\n\nCode\nhist = alt.Chart(\n    sust\n).transform_bin(\n    'esi',\n    field = 'Env_Domain',\n    bin = alt.Bin(step = 0.02)\n).transform_aggregate(\n    count = 'count()',\n    groupby = ['esi']\n).transform_calculate(\n    Density = 'datum.count/(0.02*933)',\n    ESI = 'datum.esi + 0.01'\n).mark_bar(size = 8).encode(\n    x = 'ESI:Q',\n    y = 'Density:Q'\n)\n\nhist.configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-activity.html#kde-bandwidth-selection",
    "href": "slides/week5-activity.html#kde-bandwidth-selection",
    "title": "Kernel smoothing",
    "section": "KDE bandwidth selection",
    "text": "KDE bandwidth selection\nA common choice for Gaussian KDE bandwidth is Scott’s rule:\n\\[\nb = 1.06 \\times s \\times n^{-1/5}\n\\]\n\n\nCode\n# bandwitdth parameter\nn, p = sust.shape\nsigma_hat = sust.Env_Domain.std()\nbw_scott = 1.06*sigma_hat*n**(-1/5)\n\n# plot\nsmooth = alt.Chart(\n    sust\n).transform_density(\n  'Env_Domain',\n  as_ = ['Environmental sustainability index', 'Density'],\n  extent = [0.1, 0.8],\n  bandwidth = bw_scott\n).mark_line(color = 'black').encode(\n    x = 'Environmental sustainability index:Q',\n    y = 'Density:Q'\n)\n\n(hist + smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-activity.html#computing-kde-without-altair",
    "href": "slides/week5-activity.html#computing-kde-without-altair",
    "title": "Kernel smoothing",
    "section": "Computing KDE without Altair",
    "text": "Computing KDE without Altair\nThe statsmodels package has KDE implementations with finer control. This will allow us to experiment with other kernels.\n\n# compute the gaussian KDE using statsmodels\nkde = sm.nonparametric.KDEUnivariate(sust.Env_Domain)\nkde.fit(bw = bw_scott)\n\n<statsmodels.nonparametric.kde.KDEUnivariate at 0x17aec8e42e0>\n\n\n\nThe object kde has .support and .density attributes; we’ll arrange these into a dataframe:\n\n\nCode\nkde_df = pd.DataFrame({'Environmental sustainability index': kde.support, \n                       'Density': kde.density})\nkde_df.head()\n\n\n\n\n\n\n  \n    \n      \n      Environmental sustainability index\n      Density\n    \n  \n  \n    \n      0\n      0.088614\n      0.000609\n    \n    \n      1\n      0.089334\n      0.000621\n    \n    \n      2\n      0.090054\n      0.000645\n    \n    \n      3\n      0.090774\n      0.000682\n    \n    \n      4\n      0.091494\n      0.000731"
  },
  {
    "objectID": "slides/week5-activity.html#other-kernels",
    "href": "slides/week5-activity.html#other-kernels",
    "title": "Kernel smoothing",
    "section": "Other kernels",
    "text": "Other kernels\nNow let’s try varying the kernel. The ‘local histogram’ introduced last lecture is a KDE with a uniform kernel.\n\n\nCode\n# compute density estimate\nkde = sm.nonparametric.KDEUnivariate(sust.Env_Domain)\nkde.fit(kernel = 'uni', fft = False, bw = 0.02)\n\n# arrange as dataframe\nkde_df = pd.DataFrame({'Environmental sustainability index': kde.support, 'Density': kde.density})\n\n# plot\nsmooth2 = alt.Chart(\n    kde_df\n).mark_line(color = 'black').encode(\n    x = 'Environmental sustainability index:Q',\n    y = alt.Y('Density:Q', scale = alt.Scale(domain = [0, 12]))\n)\n\n(hist + smooth2 + smooth2.mark_area(opacity = 0.3)).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-activity.html#other-kernels-1",
    "href": "slides/week5-activity.html#other-kernels-1",
    "title": "Kernel smoothing",
    "section": "Other kernels",
    "text": "Other kernels\nHere are the forms of some common kernel functions:\n\n\nTitles indicate abbreviations for use in statsmodels.nonparametric.KDEUnivariate"
  },
  {
    "objectID": "slides/week5-activity.html#exploration",
    "href": "slides/week5-activity.html#exploration",
    "title": "Kernel smoothing",
    "section": "Exploration",
    "text": "Exploration\nUse the activity notebook to experiment and answer the following.\n\nHow does the KDE differ if a parabolic (epa) kernel is used in place of a Gaussian (gau) kernel while the bandwidth is held constant?\nWhat effect does a triangular kernel (tri) have on how local peaks appear?\nPick two kernels. What will happen to the KDE for large bandwidths?\nWhich kernel seems to do the best job at capturing the shape closely without under-smoothing?"
  },
  {
    "objectID": "slides/week5-activity.html#multivariate-kde",
    "href": "slides/week5-activity.html#multivariate-kde",
    "title": "Kernel smoothing",
    "section": "Multivariate KDE",
    "text": "Multivariate KDE\nWe’ll use the same data to illustrate multivariate density estimation.\n\nFor this we’ll consider the joint distribution of environmental and economic sustainability indices.\n\n\nCode\nalt.Chart(\n    sust\n).mark_point().encode(\n    x = alt.X('Env_Domain', title = 'Environmental sustainability'),\n    y = alt.Y('Econ_Domain', title = 'Economic sustainability')\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-activity.html#multivariate-histograms",
    "href": "slides/week5-activity.html#multivariate-histograms",
    "title": "Kernel smoothing",
    "section": "Multivariate histograms",
    "text": "Multivariate histograms\nThere are a few options for displaying a 2-D histogram. One is to bin and plot a heatmap, as we saw before:\n\n\nCode\nalt.Chart(\n    sust\n).mark_rect().encode(\n    x = alt.X('Env_Domain', bin = alt.Bin(maxbins = 40), title = 'Environmental sustainability'),\n    y = alt.Y('Econ_Domain', bin = alt.Bin(maxbins = 40), title = 'Economic sustainability'),\n    color = alt.Color('count()', scale = alt.Scale(scheme = 'bluepurple'), title = 'Number of U.S. cities')\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14, \n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-activity.html#multivariate-histograms-1",
    "href": "slides/week5-activity.html#multivariate-histograms-1",
    "title": "Kernel smoothing",
    "section": "Multivariate histograms",
    "text": "Multivariate histograms\nAnother option is to make a bubble chart with the size of the bubble proportional to the count of observations in the corresponding bin:\n\n\nCode\nalt.Chart(\n    sust\n).mark_circle().encode(\n    x = alt.X('Env_Domain', bin = alt.Bin(maxbins = 40), title = 'Environmental sustainability'),\n    y = alt.Y('Econ_Domain', bin = alt.Bin(maxbins = 40), title = 'Economic sustainability'),\n    size = alt.Size('count()', scale = alt.Scale(scheme = 'bluepurple'))\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14, \n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-activity.html#multivariate-kde-1",
    "href": "slides/week5-activity.html#multivariate-kde-1",
    "title": "Kernel smoothing",
    "section": "Multivariate KDE",
    "text": "Multivariate KDE\nThe following computes a multivariate Gaussian KDE\n\n# retrieve data as 2-d array (num observations, num variables)\nfit_ary = sust.loc[:, ['Env_Domain', 'Econ_Domain']].values\n\n# compute KDE\nkde = sm.nonparametric.KDEMultivariate(fit_ary, 'cc')\nkde\n\nKDE instance\nNumber of variables: k_vars = 2\nNumber of samples:   nobs = 933\nVariable types:      cc\nBW selection method: normal_reference\n\n\n\ninput: 2-D \\((\\text{observations} \\times \\text{variables})\\) array\nvariable type specification, one per variate: continuous, discrete, etc.\nmethod of bandwidth selection (not shown)"
  },
  {
    "objectID": "slides/week5-activity.html#prediction-grids",
    "href": "slides/week5-activity.html#prediction-grids",
    "title": "Kernel smoothing",
    "section": "Prediction grids",
    "text": "Prediction grids\nA common visualization strategy is to generate a prediction grid: a mesh of values spanning a domain of interest, for the purpose of computing a function at each grid point.\n\nHere we’ll use a 100 x 100 mesh and use kde.fit() to compute the estimated density at each combination of coordinates.\n\n\nCode\n# resolution of grid (number of points to use along each axis)\ngrid_res = 100\n\n# find grid point coordinates along each axis\nx1 = np.linspace(start = sust.Env_Domain.min(), stop = sust.Env_Domain.max(), num = grid_res)\nx2 = np.linspace(start = sust.Econ_Domain.min(), stop = sust.Econ_Domain.max(), num = grid_res)\n\n# generate a mesh from the coordinates\ngrid1, grid2 = np.meshgrid(x1, x2, indexing = 'ij')\ngrid_ary = np.array([grid1.ravel(), grid2.ravel()]).T\n\n# compute the density at each grid point\nf_hat = kde.pdf(grid_ary)\n\n# rearrange as a dataframe\ngrid_df = pd.DataFrame({'env': grid1.reshape(grid_res**2), \n                        'econ': grid2.reshape(grid_res**2),\n                        'density': f_hat})\n\n# preview, for understanding\ngrid_df.head()\n\n\n\n\n\n\n  \n    \n      \n      env\n      econ\n      density\n    \n  \n  \n    \n      0\n      0.132467\n      0.143811\n      1.325548e-17\n    \n    \n      1\n      0.132467\n      0.150278\n      6.367762e-17\n    \n    \n      2\n      0.132467\n      0.156745\n      2.929104e-16\n    \n    \n      3\n      0.132467\n      0.163212\n      1.290785e-15\n    \n    \n      4\n      0.132467\n      0.169679\n      5.451826e-15"
  },
  {
    "objectID": "slides/week5-activity.html#multivariate-kde-heatmap",
    "href": "slides/week5-activity.html#multivariate-kde-heatmap",
    "title": "Kernel smoothing",
    "section": "Multivariate KDE heatmap",
    "text": "Multivariate KDE heatmap\n\n\nCode\n# kde\nkde_smooth = alt.Chart(\n    grid_df, title = 'Gaussian KDE'\n).mark_rect(opacity = 0.8).encode(\n    x = alt.X('env', bin = alt.Bin(maxbins = grid_res), title = 'Environmental sustainability'),\n    y = alt.Y('econ', bin = alt.Bin(maxbins = grid_res), title = 'Economic sustainability'),\n    color = alt.Color('mean(density)', # a bit hacky, but works\n                      scale = alt.Scale(scheme = 'bluepurple', type = 'sqrt'),\n                      title = 'Density')\n)\n\n# histogram\nbubble = alt.Chart(\n    sust\n).mark_circle().encode(\n    x = alt.X('Env_Domain', bin = alt.Bin(maxbins = 40), title = 'Environmental sustainability'),\n    y = alt.Y('Econ_Domain', bin = alt.Bin(maxbins = 40), title = 'Economic sustainability'),\n    size = alt.Size('count()', scale = alt.Scale(scheme = 'bluepurple'))\n)\n\n# layer\n(bubble + kde_smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14, \n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\nQuestions:\n\nDoes the KDE seem like a good estimate?\nWhat, if anything, does the graphic convey about the sustainability of cities?"
  },
  {
    "objectID": "slides/week5-activity.html#mixture-models",
    "href": "slides/week5-activity.html#mixture-models",
    "title": "Kernel smoothing",
    "section": "Mixture models",
    "text": "Mixture models\nKDE is a nonparametric technique: it involves no population parameters, and thus minimal assumptions about the specific form of distribution being approximated.\n\nMixture models are a parametric alternative for density estimation. The Gaussian mixture model is:\n\\[\nf(x) = a_1 \\varphi(x; \\mu_1, \\sigma_1) + \\cdots + a_n \\varphi(x; \\mu_n, \\sigma_n)\n\\]\n\n\\(f(x)\\) is the distribution of interest\n\\(\\varphi(\\cdot; \\mu, \\sigma)\\) denotes a Gaussian density with mean \\(\\mu\\) and standard deviation \\(\\sigma\\)\nthe model has \\(n\\) components\n\\(a_1, \\dots, a_n\\) are the mixing parameters\nfitted using the EM algorithm"
  },
  {
    "objectID": "slides/week5-activity.html#bivariate-mixture-model",
    "href": "slides/week5-activity.html#bivariate-mixture-model",
    "title": "Kernel smoothing",
    "section": "Bivariate mixture model",
    "text": "Bivariate mixture model\nLet’s fit a mixture to the joint distribution of environmental and economic sustainability indices:\n\n# configure and fit mixture model\ngmm = mixture.GaussianMixture(n_components = 2, covariance_type = 'full')\ngmm.fit(fit_ary)\n\nGaussianMixture(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=2)\n\n\n\nWe can inspect the estimated components’ centers (means):\n\n\nCode\ncenters = pd.DataFrame(gmm.means_).rename(columns = {0: 'env', 1: 'econ'})\ncenters\n\n\n\n\n\n\n  \n    \n      \n      env\n      econ\n    \n  \n  \n    \n      0\n      0.440006\n      0.461773\n    \n    \n      1\n      0.381889\n      0.478218\n    \n  \n\n\n\n\n\n\nAnd the mixing parameters:\n\n\nCode\npd.DataFrame({'mixing': gmm.weights_})\n\n\n\n\n\n\n  \n    \n      \n      mixing\n    \n  \n  \n    \n      0\n      0.720357\n    \n    \n      1\n      0.279643"
  },
  {
    "objectID": "slides/week5-activity.html#bivariate-mixture-model-1",
    "href": "slides/week5-activity.html#bivariate-mixture-model-1",
    "title": "Kernel smoothing",
    "section": "Bivariate mixture model",
    "text": "Bivariate mixture model\n\n\nCode\n# evaluate log-likelihood\ngrid_df['gmm'] = np.exp(gmm.score_samples(grid_ary))\n\n# gmm\ngmm_smooth = alt.Chart(\n    grid_df, title = 'GMM'\n).mark_rect(opacity = 0.8).encode(\n    x = alt.X('env', bin = alt.Bin(maxbins = grid_res), title = 'Environmental sustainability'),\n    y = alt.Y('econ', bin = alt.Bin(maxbins = grid_res), title = 'Economic sustainability'),\n    color = alt.Color('mean(gmm)', # a bit hacky, but works\n                      scale = alt.Scale(scheme = 'bluepurple', type = 'sqrt'),\n                      title = 'Density')\n)\n\n# centers of mixture components\nctr = alt.Chart(centers).mark_point(color = 'black', shape = 'triangle').encode(x = 'env', y = 'econ')\n\n((bubble + gmm_smooth + ctr) | (bubble + kde_smooth)).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14, \n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\n\nWhat differences do you observe? Which do you prefer and why?"
  },
  {
    "objectID": "slides/week5-activity.html#pros-and-cons",
    "href": "slides/week5-activity.html#pros-and-cons",
    "title": "Kernel smoothing",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nKDE is nonparametric and what’s sometimes called a memory-based procedure: it uses all available data every time an estimated value is calculated.\n\nAdvantages: minimal assumptions, highly flexible\nDisadvantages: not parsimonious, computationally intensive\n\n\nGMM’s are parametric models.\n\nAdvantages: closed-form structure, good for multimodal distributions, various kinds of inference and prediction are possible\nDisadvantages: estimation is less straightforward, may over-smooth, possible identifiability issues"
  },
  {
    "objectID": "slides/week5-activity.html#gmms-for-capturing-subpopulations",
    "href": "slides/week5-activity.html#gmms-for-capturing-subpopulations",
    "title": "Kernel smoothing",
    "section": "GMMs for capturing subpopulations",
    "text": "GMMs for capturing subpopulations\nThe GMM is especially useful if there are latent subpopulations in the data. Recall the simulated population of hawks:\n\n\nCode\n# for reproducibility\nnp.random.seed(40721)\n\n# simulate hypothetical population\nfemale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 57.5, scale = 3, size = 3000),\n            'sex': np.repeat('female', 3000)}\n)\n\nmale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 50.5, scale = 3, size = 2000),\n            'sex': np.repeat('male', 2000)}\n)\n\npopulation_hawks = pd.concat([female_hawks, male_hawks], axis = 0)\n\npopulation_hawks.groupby('sex').head(2)\n\n\n\n\n\n\n  \n    \n      \n      length\n      sex\n    \n  \n  \n    \n      0\n      53.975230\n      female\n    \n    \n      1\n      60.516768\n      female\n    \n    \n      0\n      53.076663\n      male\n    \n    \n      1\n      49.933166\n      male\n    \n  \n\n\n\n\n\nLet’s imagine we have a sample but without recorded sexes.\n\n\nCode\nnp.random.seed(50223)\nsamp = population_hawks.sample(n = 500).drop(columns = 'sex')\nsamp.head(3)\n\n\n\n\n\n\n  \n    \n      \n      length\n    \n  \n  \n    \n      840\n      62.513960\n    \n    \n      2747\n      52.490258\n    \n    \n      2698\n      56.879861"
  },
  {
    "objectID": "slides/week5-activity.html#hawks",
    "href": "slides/week5-activity.html#hawks",
    "title": "Kernel smoothing",
    "section": "Hawks",
    "text": "Hawks\nThe histogram is not obviously bimodal, but we can suppose we’re aware of the sex differences in length and just don’t have that information.\n\n\nCode\nhist_hawks = alt.Chart(samp).transform_bin(\n    'length',\n    field = 'length',\n    bin = alt.Bin(step = 2)\n).transform_aggregate(\n    count = 'count()',\n    groupby = ['length']\n).transform_calculate(\n    density = 'datum.count/1000',\n    length = 'datum.length + 1'\n).mark_bar(size = 20).encode(\n    x = 'length:Q', \n    y = 'density:Q'\n)\n\nhist_hawks.configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-activity.html#recovering-subpopulations",
    "href": "slides/week5-activity.html#recovering-subpopulations",
    "title": "Kernel smoothing",
    "section": "Recovering subpopulations",
    "text": "Recovering subpopulations\nIf we fit a mixture model with two components, amazingly, the mixture components accurately recover the distributions for each sex, even though this was a latent (unobserved) variable:\n\n# configure and fit mixture model\ngmm_hawks = mixture.GaussianMixture(n_components = 2)\ngmm_hawks.fit(samp.length.values.reshape(-1, 1))\n\n# compare components with subpopulations\nprint('gmm component means: ', gmm_hawks.means_.ravel())\nprint('population means by sex: ', population_hawks.groupby('sex').mean().values.ravel())\n\nprint('gmm component variances: ', gmm_hawks.covariances_.ravel())\nprint('population variances by sex: ', population_hawks.groupby('sex').var().values.ravel())\n\ngmm component means:  [50.70543698 57.33071086]\npopulation means by sex:  [57.5749014  50.48194081]\ngmm component variances:  [ 8.60692088 10.12549367]\npopulation variances by sex:  [9.39645762 8.522606  ]\n\n\n\nNote that the means and variances are estimated from the sample but compared with the population values above."
  },
  {
    "objectID": "slides/week5-activity.html#gmm-density-estimate",
    "href": "slides/week5-activity.html#gmm-density-estimate",
    "title": "Kernel smoothing",
    "section": "GMM density estimate",
    "text": "GMM density estimate\nFurther, the density estimate fits reasonably well:\n\n\nCode\n# compute a grid of lengths\ngrid_hawks = np.linspace(population_hawks.length.min(), population_hawks.length.max(), num = 500)\ndens = np.exp(gmm_hawks.score_samples(grid_hawks.reshape(-1, 1)))\n\ngmm_smooth_hawks = alt.Chart(\n    pd.DataFrame({'length': grid_hawks, 'density': dens})\n).mark_line(color = 'black').encode(\n    x = 'length',\n    y = 'density'\n)\n\n(hist_hawks + gmm_smooth_hawks).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-activity.html#explore",
    "href": "slides/week5-activity.html#explore",
    "title": "Kernel smoothing",
    "section": "Explore",
    "text": "Explore\nTake a few minutes and the activity notebook to explore the following questions.\n\nWhat happens if you fit the GMM with different numbers of components?\nDoes the solution change if the GMM is re-fitted?"
  },
  {
    "objectID": "slides/week5-activity.html#kernel-smoothing",
    "href": "slides/week5-activity.html#kernel-smoothing",
    "title": "Kernel smoothing",
    "section": "Kernel smoothing",
    "text": "Kernel smoothing\nKernel smoothing is a technique similar to KDE used to visualize and estimate relationships (rather than distributions).\n\nWe’ll use the GDP and life expectancy data from lab 3 to illustrate.\n\n\nCode\n# read in data and subset\nlife = pd.read_csv('data/life-gdp.csv')\nlife = life[life.Year == 2000].loc[:, ['Country Name', 'All', 'GDP per capita']]\nlife['GDP per capita'] = np.log(life['GDP per capita'])\nlife.rename(columns = {'GDP per capita': 'log_gdp', 'All': 'life_exp', 'Country Name': 'country'}, inplace=True)\n\n# scatterplot\nscatter = alt.Chart(\n    life\n).mark_point().encode(\n    x = alt.X('log_gdp', scale = alt.Scale(zero = False), title = 'log(GDP/capita)'),\n    y = alt.Y('life_exp', scale = alt.Scale(zero = False), title = 'Life expectancy')\n)\n\nscatter.configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-activity.html#the-method",
    "href": "slides/week5-activity.html#the-method",
    "title": "Kernel smoothing",
    "section": "The method",
    "text": "The method\nThe technique consists in estimating trend by local weighted averaging; a kernel function is used to determine the exact weights.\n\\[\n\\hat{y}(x) = \\frac{\\sum_i K_b (x_i - x) y_i}{\\sum_i K_b(x_i - x)}\n\\]\n\n\n\n\nIllustration of kernel smoothing"
  },
  {
    "objectID": "slides/week5-activity.html#example",
    "href": "slides/week5-activity.html#example",
    "title": "Kernel smoothing",
    "section": "Example",
    "text": "Example\n\n\nCode\n# calculate kernel smoother\nkernreg = sm.nonparametric.KernelReg(endog = life.life_exp.values,\n                                     exog = life.log_gdp.values,\n                                     reg_type = 'lc', \n                                     var_type = 'c',\n                                     ckertype = 'gaussian')\n\n# grid of gdp values\ngdp_grid = np.linspace(life.log_gdp.min(), life.log_gdp.max(), num = 100)\n\n# calculate kernel smooth at each value\nfitted_values = kernreg.fit(gdp_grid)[0]\n\n# arrange as data frame\npred_df = pd.DataFrame({'log_gdp': gdp_grid, 'life_exp': fitted_values})\n\n# plot\nkernel_smooth = alt.Chart(\n    pred_df\n).mark_line(\n    color = 'black'\n).encode(\n    x = 'log_gdp', \n    y = alt.Y('life_exp', scale = alt.Scale(zero = False))\n)\n\n(scatter + kernel_smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\nNotice that the kernel smooth trails off a bit near the boundary – this is because fewer points are being averaged once the smoothing window begins to extend beyond the range of the data."
  },
  {
    "objectID": "slides/week5-activity.html#loess",
    "href": "slides/week5-activity.html#loess",
    "title": "Kernel smoothing",
    "section": "LOESS",
    "text": "LOESS\nLocally weighted scatterplot smoothing (LOESS or LOWESS) largely corrects this issue by stitching together lines (or low-order polynomials) fitted to local subsets of data; this way, near the boundary, the estimate still takes account of any trend present in the data."
  },
  {
    "objectID": "slides/week5-activity.html#loess-example",
    "href": "slides/week5-activity.html#loess-example",
    "title": "Kernel smoothing",
    "section": "LOESS example",
    "text": "LOESS example\n\n\nCode\n# fit loess smooth\nloess = sm.nonparametric.lowess(endog = life.life_exp.values,\n                                exog = life.log_gdp.values, \n                                frac = 0.3,\n                                xvals = gdp_grid)\n\n# store as data frame\nloess_df = pd.DataFrame({'log_gdp': gdp_grid, 'life_exp': loess})\n\n# plot\nloess_smooth = alt.Chart(\n    loess_df\n).mark_line(\n    color = 'blue',\n    strokeDash = [8, 8]\n).encode(\n    x = 'log_gdp', \n    y = alt.Y('life_exp', scale = alt.Scale(zero = False))\n)\n\n(scatter + loess_smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-activity.html#comparison",
    "href": "slides/week5-activity.html#comparison",
    "title": "Kernel smoothing",
    "section": "Comparison",
    "text": "Comparison\nIf we compare the LOESS curve with the kernel smoother, the different behavior on the boundary is evident:\n\n\nCode\n(scatter + loess_smooth + kernel_smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-activity.html#explore-1",
    "href": "slides/week5-activity.html#explore-1",
    "title": "Kernel smoothing",
    "section": "Explore",
    "text": "Explore\nUse the activity notebook to answer the following questions.\n\nAre there any bandwidths that give you a straight-line fit?\nWhat seems to be the minimum bandwidth?\nWhich bandwidth best captures the pattern of scatter?"
  },
  {
    "objectID": "slides/week5-density.html#announcements",
    "href": "slides/week5-density.html#announcements",
    "title": "Exploratory analysis and density estimation",
    "section": "Announcements",
    "text": "Announcements\n\nNo assignment posted this week\nNo lab next week"
  },
  {
    "objectID": "slides/week5-density.html#this-week-eda-and-smoothing",
    "href": "slides/week5-density.html#this-week-eda-and-smoothing",
    "title": "Exploratory analysis and density estimation",
    "section": "This week: EDA and smoothing",
    "text": "This week: EDA and smoothing\n\nWhat is exploratory data analysis (EDA)?\n\nThe role of data: information or evidence?\nExploratory vs. confirmatory analysis\nEssential exploratory questions: variation and co-variation\n\nSmoothing\n\nKernel density estimation (KDE)\nLOESS"
  },
  {
    "objectID": "slides/week5-density.html#eda",
    "href": "slides/week5-density.html#eda",
    "title": "Exploratory analysis and density estimation",
    "section": "EDA",
    "text": "EDA\nThe term and spirit of exploratory data analysis (EDA) is attributed to John Tukey, whose philosophically-leaning work in statistics in the 1960’s and 1970’s stressed the need for more data-driven methods.\n\n\nFor a long time I have thought I was a statistician, interested in inferences from the particular to the general … All in all, I have come to feel that my central interest is in data analysis [which] is a larger and more varied field than inference. (Tukey, 1962)\n\n\n\nEDA is an initial stage of non-inferential and largely model-free analysis aiming at understanding the structure, patterns, and particularities present in a dataset."
  },
  {
    "objectID": "slides/week5-density.html#data-as-evidence",
    "href": "slides/week5-density.html#data-as-evidence",
    "title": "Exploratory analysis and density estimation",
    "section": "Data as evidence",
    "text": "Data as evidence\nExperimental data usually serve the role of evidence for or against prespecified hypotheses.\n\n\\[\n\\text{hypothesis} \\longrightarrow \\text{data} \\longrightarrow \\text{inference}\n\\]\n\n\nFor example, in vaccine efficacy trials, trial data are collected precisely to affirm or refute the hypothesis of no effect:\n\n\n\nVaccine group\nPlacebo group\n\n\n\n\n11 cases\n185 cases\n\n\n\n\\(\\hat{P}(\\text{case is in the vaccine group}) = 0.056 \\quad\\Longrightarrow\\quad \\text{evidence of effect}\\)"
  },
  {
    "objectID": "slides/week5-density.html#data-as-information",
    "href": "slides/week5-density.html#data-as-information",
    "title": "Exploratory analysis and density estimation",
    "section": "Data as information",
    "text": "Data as information\nBy contrast, observational data more often serve the role of information about some phenomenon.\n\nFor example, a secondary trial target might is to assess saftey by gathering observational data on side effects; for this there is no hypothesis."
  },
  {
    "objectID": "slides/week5-density.html#eda-then-cda",
    "href": "slides/week5-density.html#eda-then-cda",
    "title": "Exploratory analysis and density estimation",
    "section": "EDA then CDA",
    "text": "EDA then CDA\nThe picture that most practitioners have of modern data science is that EDA precedes confirmatory data analysis (CDA).\n\nEDA is used to generate hypotheses or formulate a model based on patterns in the data\nCDA, consisting of model specification and estimation, is used for inference and/or prediction\n\n\nAside: Historically, statistics has focused on CDA – and therefore a lot of your PSTAT coursework does, too."
  },
  {
    "objectID": "slides/week5-density.html#essential-exploratory-questions",
    "href": "slides/week5-density.html#essential-exploratory-questions",
    "title": "Exploratory analysis and density estimation",
    "section": "Essential exploratory questions",
    "text": "Essential exploratory questions\n\nWhen you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make.\n\n\nThere are two basic kinds of questions that are always useful:\n\nWhat type of variation occurs within variables?\nWhat type of covariation occurs between variables?"
  },
  {
    "objectID": "slides/week5-density.html#variation-in-one-variable",
    "href": "slides/week5-density.html#variation-in-one-variable",
    "title": "Exploratory analysis and density estimation",
    "section": "Variation in one variable",
    "text": "Variation in one variable\nVariation in data is the tendency of values to change from measurement to measurement.\n\nFor example, the following observations from your mini project data are around 8 \\(\\mu g/m^3\\), but each one is different.\n\n\n\n\n\n\n  \n    \n      \n      PM25_mean\n      City\n      State\n      Year\n    \n  \n  \n    \n      0\n      8.6\n      Aberdeen\n      SD\n      2000\n    \n    \n      1\n      8.6\n      Aberdeen\n      SD\n      2001\n    \n    \n      2\n      7.9\n      Aberdeen\n      SD\n      2002\n    \n    \n      3\n      8.4\n      Aberdeen\n      SD\n      2003\n    \n  \n\n\n\n\n\n\nWhat does it mean to ask what ‘type’ of variation there is in this data?"
  },
  {
    "objectID": "slides/week5-density.html#questions-about-variation",
    "href": "slides/week5-density.html#questions-about-variation",
    "title": "Exploratory analysis and density estimation",
    "section": "Questions about variation",
    "text": "Questions about variation\nThere aren’t exact types of variation, but here are some useful questions:\n\n(Common) Which values are most common?\n(Rare) Which values are rare?\n(Spread) How spread out are the values and how are they spread out?\n(Shape) Are values spread out evenly or irregularly?\n\n\nThese questions often lead the way to more focused ones."
  },
  {
    "objectID": "slides/week5-density.html#air-quality",
    "href": "slides/week5-density.html#air-quality",
    "title": "Exploratory analysis and density estimation",
    "section": "Air quality",
    "text": "Air quality\nThe following histogram shows the distribution of PM 2.5 concentrations across all 200 cities and 20 years.\n\n\n\n\n\n\n\n\nIt shows several statistical properties of the data related to variation:\n\nThe common values have the highest bars – values between roughly 6 and 14.\nValues under 4 and over 18 are rare, accounting for under 5% of the data.\nValues are concentrated between 4 and 18, but are spread from 2 to 52.\nThe shape is pretty even but a little more spread out to the right (“right skew”)."
  },
  {
    "objectID": "slides/week5-density.html#question-refinement",
    "href": "slides/week5-density.html#question-refinement",
    "title": "Exploratory analysis and density estimation",
    "section": "Question refinement",
    "text": "Question refinement\nNew question: The national standard is 12 micrograms per cubic meter. Over 1,000 measurements exceeded this. Was it just a few cities, or more widespread?\n\n\n\n\n\n\n\n\n\n\nMany cities exceeded the standard at some point in time: over 70% of the cities in the dataset. So it was more widespread, but these were the worst:\n\n\n\n\nCity                 State\nHanford-Corcoran      CA      20\nVisalia-Porterville   CA      20\nFresno                CA      19\nBakersfield           CA      18\nName: Years exceeding standard, dtype: int64"
  },
  {
    "objectID": "slides/week5-density.html#further-questions",
    "href": "slides/week5-density.html#further-questions",
    "title": "Exploratory analysis and density estimation",
    "section": "Further questions",
    "text": "Further questions\nHow many cities exceed the benchmark each year? Does this change from year to year?\n\n\n\n\n\n\n\n\n\n\nThere are a lot of years and it’s hard to see anything with all the overlapping bars.\n\nRemember the rule? Don’t stack histograms.\nUse density plots instead."
  },
  {
    "objectID": "slides/week5-density.html#further-questions-1",
    "href": "slides/week5-density.html#further-questions-1",
    "title": "Exploratory analysis and density estimation",
    "section": "Further questions",
    "text": "Further questions\nVisually, it’s a lot easier to distinguish overlapping lines than overlapping bars. Smoothing out the histogram produces this:\n\n\n\n\n\n\n\n\n\n\nThis shows that both the variation in PM 2.5 and the typical values are diminishing over time.\n\nsuggests fewer cities exceed the EPA standard (12 \\(\\mu g/m^3\\)) over time\na few outliers in some early year\nnot the best presentation graphic, but useful exploratory graphic"
  },
  {
    "objectID": "slides/week5-density.html#further-questions-2",
    "href": "slides/week5-density.html#further-questions-2",
    "title": "Exploratory analysis and density estimation",
    "section": "Further questions",
    "text": "Further questions\n\n\nThis gets the message across better.\n\n\n\n\n\n\n\n\nAnd here are those outlying values:\n\n\n\n\n\n\n  \n    \n      \n      PM25_mean\n      City\n      State\n      Year\n    \n  \n  \n    \n      1984\n      51.2\n      Fairbanks\n      AK\n      2004\n    \n    \n      1985\n      31.3\n      Fairbanks\n      AK\n      2005"
  },
  {
    "objectID": "slides/week5-density.html#density-estimates",
    "href": "slides/week5-density.html#density-estimates",
    "title": "Exploratory analysis and density estimation",
    "section": "Density estimates",
    "text": "Density estimates\nAll of the above has amounted to exploration of the distribution of PM 2.5 values across years and cities.\n\nDensity estimates provide smooth approximations of distributions:\n\n\n\n\n\n\n\n\n\nThese are useful tools for answering questions about variation. Relative to the histogram:\n\nEasier to see the shape, spread, and typical values quickly.\nEasier to compare multiple distributions."
  },
  {
    "objectID": "slides/week5-density.html#histograms-and-probability-densities",
    "href": "slides/week5-density.html#histograms-and-probability-densities",
    "title": "Exploratory analysis and density estimation",
    "section": "Histograms and probability densities",
    "text": "Histograms and probability densities\nFrom 120A, a probability density/mass function has two properties:\n\nNonnegative: \\(f(x) \\geq 0\\) for every \\(x \\in \\mathbb{R}\\).\nSums/integrates to one: \\(\\int_\\mathbb{R} f(x) dx = 1\\) or \\(\\sum_{x \\in \\mathbb{R}} f(x) = 1\\)\n\n\nHistograms are almost proper density functions: they satisfy (1) but not (2)."
  },
  {
    "objectID": "slides/week5-density.html#a-preliminary-indicator-functions",
    "href": "slides/week5-density.html#a-preliminary-indicator-functions",
    "title": "Exploratory analysis and density estimation",
    "section": "A preliminary: indicator functions",
    "text": "A preliminary: indicator functions\nIn what follows we’re going to express the histogram mathematically as a function of data values.\n\nThis will require the use of indicator functions, which are simply functions that are 1 or 0 depending on a condition. They are denoted like this:\n\\[\n\\mathbf{1}\\{\\text{condition}\\} = \\begin{cases} 1 &\\text{ if condition is true} \\\\ 0 &\\text{ if condition is false} \\end{cases}\n\\]\n\n\nThe sum of an indicator gives a count of how many times the condition is met:\n\\[\n\\sum_i \\mathbf{1}\\{x_i > 0\\} = \\#\\text{ of values that are positive}\n\\]"
  },
  {
    "objectID": "slides/week5-density.html#count-scale-histograms",
    "href": "slides/week5-density.html#count-scale-histograms",
    "title": "Exploratory analysis and density estimation",
    "section": "Count scale histograms",
    "text": "Count scale histograms\nWhen the bar height is a count of the number of observations in each bin, the histogram is on the count scale.\n\nMore precisely, if the values are \\(x_1, \\dots, x_n\\), then the height of the bar for the \\(j\\)th bin \\(B_j = (a_j, b_j]\\) is:\n\\[\n\\text{height}_j = \\sum_{i = 1}^n \\mathbf{1}\\{x_i \\in B_j\\}\n\\]\n\n\n\n\nBad for comparisons: the bar heights incomparable in scale whenever the bin widths and/or sample sizes differ."
  },
  {
    "objectID": "slides/week5-density.html#density-scale-histograms",
    "href": "slides/week5-density.html#density-scale-histograms",
    "title": "Exploratory analysis and density estimation",
    "section": "Density scale histograms",
    "text": "Density scale histograms\nA fix that ensures comparability of scale for any two histograms is to normalize heights by bin width \\(b\\) and sample size \\(n\\).\n\\[\n\\text{height}_j = \\color{red}{\\frac{1}{nb}} \\sum_{i = 1}^n \\mathbf{1}\\{x_i \\in B_j\\} \\quad\\text{where}\\quad b = b_j - a_j\n\\]\n\nNow the area under the histogram is \\(\\sum_j b \\times \\text{height}_j = 1\\), so we call this a density scale histogram, because it is a valid probability density."
  },
  {
    "objectID": "slides/week5-density.html#smoothing",
    "href": "slides/week5-density.html#smoothing",
    "title": "Exploratory analysis and density estimation",
    "section": "Smoothing",
    "text": "Smoothing\nKernel density estimates are local smoothings of the density scale histogram.\nThis can be seen by comparing the type of smooth curve we saw earlier with the density scale histogram."
  },
  {
    "objectID": "slides/week5-density.html#how-kde-works",
    "href": "slides/week5-density.html#how-kde-works",
    "title": "Exploratory analysis and density estimation",
    "section": "How KDE works",
    "text": "How KDE works\nTechnically, KDE is a convolution filtering. We can try to understand it in more intuitive terms by developing the idea constructively from the density histogram in two steps.\n\nDo locally adaptive binning\nReplace counting by weighted aggregation"
  },
  {
    "objectID": "slides/week5-density.html#the-histogram-as-a-step-function",
    "href": "slides/week5-density.html#the-histogram-as-a-step-function",
    "title": "Exploratory analysis and density estimation",
    "section": "The histogram as a step function",
    "text": "The histogram as a step function\nThe value (height) of the density scale histogram at an arbitrary point \\(\\color{red}{x}\\) is\n\\[\n\\text{hist}(\\color{red}{x}) = \\frac{1}{nb} \\sum_{i = 1}^n \\sum_{j} \\mathbf{1}\\{\\color{red}{x} \\in B_j\\} \\mathbf{1}\\{x_i \\in B_j\\}\n\\]\n\nHere’s what those indicators do:\n\\[\n\\mathbf{1}\\{\\color{red}{x} \\in B_j\\} \\quad \\text{finds the right bin}\\;,\\quad\n\\mathbf{1}\\{x_i \\in B_j\\} \\quad \\text{picks out the data points in the bin}\n\\]"
  },
  {
    "objectID": "slides/week5-density.html#a-local-histogram",
    "href": "slides/week5-density.html#a-local-histogram",
    "title": "Exploratory analysis and density estimation",
    "section": "A ‘local’ histogram",
    "text": "A ‘local’ histogram\nOne could do a ‘moving window’ binning by allowing the height at \\(\\color{red}{x}\\) to be a normalization of the count in a neighborhood of \\(x\\) of width \\(b\\) rather than in one of a fixed set of bins:\n\\[\n\\text{hist}_\\text{local}(\\color{red}{x}) = \\frac{1}{nb} \\sum_{i = 1}^n \\mathbf{1}\\left\\{|x_i - \\color{red}{x}| < \\frac{b}{2}\\right\\}\n\\]\n\nLet’s call this a local histogram, because the height at any point \\(\\color{red}{x}\\) is determined relative to the exact location of \\(\\color{red}{x}\\)."
  },
  {
    "objectID": "slides/week5-density.html#drawing-a-local-histogram",
    "href": "slides/week5-density.html#drawing-a-local-histogram",
    "title": "Exploratory analysis and density estimation",
    "section": "Drawing a local histogram",
    "text": "Drawing a local histogram\n\\[\n\\text{hist}_\\text{local}(\\color{red}{x}) = \\frac{1}{nb} \\sum_{i = 1}^n \\mathbf{1}\\left\\{|x_i - \\color{red}{x}| < \\frac{b}{2}\\right\\}\n\\]"
  },
  {
    "objectID": "slides/week5-density.html#pm-2.5-data",
    "href": "slides/week5-density.html#pm-2.5-data",
    "title": "Exploratory analysis and density estimation",
    "section": "PM 2.5 data",
    "text": "PM 2.5 data\nHere’s what that would look like with \\(b = 1\\) for the air quality data:"
  },
  {
    "objectID": "slides/week5-density.html#pm-2.5-data-1",
    "href": "slides/week5-density.html#pm-2.5-data-1",
    "title": "Exploratory analysis and density estimation",
    "section": "PM 2.5 data",
    "text": "PM 2.5 data\nZooming in reveals that this is still a step function:"
  },
  {
    "objectID": "slides/week5-density.html#the-kernel-function",
    "href": "slides/week5-density.html#the-kernel-function",
    "title": "Exploratory analysis and density estimation",
    "section": "The kernel function",
    "text": "The kernel function\nThe local histogram is in fact a density estimate with a uniform ‘kernel’:\n\\[\n\\text{hist}_\\text{local}(\\color{red}{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\underbrace{\\frac{1}{b}\\mathbf{1}\\left\\{|x_i - \\color{red}{x}| < \\frac{b}{2}\\right\\}}_\\text{kernel function}\n\\]\n\nuniform because the kernel function is constant about \\(x\\)\nwhen \\(x_1, \\dots, x_n\\) are a random sample, this is an estimate of the population denisty"
  },
  {
    "objectID": "slides/week5-density.html#gaussian-kde",
    "href": "slides/week5-density.html#gaussian-kde",
    "title": "Exploratory analysis and density estimation",
    "section": "Gaussian KDE",
    "text": "Gaussian KDE\nReplacing the uniform kernel with a Gaussian kernel yields a smooth density estimate:\n\\[\n\\hat{f}(\\color{red}{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\frac{1}{b}\\varphi\\left(\\frac{x_i - \\color{red}{x}}{b}\\right)\n\\]\n\n\\(\\varphi\\) is the standard Gaussian density \\(\\varphi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{- \\frac{z^2}{2}\\right\\}\\)\n\\(b\\) is the smoothing ‘bandwidth’\n\n\nIn effect, the KDE curve at any point \\(\\color{red}{x}\\) is a weighted aggregation of all the data with weights proportional to their distance from \\(\\color{red}{x}\\)."
  },
  {
    "objectID": "slides/week5-density.html#smoothing-bandwidth",
    "href": "slides/week5-density.html#smoothing-bandwidth",
    "title": "Exploratory analysis and density estimation",
    "section": "Smoothing bandwidth",
    "text": "Smoothing bandwidth\nThe bandwidth parameter \\(b\\) controls how wiggly the KDE curve is.\n\n\n\n\n\n\n\n\nThe choice of smoothing bandwidth can change the visual impression.\n\ntoo much smoothing can obscure outliers and multiple modes\ntoo little smoothing can misleadingly overemphasize sample artefacts"
  },
  {
    "objectID": "slides/week5-density.html#other-kernels",
    "href": "slides/week5-density.html#other-kernels",
    "title": "Exploratory analysis and density estimation",
    "section": "Other kernels",
    "text": "Other kernels\nIn general, a KDE can be computed with any appropriately normalized nonnegative kernel function \\(K_b(\\cdot)\\).\n\\[\n\\hat{f}_{K_b}(\\color{red}{x}) = \\frac{1}{n} \\sum_{i = 1}^n K_b\\left(x_i - \\color{red}{x}\\right)\n\\]\nOther common kernel functions include:\n\ntriangular kernel \\(K(z) = 1 - |z|\\)\nparabolic kernel \\(K(z) \\propto 1 - z^2\\)\ncosine kernel \\(K(z) \\propto \\cos\\left(\\frac{\\pi z}{2}\\right)\\)\ncircular density \\(K(z) \\propto \\exp\\left\\{k\\cos(z)\\right\\}\\)\nany symmetric continuous probability density function"
  },
  {
    "objectID": "slides/week5-density.html#kde-in-higher-dimensions",
    "href": "slides/week5-density.html#kde-in-higher-dimensions",
    "title": "Exploratory analysis and density estimation",
    "section": "KDE in higher dimensions",
    "text": "KDE in higher dimensions\nUsually for multivariate data it’s easier to work with conditional distributions, but KDE can be generalized to estimating joint densities in \\(p\\) dimensions:\n\\[\n\\hat{f}_K (\\mathbf{x}) = \\frac{1}{n} \\sum_i |\\mathbf{B}|^{-1/2} K \\left(\\mathbf{B^{-1/2}}(\\mathbf{x} - \\mathbf{x}_i)\\right)\n\\]\n\n\\(\\mathbf{x}\\in\\mathbb{R}^p\\) is a \\(p\\)-dimensional vector\n\\(K:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) is a nonnegative kernel function\n\\(B\\) is a \\(p\\times p\\) matrix of bandwidth parameters\n\n\nThe usual approach is to decorrelate the variates and apply a product kernel \\(K(\\mathbf{z}) = K_1(z_1)K_2(z_2)\\cdots K_p(z_p)\\) with separate bandwidths for each dimension."
  },
  {
    "objectID": "slides/week5-density.html#bivariate-example",
    "href": "slides/week5-density.html#bivariate-example",
    "title": "Exploratory analysis and density estimation",
    "section": "Bivariate example",
    "text": "Bivariate example\n\n\n\n\n\n\nBivariate histogram shown as a raster plot\n\n\n\n\n\n\n\nContours of density estimate\n\n\n\n\n\nDoes race time seem correlated with runner’s age?"
  },
  {
    "objectID": "slides/week5-smoothing.html#outline",
    "href": "slides/week5-smoothing.html#outline",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Outline",
    "text": "Outline\n\nMore on density estimation\n\nnon-Gaussian kernels\nmultivariate KDE\nmixture models\n\nScatterplot smoothing\n\nKernel smoothing\nLOESS"
  },
  {
    "objectID": "slides/week5-smoothing.html#sustainability-data",
    "href": "slides/week5-smoothing.html#sustainability-data",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Sustainability data",
    "text": "Sustainability data\nWe’ll work with sustainability index data for US cities to explore density estimation further.\n\n\nCode\nsust = pd.read_csv('data/sustainability.csv')\nsust.iloc[:, 1:5].head(2)\n\n\n\n\n\n\n  \n    \n      \n      Name\n      Econ_Domain\n      Social_Domain\n      Env_Domain\n    \n  \n  \n    \n      0\n      Aberdeen, SD Micro Area\n      0.565264\n      0.591259\n      0.444472\n    \n    \n      1\n      Aberdeen, WA Micro Area\n      0.427671\n      0.520744\n      0.429274\n    \n  \n\n\n\n\n\n933 distinct cities\nindices for sustainability in enviornmental, social, and eonomic domains"
  },
  {
    "objectID": "slides/week5-smoothing.html#environmental-sustainability-index",
    "href": "slides/week5-smoothing.html#environmental-sustainability-index",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Environmental sustainability index",
    "text": "Environmental sustainability index\nLet’s use the environmental sustainability index (Env_Domain) initially. The distribution of environmental sustainability across cities is shown below.\n\n\nCode\nhist = alt.Chart(\n    sust\n).transform_bin(\n    'esi',\n    field = 'Env_Domain',\n    bin = alt.Bin(step = 0.02)\n).transform_aggregate(\n    count = 'count()',\n    groupby = ['esi']\n).transform_calculate(\n    Density = 'datum.count/(0.02*933)',\n    ESI = 'datum.esi + 0.01'\n).mark_bar(size = 8).encode(\n    x = 'ESI:Q',\n    y = 'Density:Q'\n)\n\nhist.configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#kde-bandwidth-selection",
    "href": "slides/week5-smoothing.html#kde-bandwidth-selection",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "KDE bandwidth selection",
    "text": "KDE bandwidth selection\nA common choice for Gaussian KDE bandwidth is Scott’s rule:\n\\[\nb = 1.06 \\times s \\times n^{-1/5}\n\\]\n\n\nCode\n# bandwitdth parameter\nn, p = sust.shape\nsigma_hat = sust.Env_Domain.std()\nbw_scott = 1.06*sigma_hat*n**(-1/5)\n\n# plot\nsmooth = alt.Chart(\n    sust\n).transform_density(\n  'Env_Domain',\n  as_ = ['Environmental sustainability index', 'Density'],\n  extent = [0.1, 0.8],\n  bandwidth = bw_scott\n).mark_line(color = 'black').encode(\n    x = 'Environmental sustainability index:Q',\n    y = 'Density:Q'\n)\n\n(hist + smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#computing-kde-without-altair",
    "href": "slides/week5-smoothing.html#computing-kde-without-altair",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Computing KDE without Altair",
    "text": "Computing KDE without Altair\nThe statsmodels package has KDE implementations with finer control. This will allow us to experiment with other kernels.\n\n# compute the gaussian KDE using statsmodels\nkde = sm.nonparametric.KDEUnivariate(sust.Env_Domain)\nkde.fit(bw = bw_scott)\n\n<statsmodels.nonparametric.kde.KDEUnivariate at 0x2a441e42cd0>\n\n\n\nThe object kde has .support and .density attributes; we’ll arrange these into a dataframe:\n\n\nCode\nkde_df = pd.DataFrame({'Environmental sustainability index': kde.support, 'Density': kde.density})\nkde_df.head(3)\n\n\n\n\n\n\n  \n    \n      \n      Environmental sustainability index\n      Density\n    \n  \n  \n    \n      0\n      0.088614\n      0.000609\n    \n    \n      1\n      0.089334\n      0.000621\n    \n    \n      2\n      0.090054\n      0.000645"
  },
  {
    "objectID": "slides/week5-smoothing.html#other-kernels-1",
    "href": "slides/week5-smoothing.html#other-kernels-1",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Other kernels",
    "text": "Other kernels\n\n\nTitles indicate statsmodels.nonparametric.KDEUnivariate abbreviations\nNote different axis scales – not as similar as they look!"
  },
  {
    "objectID": "slides/week5-smoothing.html#exploration",
    "href": "slides/week5-smoothing.html#exploration",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Exploration",
    "text": "Exploration\nUse the activity notebook to experiment and answer the following.\n\nHow does the KDE differ if a parabolic (epa) kernel is used in place of a Gaussian (gau) kernel while the bandwidth is held constant?\nWhat effect does a triangular kernel (tri) have on how local peaks appear?\nPick two kernels. What will happen to the KDE for large bandwidths?\nWhich kernel seems to do the best job at capturing the shape closely without under-smoothing?"
  },
  {
    "objectID": "slides/week5-smoothing.html#multivariate-kde",
    "href": "slides/week5-smoothing.html#multivariate-kde",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Multivariate KDE",
    "text": "Multivariate KDE\nNow let’s estimate the joint distribution of environmental and economic sustainability indices. Here are the values:\n\n\nCode\nalt.Chart(\n    sust\n).mark_point().encode(\n    x = alt.X('Env_Domain', title = 'Environmental sustainability'),\n    y = alt.Y('Econ_Domain', title = 'Economic sustainability')\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#multivariate-histograms",
    "href": "slides/week5-smoothing.html#multivariate-histograms",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Multivariate histograms",
    "text": "Multivariate histograms\nThere are a few options for displaying a 2-D histogram. One is to bin and plot a heatmap, as we saw before:\n\n\nCode\nalt.Chart(\n    sust\n).mark_rect().encode(\n    x = alt.X('Env_Domain', bin = alt.Bin(maxbins = 40), title = 'Environmental sustainability'),\n    y = alt.Y('Econ_Domain', bin = alt.Bin(maxbins = 40), title = 'Economic sustainability'),\n    color = alt.Color('count()', scale = alt.Scale(scheme = 'bluepurple'), title = 'Number of U.S. cities')\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14, \n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#multivariate-histograms-1",
    "href": "slides/week5-smoothing.html#multivariate-histograms-1",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Multivariate histograms",
    "text": "Multivariate histograms\nAnother option is to make a bubble chart with the size of the bubble proportional to the count of observations in the corresponding bin:\n\n\nCode\nalt.Chart(\n    sust\n).mark_circle().encode(\n    x = alt.X('Env_Domain', bin = alt.Bin(maxbins = 40), title = 'Environmental sustainability'),\n    y = alt.Y('Econ_Domain', bin = alt.Bin(maxbins = 40), title = 'Economic sustainability'),\n    size = alt.Size('count()', scale = alt.Scale(scheme = 'bluepurple'))\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14, \n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#multivariate-kde-1",
    "href": "slides/week5-smoothing.html#multivariate-kde-1",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Multivariate KDE",
    "text": "Multivariate KDE\nThe following computes a multivariate Gaussian KDE\n\n# retrieve data as 2-d array (num observations, num variables)\nfit_ary = sust.loc[:, ['Env_Domain', 'Econ_Domain']].values\n\n# compute KDE\nkde = sm.nonparametric.KDEMultivariate(data = fit_ary, var_type = 'cc')\nkde\n\nKDE instance\nNumber of variables: k_vars = 2\nNumber of samples:   nobs = 933\nVariable types:      cc\nBW selection method: normal_reference\n\n\n\ninput: 2-D array \\((\\text{observations} \\times \\text{variables})\\)\nvariable type specification, one per variate: continuous, discrete, etc.\nmethod of bandwidth selection (not shown)"
  },
  {
    "objectID": "slides/week5-smoothing.html#prediction-grids",
    "href": "slides/week5-smoothing.html#prediction-grids",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Prediction grids",
    "text": "Prediction grids\nA common visualization strategy is to generate a prediction grid: a mesh of values spanning a domain of interest, for the purpose of computing a function at each grid point.\n\nHere is a 20 x 20 mesh:\n\n\nCode\n# resolution of grid (number of points to use along each axis)\ngrid_res = 20\n\n# find grid point coordinates along each axis\nx1 = np.linspace(start = sust.Env_Domain.min(), stop = sust.Env_Domain.max(), num = grid_res)\nx2 = np.linspace(start = sust.Econ_Domain.min(), stop = sust.Econ_Domain.max(), num = grid_res)\n\n# generate a mesh from the coordinates\ngrid1, grid2 = np.meshgrid(x1, x2, indexing = 'ij')\ngrid_ary = np.array([grid1.ravel(), grid2.ravel()]).T\n\n# plot grid points\nalt.Chart(\n    pd.DataFrame(grid_ary).rename(columns = {0: 'env', 1: 'econ'})\n).mark_point(color = 'black').encode(\n    x = alt.X('env', scale = alt.Scale(zero = False)),\n    y = alt.Y('econ', scale = alt.Scale(zero = False))\n).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#multivariate-kde-heatmap",
    "href": "slides/week5-smoothing.html#multivariate-kde-heatmap",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Multivariate KDE heatmap",
    "text": "Multivariate KDE heatmap\n\n\nCode\n# kde\nkde_smooth = alt.Chart(\n    grid_df, title = 'Gaussian KDE'\n).mark_rect(opacity = 0.8).encode(\n    x = alt.X('env', bin = alt.Bin(maxbins = grid_res), title = 'Environmental sustainability'),\n    y = alt.Y('econ', bin = alt.Bin(maxbins = grid_res), title = 'Economic sustainability'),\n    color = alt.Color('mean(density)', # a bit hacky, but works\n                      scale = alt.Scale(scheme = 'bluepurple', type = 'sqrt'),\n                      title = 'Density')\n)\n\n# histogram\nbubble = alt.Chart(\n    sust\n).mark_circle().encode(\n    x = alt.X('Env_Domain', bin = alt.Bin(maxbins = 40), title = 'Environmental sustainability'),\n    y = alt.Y('Econ_Domain', bin = alt.Bin(maxbins = 40), title = 'Economic sustainability'),\n    size = alt.Size('count()', scale = alt.Scale(scheme = 'bluepurple'), title = 'Cities')\n)\n\n# layer\n(bubble + kde_smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14, \n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\n\nDoes the KDE seem like a good estimate?\nWhat, if anything, does the graphic convey about the sustainability of cities?"
  },
  {
    "objectID": "slides/week5-smoothing.html#mixture-models",
    "href": "slides/week5-smoothing.html#mixture-models",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Mixture models",
    "text": "Mixture models\nKDE is a nonparametric technique: it involves no population parameters.\n\nMixture models are a parametric alternative for density estimation. The Gaussian mixture model is:\n\\[\nf(x) = a_1 \\varphi(x; \\mu_1, \\sigma_1) + \\cdots + a_n \\varphi(x; \\mu_n, \\sigma_n)\n\\]\n\n\\(f(x)\\) is the distribution of interest\n\\(\\varphi(\\cdot; \\mu, \\sigma)\\) denotes a Gaussian density with mean \\(\\mu\\) and standard deviation \\(\\sigma\\)\nthe model has \\(n\\) components\n\\(a_1, \\dots, a_n\\) are the mixing parameters\nfitted using the EM algorithm"
  },
  {
    "objectID": "slides/week5-smoothing.html#bivariate-mixture-model",
    "href": "slides/week5-smoothing.html#bivariate-mixture-model",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Bivariate mixture model",
    "text": "Bivariate mixture model\nLet’s fit a mixture to the joint distribution of environmental and economic sustainability indices:\n\n# configure and fit mixture model\ngmm = mixture.GaussianMixture(n_components = 2, covariance_type = 'full')\ngmm.fit(fit_ary)\n\n\nWe can inspect the estimated components’ centers (means):\n\n\nCode\ncenters = pd.DataFrame(gmm.means_).rename(columns = {0: 'env', 1: 'econ'})\ncenters\n\n\n\n\n\n\n  \n    \n      \n      env\n      econ\n    \n  \n  \n    \n      0\n      0.440006\n      0.461773\n    \n    \n      1\n      0.381889\n      0.478218\n    \n  \n\n\n\n\n\n\nAnd the mixing parameters:\n\n\nCode\nprint('mixing parameters', gmm.weights_)\n\n\nmixing parameters [0.7203574 0.2796426]"
  },
  {
    "objectID": "slides/week5-smoothing.html#bivariate-mixture-model-1",
    "href": "slides/week5-smoothing.html#bivariate-mixture-model-1",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Bivariate mixture model",
    "text": "Bivariate mixture model\n\n\nCode\n# evaluate log-likelihood\ngrid_df['gmm'] = np.exp(gmm.score_samples(grid_ary))\n\n# gmm\ngmm_smooth = alt.Chart(\n    grid_df, title = 'GMM'\n).mark_rect(opacity = 0.8).encode(\n    x = alt.X('env', bin = alt.Bin(maxbins = grid_res), title = 'Environmental sustainability'),\n    y = alt.Y('econ', bin = alt.Bin(maxbins = grid_res), title = 'Economic sustainability'),\n    color = alt.Color('mean(gmm)', # a bit hacky, but works\n                      scale = alt.Scale(scheme = 'bluepurple', type = 'sqrt'),\n                      title = 'Density')\n)\n\n# centers of mixture components\nctr = alt.Chart(centers).mark_point(color = 'black', shape = 'triangle').encode(x = 'env', y = 'econ')\n\n((bubble + gmm_smooth + ctr) | (bubble + kde_smooth)).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14, \n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\n\nWhat differences do you observe?\nWhich do you prefer and why?"
  },
  {
    "objectID": "slides/week5-smoothing.html#pros-and-cons",
    "href": "slides/week5-smoothing.html#pros-and-cons",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nKDE is a nonparametric method and is sometimes called a memory-based procedure: it uses all available data every time an estimated value is calculated.\n\nAdvantages: minimal assumptions, highly flexible\nDisadvantages: not parsimonious, computationally intensive\n\n\nGMM’s are parametric models.\n\nAdvantages: closed-form structure, good for multimodal distributions, various kinds of inference and prediction are possible\nDisadvantages: estimation is less straightforward, may over-smooth, possible identifiability issues"
  },
  {
    "objectID": "slides/week5-smoothing.html#gmms-for-capturing-subpopulations",
    "href": "slides/week5-smoothing.html#gmms-for-capturing-subpopulations",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "GMMs for capturing subpopulations",
    "text": "GMMs for capturing subpopulations\nThe GMM is especially useful if there are latent subpopulations in the data. Recall the simulated population of hawks:\n\n\nCode\n# for reproducibility\nnp.random.seed(40721)\n\n# simulate hypothetical population\nfemale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 57.5, scale = 3, size = 3000),\n            'sex': np.repeat('female', 3000)}\n)\n\nmale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 50.5, scale = 3, size = 2000),\n            'sex': np.repeat('male', 2000)}\n)\n\npopulation_hawks = pd.concat([female_hawks, male_hawks], axis = 0)\n\npopulation_hawks.groupby('sex').head(1)\n\n\n\n\n\n\n  \n    \n      \n      length\n      sex\n    \n  \n  \n    \n      0\n      53.975230\n      female\n    \n    \n      0\n      53.076663\n      male\n    \n  \n\n\n\n\n\nLet’s imagine we have a sample but without recorded sexes.\n\n\nCode\nnp.random.seed(50223)\nsamp = population_hawks.sample(n = 500).drop(columns = 'sex')\nsamp.head(3)\n\n\n\n\n\n\n  \n    \n      \n      length\n    \n  \n  \n    \n      840\n      62.513960\n    \n    \n      2747\n      52.490258\n    \n    \n      2698\n      56.879861"
  },
  {
    "objectID": "slides/week5-smoothing.html#hawks",
    "href": "slides/week5-smoothing.html#hawks",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Hawks",
    "text": "Hawks\nThe histogram is not obviously bimodal, but we can suppose we’re aware of the sex differences in length and just don’t have that information.\n\n\nCode\nhist_hawks = alt.Chart(samp).transform_bin(\n    'length',\n    field = 'length',\n    bin = alt.Bin(step = 2)\n).transform_aggregate(\n    count = 'count()',\n    groupby = ['length']\n).transform_calculate(\n    density = 'datum.count/1000',\n    length = 'datum.length + 1'\n).mark_bar(size = 20).encode(\n    x = 'length:Q', \n    y = 'density:Q'\n)\n\nhist_hawks.configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#recovering-subpopulations",
    "href": "slides/week5-smoothing.html#recovering-subpopulations",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Recovering subpopulations",
    "text": "Recovering subpopulations\nIf we fit a mixture model with two components, amazingly, the mixture components accurately recover the distributions for each sex, even though this was a latent (unobserved) variable:\n\n# configure and fit mixture model\ngmm_hawks = mixture.GaussianMixture(n_components = 2)\ngmm_hawks.fit(samp.length.values.reshape(-1, 1))\n\n# compare components with subpopulations\nprint('gmm component means: ', gmm_hawks.means_.ravel())\nprint('population means by sex: ', np.sort(population_hawks.groupby('sex').mean().values.ravel()))\n\nprint('gmm component variances: ', gmm_hawks.covariances_.ravel())\nprint('population variances by sex: ', np.sort(population_hawks.groupby('sex').var().values.ravel()))\n\ngmm component means:  [50.70543698 57.33071086]\npopulation means by sex:  [50.48194081 57.5749014 ]\ngmm component variances:  [ 8.60692088 10.12549367]\npopulation variances by sex:  [8.522606   9.39645762]\n\n\n\nNote that the means and variances are estimated from the sample but compared with the population values above."
  },
  {
    "objectID": "slides/week5-smoothing.html#gmm-density-estimate",
    "href": "slides/week5-smoothing.html#gmm-density-estimate",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "GMM density estimate",
    "text": "GMM density estimate\nFurther, the density estimate fits reasonably well:\n\n\nCode\n# compute a grid of lengths\ngrid_hawks = np.linspace(population_hawks.length.min(), population_hawks.length.max(), num = 500)\ndens = np.exp(gmm_hawks.score_samples(grid_hawks.reshape(-1, 1)))\n\ngmm_smooth_hawks = alt.Chart(\n    pd.DataFrame({'length': grid_hawks, 'density': dens})\n).mark_line(color = 'black').encode(\n    x = 'length',\n    y = 'density'\n)\n\n(hist_hawks + gmm_smooth_hawks).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#explore",
    "href": "slides/week5-smoothing.html#explore",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Explore",
    "text": "Explore\nTake 5-6 minutes with your neighbor and use the activity notebook to experiment and answer the following.\n\nHow does the KDE differ if a parabolic (epa) kernel is used in place of a Gaussian (gau) kernel while the bandwidth is held constant?\nWhat effect does a triangular kernel (tri) have on how local peaks appear?\nPick two kernels. What will happen to the KDE for large bandwidths?\nWhich kernel seems to do the best job at capturing the shape closely without under-smoothing?"
  },
  {
    "objectID": "slides/week5-smoothing.html#kernel-smoothing",
    "href": "slides/week5-smoothing.html#kernel-smoothing",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Kernel smoothing",
    "text": "Kernel smoothing\nKernel smoothing is a technique similar to KDE used to visualize and estimate relationships (rather than distributions).\n\nWe’ll use the GDP and life expectancy data from lab 3 to illustrate.\n\n\nCode\n# read in data and subset\nlife = pd.read_csv('data/life-gdp.csv')\nlife = life[life.Year == 2000].loc[:, ['Country Name', 'All', 'GDP per capita']]\nlife['GDP per capita'] = np.log(life['GDP per capita'])\nlife.rename(columns = {'GDP per capita': 'log_gdp', 'All': 'life_exp', 'Country Name': 'country'}, inplace=True)\n\n# scatterplot\nscatter = alt.Chart(\n    life\n).mark_point().encode(\n    x = alt.X('log_gdp', scale = alt.Scale(zero = False), title = 'log(GDP/capita)'),\n    y = alt.Y('life_exp', scale = alt.Scale(zero = False), title = 'Life expectancy')\n)\n\nscatter.configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#the-method",
    "href": "slides/week5-smoothing.html#the-method",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "The method",
    "text": "The method\nThe technique consists in estimating trend by local weighted averaging; a kernel function is used to determine the exact weights.\n\\[\n\\hat{y}(x) = \\frac{\\sum_i K_b (x_i - x) y_i}{\\sum_i K_b(x_i - x)}\n\\]\n\n\n\n\nIllustration of kernel smoothing"
  },
  {
    "objectID": "slides/week5-smoothing.html#example",
    "href": "slides/week5-smoothing.html#example",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Example",
    "text": "Example\n\n\nCode\n# calculate kernel smoother\nkernreg = sm.nonparametric.KernelReg(endog = life.life_exp.values,\n                                     exog = life.log_gdp.values,\n                                     reg_type = 'lc', \n                                     var_type = 'c',\n                                     ckertype = 'gaussian')\n\n# grid of gdp values\ngdp_grid = np.linspace(life.log_gdp.min(), life.log_gdp.max(), num = 100)\n\n# calculate kernel smooth at each value\nfitted_values = kernreg.fit(gdp_grid)[0]\n\n# arrange as data frame\npred_df = pd.DataFrame({'log_gdp': gdp_grid, 'life_exp': fitted_values})\n\n# plot\nkernel_smooth = alt.Chart(\n    pred_df\n).mark_line(\n    color = 'black'\n).encode(\n    x = 'log_gdp', \n    y = alt.Y('life_exp', scale = alt.Scale(zero = False))\n)\n\n(scatter + kernel_smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)\n\n\n\n\n\n\n\n\nNotice that the kernel smooth trails off a bit near the boundary – this is because fewer points are being averaged once the smoothing window begins to extend beyond the range of the data."
  },
  {
    "objectID": "slides/week5-smoothing.html#loess",
    "href": "slides/week5-smoothing.html#loess",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "LOESS",
    "text": "LOESS\nLocally weighted scatterplot smoothing (LOESS or LOWESS) largely corrects this issue by stitching together lines (or low-order polynomials) fitted to local subsets of data; this way, near the boundary, the estimate still takes account of any trend present in the data."
  },
  {
    "objectID": "slides/week5-smoothing.html#loess-example",
    "href": "slides/week5-smoothing.html#loess-example",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "LOESS example",
    "text": "LOESS example\n\n\nCode\n# fit loess smooth\nloess = sm.nonparametric.lowess(endog = life.life_exp.values,\n                                exog = life.log_gdp.values, \n                                frac = 0.3,\n                                xvals = gdp_grid)\n\n# store as data frame\nloess_df = pd.DataFrame({'log_gdp': gdp_grid, 'life_exp': loess})\n\n# plot\nloess_smooth = alt.Chart(\n    loess_df\n).mark_line(\n    color = 'blue',\n    strokeDash = [8, 8]\n).encode(\n    x = 'log_gdp', \n    y = alt.Y('life_exp', scale = alt.Scale(zero = False))\n)\n\n(scatter + loess_smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#comparison",
    "href": "slides/week5-smoothing.html#comparison",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Comparison",
    "text": "Comparison\nIf we compare the LOESS curve with the kernel smoother, the different behavior on the boundary is evident:\n\n\nCode\n(scatter + loess_smooth + kernel_smooth).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "slides/week5-smoothing.html#explore-1",
    "href": "slides/week5-smoothing.html#explore-1",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Explore",
    "text": "Explore\nTake 2 minutes with your neighbor and use the activity notebook to explore the following questions.\n\nWhat happens if you fit the GMM with different numbers of components?\nDoes the solution change if the GMM is re-fitted?"
  },
  {
    "objectID": "slides/week5-smoothing.html#outline-for-today",
    "href": "slides/week5-smoothing.html#outline-for-today",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Outline for today",
    "text": "Outline for today\n\nMore on density estimation\n\nnon-Gaussian kernels\nmultivariate KDE\nmixture models\n\nScatterplot smoothing\n\nKernel smoothing\nLOESS"
  },
  {
    "objectID": "slides/week5-smoothing.html#kdes-with-statsmodels",
    "href": "slides/week5-smoothing.html#kdes-with-statsmodels",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "KDEs with statsmodels",
    "text": "KDEs with statsmodels\nThe statsmodels package has KDE implementations with finer control. This will allow us to experiment with other kernels.\n\n# compute the gaussian KDE using statsmodels\nkde = sm.nonparametric.KDEUnivariate(sust.Env_Domain)\nkde.fit(bw = bw_scott)\n\n<statsmodels.nonparametric.kde.KDEUnivariate at 0x1a9004f34c0>\n\n\n\nThe object kde has .support (\\(x\\)) and .density (\\(\\hat{f}(x)\\)) attributes:\n\n\nCode\nkde_df = pd.DataFrame({\n    'Environmental sustainability index': kde.support, \n    'Density': kde.density\n    })\nkde_df.head(3)\n\n\n\n\n\n\n  \n    \n      \n      Environmental sustainability index\n      Density\n    \n  \n  \n    \n      0\n      0.088614\n      0.000609\n    \n    \n      1\n      0.089334\n      0.000621\n    \n    \n      2\n      0.090054\n      0.000645"
  },
  {
    "objectID": "slides/week5-smoothing.html#prediction-grids-1",
    "href": "slides/week5-smoothing.html#prediction-grids-1",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Prediction grids",
    "text": "Prediction grids\nWe’ll use a 100 x 100 mesh and kde.pdf() to compute the estimated density at each grid point:\n\n\nCode\n# resolution of grid (number of points to use along each axis)\ngrid_res = 100\n\n# find grid point coordinates along each axis\nx1 = np.linspace(start = sust.Env_Domain.min(), stop = sust.Env_Domain.max(), num = grid_res)\nx2 = np.linspace(start = sust.Econ_Domain.min(), stop = sust.Econ_Domain.max(), num = grid_res)\n\n# generate a mesh from the coordinates\ngrid1, grid2 = np.meshgrid(x1, x2, indexing = 'ij')\ngrid_ary = np.array([grid1.ravel(), grid2.ravel()]).T\n\n# compute the density at each grid point\nf_hat = kde.pdf(grid_ary)\n\n# rearrange as a dataframe\ngrid_df = pd.DataFrame({'env': grid1.reshape(grid_res**2), \n                        'econ': grid2.reshape(grid_res**2),\n                        'density': f_hat})\n\n# preview, for understanding\ngrid_df.head()\n\n\n\n\n\n\n  \n    \n      \n      env\n      econ\n      density\n    \n  \n  \n    \n      0\n      0.132467\n      0.143811\n      1.325548e-17\n    \n    \n      1\n      0.132467\n      0.150278\n      6.367762e-17\n    \n    \n      2\n      0.132467\n      0.156745\n      2.929104e-16\n    \n    \n      3\n      0.132467\n      0.163212\n      1.290785e-15\n    \n    \n      4\n      0.132467\n      0.169679\n      5.451826e-15"
  },
  {
    "objectID": "slides/week5-smoothing.html#explore-2",
    "href": "slides/week5-smoothing.html#explore-2",
    "title": "Density estimation, mixture models, and smoothing",
    "section": "Explore",
    "text": "Explore\nTake 2-3 minutes with your neighbor and use the activity notebook to answer the following questions.\n\nAre there any bandwidths that give you a straight-line fit?\nWhat seems to be the minimum bandwidth?\nWhich bandwidth best captures the pattern of scatter?"
  }
]