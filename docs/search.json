[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Instructor: Trevor Ruiz\nTeaching assistants: Mengye Liu, Harry Yu, Gabrielle Salo\nClass meetings: M-W 12:30pm – 1:45pm Buchanan 1920\nSection meetings:\n\nM 2:00pm – 2:50pm Phelps 1525 (Mengye)\nM 3:00pm – 3:50pm Phelps 1525 (Mengye)\nM 4:00pm – 4:50pm Phelps 1513 (Harry)\nM 5:00pm – 5:50pm Phelps 1525 (Harry)\nM 6:00pm – 6:50pm Phelps 1525 (Gabrielle)\n\nOffice hours: TBD"
  },
  {
    "objectID": "about.html#content-and-materials",
    "href": "about.html#content-and-materials",
    "title": "Course syllabus",
    "section": "Content and materials",
    "text": "Content and materials\nData Science Concepts and Analysis (PSTAT100) is a hands-on introduction to data science intended for intermediate-level students from any discipline with some exposure to probability and basic computing skills, but few or no upper-division courses in statistics or computer science. The course introduces central concepts in statistics – such as sampling variation, uncertainty, and inference – in an applied setting together with techniques for data exploration and analysis. Applications emphasize end-to-end data analyses. Course activities model standard data science workflow practices by example, and successful students acquire programming skills, project management skills, and subject exposure that will serve them well in upper-division courses as well as in independent research or projects.\n\nCatalog description\nOverview of data science key concepts and the use of tools for data retrieval, analysis, visualization, and reproducible research. Topics include an introduction to inference and prediction, principles of measurement, missing data, and notions of causality, statistical “traps”, and concepts in data ethics and privacy. Case studies will illustrate the importance of domain knowledge. Credit units: 4.\n\n\nPrerequisites\n\nProbability and Statistics I (PSTAT 120A)\nLinear Algebra (MATH 4A)\nPrior experience with Python or another programming language (CMPSC 9 or CMPSC 16).\n\n\n\nLearning outcomes\nSuccessful students will establish foundational data science skills:\n\ncritical assessment of data quality and sampling design\nretrieval, inspection, and cleaning of raw data\nexploratory, descriptive, visual, and inferential techniques\ninterpretation and communication of results in context\n\nThese skills will be discussed in depth during course lectures; students will practice them through lab activities, homework assignments, and project work.\n\n\nAssessments\nAttainment of course learning outcomes will be measured by assessment of submitted work. Submitted work falls into four categories:\n\nLabs will be assigned weekly in most weeks. These are structured coding assignments with small exercises throughout that introduce the programming skills needed to complete homework assignments.\nHomeworks will be assigned biweekly. These are fairly involved assignments which apply concepts and techniques from the lectures and programming skills from the labs to real data sets in order to reproduce an analysis and answer substantive questions. Collaboration is encouraged, and group submissions will be allowed for groups of up to 3 students.\nMini projects will be assigned biweekly in alternation with homeworks. These assignments prompt students to use skills from the course in an unstructured setting to answer high-level questions pertaining to one or more datasets. Mini projects should be completed collaboratively.\nA course project will be assigned requiring students to carry out an open-ended data analysis. This will be completed in teams. Each team will prepare a project plan for initial feedback a few weeks before the end of the quarter, and submit a final report of work and findings by the end of the quarter.\n\nOverall scores in the course will be calculated for each student as the weighted average of scores on all submitted work; the relative weighting and letter grade assignments will depend entirely on the score distribution of the class as a whole and as such reflect each student’s performance relative to their peers.\n\n\nSchedule\nThe tentative topic and assignment schedule is given below. Assignments are indicated by due date: all assignments are due by Monday 11:59pm in the week indicated. Late submissions are allowed, with a possible penalty, for up to 48 hours.\nThe schedule is subject to change based on the progress of the class.\n\n\n\nWeek\nTopic\nLab\nHomework\nProject\n\n\n\n\n1\nData science life cycle\n\n\n\n\n\n2\nTidy data\nL0\n\n\n\n\n3\nSampling and bias\nL1\n\n\n\n\n4\nStatistical graphics\nL2\nH1\n\n\n\n5\nKernel density estimation\nL3\n\nMP1\n\n\n6\nPrincipal components\nL4\nH2\n\n\n\n7\nSimple regression\n\n\nMP2\n\n\n8\nMultiple regression\nL5\nH3\n\n\n\n9\nClassification and clustering\n\n\nCP1\n\n\n10\nCase study\n\nH4\n\n\n\n11\nFinals\n\n\nCP2\n\n\n\n\nL: lab\nH: homework\nMP: mini project\nCP: course project\n\n\n\nMaterials\nThe course website pstat100.github.io will link to all course content and resources. Readings for the course will draw on multiple sources, including:\n\nPython Data Science Handbook (PDSH);\nLearning Data Science (LDS);\ncollected articles distributed as assigned.\n\n\n\nSoftware\nComputing will be hosted via the course LSIT server pstat100.lsit.ucsb.edu. Students need only a web browser and stable internet connection to complete all course work. It is strongly recommended that students download backup copies of their assignments from the LSIT server.\nInterested students are encouraged to install the software needed to open, edit, and execute notebooks on their own machine, in particular:\n\na Python install;\n(recommended) Miniconda\nJupyter;\npackages utilized in course materials (primarily numpy, pandas, altair, and scikit-learn).\n\nManaging package installations will require some (straightforward) use of the package installer pip or pip3 in the terminal to retrieve/install packages from the Python Package Index repository. Documentation for specific packages (or a Google search) will indicate the appropriate pip command."
  },
  {
    "objectID": "about.html#policies",
    "href": "about.html#policies",
    "title": "Course syllabus",
    "section": "Policies",
    "text": "Policies\n\nCommunication\nThere are two primary means of communication outside of scheduled class meetings: office hours and a discussion board.\nCourse staff have limited availability via email. Course staff will make every effort to respond to individual communication within 48 weekday-hours on the following (or similar) matters:\n\naccommodations/extensions due to personal circumstances;\nlogistical issues such as access to materials or missing scores;\ngeneral advising.\n\nEmail should not be used to ask content questions or submit assignments (unless specifically requested). Emails related to the following (or similar) matters may not receive replies and should be redirected:\n\n\n\nTopic\nRedirect to…\n\n\n\n\nTroubleshooting codes\nDiscussion board\n\n\nChecking answers\nOffice hours or discussion board\n\n\nClarifying assignment content\nOffice hours or discussion board\n\n\nAssignment submission\nGradescope\n\n\nRe-evaluation request\nGradescope\n\n\n\n\n\nExpected time commitment\nThe course is 4 credit units; each credit unit corresponds to an approximate time commitment of 3 hours. So, students should expect to allocate 12 hours per week to the course on average. Course staff are available to help any students spending considerably more time on the class balance the workload.\n\n\nScores and grades\nScores on submitted work can be monitored on Gradescope to ensure fair assignment of course grades. On any individual assignment, re-evaluation can be requested within one week of receiving a score. Requests for re-evaluation made beyond one week after publication of scores may or may not be considered on a discretionary basis.\nDetermination of letter grade assignments is made entirely at the discretion of the instructor based on the assessments outlined above and consistent with university policy. Students are not permitted to negotiate their grades, and are discouraged from requesting audits, recalculations, or verification of self-calculations after the course has concluded. The instructor is under no obligation to share the details of grade calculations with students or to respond to such requests.\nIf at the end of the course a student believes their grade was unfairly assigned, either due to discrimination or without basis in coursework, they are entitled to contest it according to the procedure outlined here.\n\n\nConduct\nStudents are expected to uphold the student code of conduct and to maintain integrity. All individually-submitted work must be an honest reflection of individual effort. Evidence of dishonest conduct will be discussed with the student(s) involved and reported to the Office of Student Conduct (OSC). Depending on the nature of the evidence and the violation, penalty in the course may range from a warning to loss of credit to automatic failure. For a definition and examples of dishonesty, a discussion of what constitutes an appropriate response from faculty, and an explanation of the reporting and investigation process, see the OSC page on academic integrity.\n\n\nDeadlines and late work\nThere is a one-hour grace period on all submission deadlines. After that, work may be submitted within 48 hours of the original deadline (not the deadline plus grace period) and will be considered late. Every student can submit two late assignments without penalty. Subsequent late submissions will be evaluated for 75% credit.\n\n\nAccommodations\nReasonable accommodations will be made for any student with a qualifying disability. Such requests should be made through the Disabled Students Program (DSP). More information, instructions on how to access accommodations, and information on related resources can be found on the DSP website.\n\n\nFeedback\nToward the end of the term students will be given an opportunity to provide feedback about the course via ESCI. This feedback is valuable for improvement of the course in future terms, and students are strongly encouraged to provide thoughtful course evaluations. The identities of student respondents to ESCI surveys are not disclosed to instructors."
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Materials",
    "section": "",
    "text": "Attendance form (fill out once per class meeting, including sections)\nGradescope (for assignment submissions)\nLSIT server (for course computing)\nDiscussion board (check back soon)"
  },
  {
    "objectID": "content.html#week-1",
    "href": "content.html#week-1",
    "title": "Materials",
    "section": "Week 1",
    "text": "Week 1\nReadings:\n\nLDS1 The Data Science Lifecycle\nLDS5 Case Study: Why is my Bus Always Late?\nPDSH2.1 Understanding data types in python\nPDSH2.2 The basics of numpy arrays\nPDSH2.4 Aggregations: min, max, and everything in between\n\nMonday: Course introduction [slides]\nLab sections: Orientation to Jupyter notebooks [activity] [notebook]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Concepts and Analysis",
    "section": "",
    "text": "This is the course website for UCSB’s Data Science Concepts and Analysis class (PSTAT100). Content is directed towards currently enrolled students. Please ask permission before using course materials in any other capacity."
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab0-gettingstarted.ipynb\")"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#jupyter-notebooks",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#jupyter-notebooks",
    "title": "PSTAT100",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nJupyter notebooks are organized into ‘cells’ that can contain either text or codes. For example, this is a text cell.\nTechnically, Jupyter is an application/interface that runs atop a kernel – a programming-language-specific independent environment in which code cells are executed. This basic organization allows for interactive computing with text integration.\nSelecting a cell and pressing Enter will enter edit mode and allow you to edit the cell. From edit mode, pressing Esc will revert to command mode and allow you to navigate the notebook’s cells.\nIn edit mode, most of the keyboard is dedicated to typing into the cell’s editor. Thus, in edit mode there are relatively few shortcuts. In command mode, the entire keyboard is available for shortcuts, so there are many more. Here are a few useful ones:\n\nCtrl + Return : Evaluate the current cell\nShift + Return: Evaluate the current cell and move to the next\nSaving the notebook: s\nBasic navigation: up one cell k, down one cell j\na : create a cell above\nb : create a cell below\ndd : delete a cell\nz : undo the last cell operation\nm : convert a cell to markdown\ny : convert a cell to code\n\nTake a moment to find out what the following commands do:\n\nCell editing: x, c, v, z\nKernel operations: i, 0 (press twice)\n\n\n# Practice the above commands on this cell\n\n\nRunning Cells and Displaying Output\nRun the following cell.\n\nprint(\"Hello, World!\")\n\nHello, World!\n\n\nIn Jupyter notebooks, all print statements are displayed below the cell. Furthermore, the output of only the last line is displayed following the cell upon execution.\n\n\"Will this line be displayed?\"\n\nprint(\"Hello\" + \",\", \"world!\")\n\n5 + 3\n\nHello, world!\n\n\n8\n\n\n\n\nViewing Documentation\nTo output the documentation for a function, use the help() function.\n\nhelp(print)\n\nHelp on built-in function print in module builtins:\n\nprint(...)\n    print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\n    \n    Prints the values to a stream, or to sys.stdout by default.\n    Optional keyword arguments:\n    file:  a file-like object (stream); defaults to the current sys.stdout.\n    sep:   string inserted between values, default a space.\n    end:   string appended after the last value, default a newline.\n    flush: whether to forcibly flush the stream.\n\n\n\nYou can also use Jupyter to view function documentation inside your notebook. The function must already be defined in the kernel for this to work.\nBelow, click your mouse anywhere on print() and use Shift + Tab to view the function’s documentation.\n\nprint('Welcome to this course!')\n\nWelcome to this course!\n\n\n\n\nImporting Libraries\nIn this course, we will be using common Python libraries to help us retrieve, manipulate, and perform operations on data. By convention, we import all libraries at the very top of the notebook. There are also a set of standard aliases that are used to shorten the library names. Below are some of the libraries that you may encounter throughout the course, along with their respective aliases.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#practice-questions-and-numpy-review",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#practice-questions-and-numpy-review",
    "title": "PSTAT100",
    "section": "Practice questions and numpy review",
    "text": "Practice questions and numpy review\nMost assignments for this class will be given as notebooks organized into explanation and prompts followed by response cells; you will complete assignments by filling in all of the response cells.\nMany response cells are followed by a test cell that performs a few checks on your work. Please be aware that test cells don’t always confirm that your response is correct or incorrect. They are meant to give you some useful feedback, but it’s your responsibility to interpret the feedback – please be sure to read and think about test output if tests fail, and make your own assessment of whether you need to revise your response.\nBelow are a few practice questions for you to familiarize yourself with the process. These assume familiarity with basic python syntax and the numpy package.\n\nQuestion 1\nWrite a function summation that evaluates the following summation for \\(n \\geq 1\\):\n\\[\\sum_{i=1}^{n} \\left(i^3 + 5 i^3\\right)\\]\nHint: np.arange(5).sum() will generate an array comprising \\(1, 2, \\dots, 5\\) and then add up the elements of the array.\n\ndef summation(n):\n    \"\"\"Compute the summation i^3 + 5 * i^3 for 1 <= i <= n.\"\"\"\n    # BEGIN SOLUTION\n    out = (6*(np.arange(n + 1)**3)).sum()\n    return out\n    # END SOLUTION\n\n\ngrader.check(\"q1\")\n\nUse your function to compute the sum for…\n\n# n = 2\n...\n\n\n# n = 20\n...\n\n\n\nQuestion 2\nThe core of numpy is the array. Let’s use np.array to create an array. It takes a sequence, such as a list or range (remember that list elements are included between the square brackets [ and ], such as [1, 5, 3]).\nBelow, create an array containing the values 1, 2, 3, 4, and 5 (in that order) and assign it the name my_array.\n\nmy_array = np.array([1, 2, 3, 4, 5]) #SOLUTION\n\n\ngrader.check(\"q2\")\n\nNumpy arrays are integer-indexed by position, with the first element indexed as position 0. Elements can be retrieved by enclosing the desired positions in brackets [].\n\nmy_array[3]\n\n4\n\n\nTo retrieve consecutive positions, specify the starting index and the ending index separated by :, for instance, arr[from:to]. This syntax is non-inclusive of the left endpoint, meaning that the starting index is not included in the output.\n\nmy_array[2:4]\n\narray([3, 4])\n\n\nIn addition to values in the array, we can access attributes such as array’s shape and data type that can be retrieved by name using syntax of the form array.attr. Some useful attributes are:\n\n.shape, a tuple with the length of each array dimension\n.size, the length of the first array dimension\n.dtype, the data type of the entries (float, integer, etc.)\n\nA full list of attributes is here.\n\nmy_array.shape\n\n(5,)\n\n\n\nmy_array.size\n\n5\n\n\n\nmy_array.dtype\n\ndtype('int32')\n\n\nArrays, unlike Python lists, cannot store items of different data types.\n\n# A regular Python list can store items of different data types\n[1, '3']\n\n[1, '3']\n\n\n\n# Arrays will convert everything to the same data type\nnp.array([1, '3'])\n\narray(['1', '3'], dtype='<U11')\n\n\n\n# Another example of array type conversion\nnp.array([5, 8.3])\n\narray([5. , 8.3])\n\n\nArrays are also useful in performing vectorized operations. Given two or more arrays of equal length, arithmetic will perform element-wise computations across the arrays.\nFor example, observe the following:\n\n# Python list addition will concatenate the two lists\n[1, 2, 3] + [4, 5, 6]\n\n[1, 2, 3, 4, 5, 6]\n\n\n\n# NumPy array addition will add them element-wise\nnp.array([1, 2, 3]) + np.array([4, 5, 6])\n\narray([5, 7, 9])\n\n\nArrays can be subsetted by index position, as shown above, or by a logical vector of the same length. For example:\n\nexample_arr = np.arange(4, 10)\nexample_arr\n\narray([4, 5, 6, 7, 8, 9])\n\n\nSuppose we want the last three elements. One option is to use index position:\n\nexample_arr[3:6]\n\narray([7, 8, 9])\n\n\nOr a logical vector:\n\nexample_arr[np.array([False, False, False, True, True, True])]\n\narray([7, 8, 9])\n\n\nThe latter approach allows one to subset based on a condition defined by the values of the vector. For example, we can use the condition \\(x \\geq 7\\) to obtain the logical vector used above.\n\nexample_arr >= 7\n\narray([False, False, False,  True,  True,  True])\n\n\nAnd then we can subset just as before:\n\nexample_arr[example_arr >= 7]\n\narray([7, 8, 9])\n\n\nYou’ll see this done frequently, and it’s sometimes referred to as filtering, because we’re selectively removing values.\n\n\nQuestion 3\nGiven the array random_arr, create an array containing all values \\(x\\) such that \\(2x^4 > 1\\). Name the array valid_values.\n\n# for reproducibility - setting the seed will result in the same random draw each time\nnp.random.seed(42)\n\n# draw 60 uniformly random integers between 0 and 1\nrandom_arr = np.random.rand(60)\n\n# solution here\nvalid_values = random_arr[2*(random_arr**4) > 1] # SOLUTION\n\n\ngrader.check(\"q3\")\n\n\n\nA note on np.arange and np.linspace\nUsually we use np.arange to return an array that steps from a to b with a fixed step size s. While this is fine in some cases, we sometimes prefer to use np.linspace(a, b, N), which divides the interval [a, b] into N equally spaced points.\nnp.arange(start, stop, step) produces an array with all the numbers starting at start, incremendted up by step, stopping before stop is reached. For example, the value of np.arange(1, 6, 2) is an array with elements 1, 3, and 5 – it starts at 1 and counts up by 2, then stops before 6. np.arange(4, 9, 1) is an array with elements 4, 5, 6, 7, and 8. (It doesn’t contain 9 because np.arange stops before the stop value is reached.)\nnp.linspace always includes both end points while np.arange will not include the second end point b. For this reason, especially when we are plotting ranges of values we tend to prefer np.linspace.\nNotice how the following two statements have different parameters but return the same result.\n\nnp.arange(-5, 6, 1.0)\n\narray([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])\n\n\n\nnp.linspace(-5, 5, 11)\n\narray([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])\n\n\nCheck your understanding. Will np.arange(1, 10) produce an array that contains 10? Add a cell below and check to confirm your answer."
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab0-gettingstarted.ipynb\")"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html#jupyter-notebooks",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html#jupyter-notebooks",
    "title": "PSTAT100",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nJupyter notebooks are organized into ‘cells’ that can contain either text or codes. For example, this is a text cell.\nTechnically, Jupyter is an application/interface that runs atop a kernel – a programming-language-specific independent environment in which code cells are executed. This basic organization allows for interactive computing with text integration.\nSelecting a cell and pressing Enter will enter edit mode and allow you to edit the cell. From edit mode, pressing Esc will revert to command mode and allow you to navigate the notebook’s cells.\nIn edit mode, most of the keyboard is dedicated to typing into the cell’s editor. Thus, in edit mode there are relatively few shortcuts. In command mode, the entire keyboard is available for shortcuts, so there are many more. Here are a few useful ones:\n\nCtrl + Return : Evaluate the current cell\nShift + Return: Evaluate the current cell and move to the next\nSaving the notebook: s\nBasic navigation: up one cell k, down one cell j\na : create a cell above\nb : create a cell below\ndd : delete a cell\nz : undo the last cell operation\nm : convert a cell to markdown\ny : convert a cell to code\n\nTake a moment to find out what the following commands do:\n\nCell editing: x, c, v, z\nKernel operations: i, 0 (press twice)\n\n\n# Practice the above commands on this cell\n\n\nRunning Cells and Displaying Output\nRun the following cell.\n\nprint(\"Hello, World!\")\n\nIn Jupyter notebooks, all print statements are displayed below the cell. Furthermore, the output of only the last line is displayed following the cell upon execution.\n\n\"Will this line be displayed?\"\n\nprint(\"Hello\" + \",\", \"world!\")\n\n5 + 3\n\n\n\nViewing Documentation\nTo output the documentation for a function, use the help() function.\n\nhelp(print)\n\nYou can also use Jupyter to view function documentation inside your notebook. The function must already be defined in the kernel for this to work.\nBelow, click your mouse anywhere on print() and use Shift + Tab to view the function’s documentation.\n\nprint('Welcome to this course!')\n\n\n\nImporting Libraries\nIn this course, we will be using common Python libraries to help us retrieve, manipulate, and perform operations on data. By convention, we import all libraries at the very top of the notebook. There are also a set of standard aliases that are used to shorten the library names. Below are some of the libraries that you may encounter throughout the course, along with their respective aliases.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html#practice-questions-and-numpy-review",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html#practice-questions-and-numpy-review",
    "title": "PSTAT100",
    "section": "Practice questions and numpy review",
    "text": "Practice questions and numpy review\nMost assignments for this class will be given as notebooks organized into explanation and prompts followed by response cells; you will complete assignments by filling in all of the response cells.\nMany response cells are followed by a test cell that performs a few checks on your work. Please be aware that test cells don’t always confirm that your response is correct or incorrect. They are meant to give you some useful feedback, but it’s your responsibility to interpret the feedback – please be sure to read and think about test output if tests fail, and make your own assessment of whether you need to revise your response.\nBelow are a few practice questions for you to familiarize yourself with the process. These assume familiarity with basic python syntax and the numpy package.\n\nQuestion 1\nWrite a function summation that evaluates the following summation for \\(n \\geq 1\\):\n\\[\\sum_{i=1}^{n} \\left(i^3 + 5 i^3\\right)\\]\nHint: np.arange(5).sum() will generate an array comprising \\(1, 2, \\dots, 5\\) and then add up the elements of the array.\n\ndef summation(n):\n    \"\"\"Compute the summation i^3 + 5 * i^3 for 1 <= i <= n.\"\"\"\n    ...\n\n\ngrader.check(\"q1\")\n\nUse your function to compute the sum for…\n\n# n = 2\n...\n\n\n# n = 20\n...\n\n\n\nQuestion 2\nThe core of numpy is the array. Let’s use np.array to create an array. It takes a sequence, such as a list or range (remember that list elements are included between the square brackets [ and ], such as [1, 5, 3]).\nBelow, create an array containing the values 1, 2, 3, 4, and 5 (in that order) and assign it the name my_array.\n\nmy_array = ...\n\n\ngrader.check(\"q2\")\n\nNumpy arrays are integer-indexed by position, with the first element indexed as position 0. Elements can be retrieved by enclosing the desired positions in brackets [].\n\nmy_array[3]\n\nTo retrieve consecutive positions, specify the starting index and the ending index separated by :, for instance, arr[from:to]. This syntax is non-inclusive of the left endpoint, meaning that the starting index is not included in the output.\n\nmy_array[2:4]\n\nIn addition to values in the array, we can access attributes such as array’s shape and data type that can be retrieved by name using syntax of the form array.attr. Some useful attributes are:\n\n.shape, a tuple with the length of each array dimension\n.size, the length of the first array dimension\n.dtype, the data type of the entries (float, integer, etc.)\n\nA full list of attributes is here.\n\nmy_array.shape\n\n\nmy_array.size\n\n\nmy_array.dtype\n\nArrays, unlike Python lists, cannot store items of different data types.\n\n# A regular Python list can store items of different data types\n[1, '3']\n\n\n# Arrays will convert everything to the same data type\nnp.array([1, '3'])\n\n\n# Another example of array type conversion\nnp.array([5, 8.3])\n\nArrays are also useful in performing vectorized operations. Given two or more arrays of equal length, arithmetic will perform element-wise computations across the arrays.\nFor example, observe the following:\n\n# Python list addition will concatenate the two lists\n[1, 2, 3] + [4, 5, 6]\n\n\n# NumPy array addition will add them element-wise\nnp.array([1, 2, 3]) + np.array([4, 5, 6])\n\nArrays can be subsetted by index position, as shown above, or by a logical vector of the same length. For example:\n\nexample_arr = np.arange(4, 10)\nexample_arr\n\nSuppose we want the last three elements. One option is to use index position:\n\nexample_arr[3:6]\n\nOr a logical vector:\n\nexample_arr[np.array([False, False, False, True, True, True])]\n\nThe latter approach allows one to subset based on a condition defined by the values of the vector. For example, we can use the condition \\(x \\geq 7\\) to obtain the logical vector used above.\n\nexample_arr >= 7\n\nAnd then we can subset just as before:\n\nexample_arr[example_arr >= 7]\n\nYou’ll see this done frequently, and it’s sometimes referred to as filtering, because we’re selectively removing values.\n\n\nQuestion 3\nGiven the array random_arr, create an array containing all values \\(x\\) such that \\(2x^4 > 1\\). Name the array valid_values.\n\n# for reproducibility - setting the seed will result in the same random draw each time\nnp.random.seed(42)\n\n# draw 60 uniformly random integers between 0 and 1\nrandom_arr = np.random.rand(60)\n\n# solution here\nvalid_values = ...\n\n\ngrader.check(\"q3\")\n\n\n\nA note on np.arange and np.linspace\nUsually we use np.arange to return an array that steps from a to b with a fixed step size s. While this is fine in some cases, we sometimes prefer to use np.linspace(a, b, N), which divides the interval [a, b] into N equally spaced points.\nnp.arange(start, stop, step) produces an array with all the numbers starting at start, incremendted up by step, stopping before stop is reached. For example, the value of np.arange(1, 6, 2) is an array with elements 1, 3, and 5 – it starts at 1 and counts up by 2, then stops before 6. np.arange(4, 9, 1) is an array with elements 4, 5, 6, 7, and 8. (It doesn’t contain 9 because np.arange stops before the stop value is reached.)\nnp.linspace always includes both end points while np.arange will not include the second end point b. For this reason, especially when we are plotting ranges of values we tend to prefer np.linspace.\nNotice how the following two statements have different parameters but return the same result.\n\nnp.arange(-5, 6, 1.0)\n\n\nnp.linspace(-5, 5, 11)\n\nCheck your understanding. Will np.arange(1, 10) produce an array that contains 10? Add a cell below and check to confirm your answer."
  },
  {
    "objectID": "miscellany.html",
    "href": "miscellany.html",
    "title": "Miscellany",
    "section": "",
    "text": "Automated tests in assignment notebooks are a guide, not a confirmation or refutation of your answer. Don’t rely too heavily on them, but do read the output message if they fail and think about what the message is telling you. On some occasions they will fail despite a correct answer; on others they will pass despite an incorrect answer. Furthermore, they will guide you to one particular strategy for obtaining the solution; most problems admit a few possible strategies.\nAll assignments are due on Mondays. You get two free late assignments. Late submissions are due Wednesdays.\nStart your homeworks and mini-projects early.\nTake your own notes during class; don’t simply rely on lecture slides."
  },
  {
    "objectID": "miscellany.html#troubleshooting",
    "href": "miscellany.html#troubleshooting",
    "title": "Miscellany",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nIf you try to open a notebook and the server fails at the ‘synchronizing git repository’ stage, open the LSIT server separately, rename the pstat100-content directory, and then try opening the notebook from the website link again. If successfull, you will need to migrate all of your previous work into the new pstat100-content directory."
  },
  {
    "objectID": "slides/week1-intro.html#attendance-form",
    "href": "slides/week1-intro.html#attendance-form",
    "title": "Course introduction",
    "section": "Attendance form",
    "text": "Attendance form"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-1-ace-and-health",
    "href": "slides/week1-intro.html#case-study-1-ace-and-health",
    "title": "Course introduction",
    "section": "Case study 1: ACE and health",
    "text": "Case study 1: ACE and health\n\nAssociation between adverse childhood experiences and general health, by sex."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-1-ace-and-health-1",
    "href": "slides/week1-intro.html#case-study-1-ace-and-health-1",
    "title": "Course introduction",
    "section": "Case study 1: ACE and health",
    "text": "Case study 1: ACE and health\nYou will:\n\nprocess and recode 10K survey responses from CDC’s 2019 behavior risk factor surveillance survey (BRFSS)\ncross-tabulate health-related measurements with frequency of adverse childhood experiences"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-2-seda",
    "href": "slides/week1-intro.html#case-study-2-seda",
    "title": "Course introduction",
    "section": "Case study 2: SEDA",
    "text": "Case study 2: SEDA\n\nEducation achievement gaps as functions of socioeconomic indicators, by gender."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-2-seda-1",
    "href": "slides/week1-intro.html#case-study-2-seda-1",
    "title": "Course introduction",
    "section": "Case study 2: SEDA",
    "text": "Case study 2: SEDA\nYou will:\n\nmerge test scores and socioeconomic indicators from the 2018 Standford Education Data Archive by school district\nvisually assess correlations between gender achievement gaps among grade schoolers and socioeconomic indicators across school districts in CA"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\n\nSea surface temperature reconstruction over the past 16,000 years."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology-1",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology-1",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\n\nClustering of diatom relative abundances in pleistocene (pre-11KyBP) vs. holocene (post-11KyBP) epochs."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology-2",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology-2",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\nYou will:\n\nexplore ecological community structure from relative abundances of diatoms measured in ocean sediment core samples spanning ~15,000 years\nuse dimension reduction techniques to obtain measures of community structure\nidentify shifts associated with the transition from pleistocene to holocene epochs"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nApparent disparity in allocation of DDS benefits across racial groups."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-1",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-1",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nExpenditure is strongly associated with age."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-2",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-2",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nCorrecting for age shows comparable expenditure across racial groups."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-3",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-3",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\nYou will:\n\nassess the case for discrimination in allocation of DDS benefits\nidentify confounding factors present in the sample\nmodel median expenditure by racial group after correcting for age"
  },
  {
    "objectID": "slides/week1-intro.html#scope",
    "href": "slides/week1-intro.html#scope",
    "title": "Course introduction",
    "section": "Scope",
    "text": "Scope\nThis course is about developing your data science toolkit with foundational skills:\n\nCore competency with Python data science libraries\nCritical thinking about data\nVisualization and exploratory analysis\nApplication of basic statistical concepts and methods in practice\nCommunication and interpretation of results"
  },
  {
    "objectID": "slides/week1-intro.html#whats-unique-about-pstat100",
    "href": "slides/week1-intro.html#whats-unique-about-pstat100",
    "title": "Course introduction",
    "section": "What’s unique about PSTAT100?",
    "text": "What’s unique about PSTAT100?\nThere are a few distinctive aspects:\n\nmultiple end-to-end case studies\nquestion-driven rather than method-driven\nemphasis on project workflow\ndata storytelling and communication"
  },
  {
    "objectID": "slides/week1-intro.html#limitations",
    "href": "slides/week1-intro.html#limitations",
    "title": "Course introduction",
    "section": "Limitations",
    "text": "Limitations\nThere are also some things we won’t cover:\n\nPredictive modeling or machine learning\nAlgorithm design and implementation\nTechniques and methods for big data\nTheoretical basis for methods"
  },
  {
    "objectID": "slides/week1-intro.html#weekly-pattern",
    "href": "slides/week1-intro.html#weekly-pattern",
    "title": "Course introduction",
    "section": "Weekly Pattern",
    "text": "Weekly Pattern\nWe’ll follow a simple weekly pattern:\n\nMondays\n\nLecture\nSections\nAssignments due 11:59pm PST\n\nWednesdays\n\nLecture\nLate work due 11:59pm PST"
  },
  {
    "objectID": "slides/week1-intro.html#course-pages-materials",
    "href": "slides/week1-intro.html#course-pages-materials",
    "title": "Course introduction",
    "section": "Course pages & materials",
    "text": "Course pages & materials\n\nMaterials via course website ruizt.github.io/pstat100\nComputing at pstat100.lsit.ucsb.edu\nAssignments/gradebook at Gradescope\nDiscussion board (TBA)"
  },
  {
    "objectID": "slides/week1-intro.html#tentative-schedule",
    "href": "slides/week1-intro.html#tentative-schedule",
    "title": "Course introduction",
    "section": "Tentative schedule",
    "text": "Tentative schedule\n\n\n\nWeek\nTopic\nLab\nHomework\nProject\n\n\n\n\n1\nData science life cycle\n\n\n\n\n\n2\nTidy data\nL0\n\n\n\n\n3\nSampling and bias\nL1\n\n\n\n\n4\nStatistical graphics\nL2\nH1\n\n\n\n5\nKernel density estimation\nL3\n\nMP1\n\n\n6\nPrincipal components\nL4\nH2\n\n\n\n7\nSimple regression\n\n\nMP2\n\n\n8\nMultiple regression\nL5\nH3\n\n\n\n9\nClassification and clustering\n\n\nCP1\n\n\n10\nCase study\n\nH4\n\n\n\n11\nFinals week\n\n\nCP2"
  },
  {
    "objectID": "slides/week1-intro.html#assessments",
    "href": "slides/week1-intro.html#assessments",
    "title": "Course introduction",
    "section": "Assessments",
    "text": "Assessments\n\nLabs introduce and develop core skills\nHomeworks apply core skills to case studies\nProjects practice creative problem-solving"
  },
  {
    "objectID": "slides/week1-intro.html#policies",
    "href": "slides/week1-intro.html#policies",
    "title": "Course introduction",
    "section": "Policies",
    "text": "Policies\n\nCommunication\n\nIf you have questions, please come to office hours\nAvoid email except for personal matters\n\nDeadlines and late work\n\nOne-hour grace period on all deadlines\n48-hour late submissions\nTwo free lates on any assignment (except last assignment)\n75% partial credit thereafter for late work"
  },
  {
    "objectID": "slides/week1-intro.html#policies-1",
    "href": "slides/week1-intro.html#policies-1",
    "title": "Course introduction",
    "section": "Policies",
    "text": "Policies\n\nGrades\n\nRoughly 10-20-30-40 attendance-labs-homeworks-projects\nFinal weighting and grade assignment at instructor’s discretion\nDo not expect 92+% = A, 90-92% = A-, 87-89.9 = B+, etc.\nA’s are awarded sparingly and indicate exceptional work"
  },
  {
    "objectID": "slides/week1-intro.html#other-info",
    "href": "slides/week1-intro.html#other-info",
    "title": "Course introduction",
    "section": "Other info",
    "text": "Other info\n\nInformal section swaps are allowed with TA permission\nAttendance required at all class meetings, but a few absences without notice are okay\nHonors contracts not available this quarter\nOffice hours start week 2, check website for schedule"
  },
  {
    "objectID": "slides/week1-intro.html#getting-started",
    "href": "slides/week1-intro.html#getting-started",
    "title": "Course introduction",
    "section": "Getting started",
    "text": "Getting started\n\nLab this week will introduce you to computing and course infrastructure\nPlease fill out intake survey ASAP\nCheck access to Gradescope, LSIT, course page\nReview syllabus"
  },
  {
    "objectID": "slides/week1-lifecycle.html#whats-data-science",
    "href": "slides/week1-lifecycle.html#whats-data-science",
    "title": "Data science lifecycle",
    "section": "What’s data science?",
    "text": "What’s data science?\nData science is a term of art encompassing a wide range of activities that involve uncovering insights from quantitative information.\n\nPeople that refer to themselves as data scientists typically combine specific interests (“domain knowledge”, e.g., biology) with computation, mathematics, and statistics and probability to contribute to knowledge in their communities.\n\nIntersectional in nature\nNo singular disciplinary background among practitioners"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-science-lifecycle",
    "href": "slides/week1-lifecycle.html#data-science-lifecycle",
    "title": "Data science lifecycle",
    "section": "Data science lifecycle",
    "text": "Data science lifecycle\n\nData science lifecycle: an end-to-end process resulting in a data analysis product\n\n\nQuestion formulation\nData collection and cleaning\nExploration\nAnalysis\n\n\nThese form a cycle in the sense that the steps are iterated for question refinement and futher discovery."
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-science-lifecylce",
    "href": "slides/week1-lifecycle.html#data-science-lifecylce",
    "title": "Data science lifecycle",
    "section": "Data science lifecylce",
    "text": "Data science lifecylce\n\n\nThe point isn’t really the exact steps, but rather the notion of an iterative process."
  },
  {
    "objectID": "slides/week1-lifecycle.html#starting-with-a-question",
    "href": "slides/week1-lifecycle.html#starting-with-a-question",
    "title": "Data science lifecycle",
    "section": "Starting with a question",
    "text": "Starting with a question\nThe scaling of brains with bodies is thought to contain clues about evolutionary patterns pertaining to intelligence.\n\nThere are lots of datasets out there with brain and body weight measurements, so let’s consider the question:\n\nWhat is the relationship between an animal’s brain and body weight?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-acquisition",
    "href": "slides/week1-lifecycle.html#data-acquisition",
    "title": "Data science lifecycle",
    "section": "Data acquisition",
    "text": "Data acquisition\nFrom Allison et al. 1976, average body and brain weights for 62 mammals.\n\n\n\n\n\n\n  \n    \n      \n      species\n      body_wt\n      brain_wt\n    \n  \n  \n    \n      0\n      Africanelephant\n      6654.000\n      5712.0\n    \n    \n      1\n      Africangiantpouchedrat\n      1.000\n      6.6\n    \n    \n      2\n      ArcticFox\n      3.385\n      44.5\n    \n  \n\n\n\n\nUnits of measurement\n\nbody weight in kilograms\nbrain weight in grams"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment",
    "href": "slides/week1-lifecycle.html#data-assessment",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nHow well-matched is the data to our question?\n\nMammals only (no birds, fish, reptiles, etc.)\nSpecies are those for which convenient specimens were available\nAverages across specimens are reported (‘aggregated’ data)\n\n\nWhat do you think? Take a moment to discuss with your neighbor."
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment-1",
    "href": "slides/week1-lifecycle.html#data-assessment-1",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nBased on the great points you just made, we really only stand to learn something about this particular sample of animals.\n\nIn other words, no inference is possible.\n\n\n\nDo you think the data are still useful?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#inpection",
    "href": "slides/week1-lifecycle.html#inpection",
    "title": "Data science lifecycle",
    "section": "Inpection",
    "text": "Inpection\nThis dataset is already impeccably neat: each row is an observation for some species of mammal, and the columns are the two variables (average weight).\nSo no tidying needed – we’ll just check the dimensions and see if any values are missing.\n\n# dimensions?\nbb_weights.shape\n\n(62, 3)\n\n\n\n# missing values?\nbb_weights.isna().sum(axis = 0)\n\nspecies     0\nbody_wt     0\nbrain_wt    0\ndtype: int64"
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration",
    "href": "slides/week1-lifecycle.html#exploration",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nVisualization is usually a good starting point for exploring data.\n\n\n\n\n\n\n\nNotice the apparent density of points near \\((0, 0)\\) – that suggests we shouldn’t look for a relationship on the scale of kg/g."
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration-1",
    "href": "slides/week1-lifecycle.html#exploration-1",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nA simple transformation of the axes reveals a clearer pattern."
  },
  {
    "objectID": "slides/week1-lifecycle.html#analysis",
    "href": "slides/week1-lifecycle.html#analysis",
    "title": "Data science lifecycle",
    "section": "Analysis",
    "text": "Analysis\nThe plot shows us that there’s a roughly linear relationship on the log scale:\n\\[\\log(\\text{brain}) = \\alpha \\log(\\text{body}) + c\\]\n\nSo what does that mean in terms of brain and body weights? A little algebra and we have a “power law”:\n\\[(\\text{brain}) \\propto (\\text{body})^\\alpha\\]"
  },
  {
    "objectID": "slides/week1-lifecycle.html#interpretation",
    "href": "slides/week1-lifecycle.html#interpretation",
    "title": "Data science lifecycle",
    "section": "Interpretation",
    "text": "Interpretation\nSo it appears that the brain-body scaling is well-described by a power law:\n\namong selected specimens of these 62 species of mammal, average brain weight is approximately proportional to a power of average body weight\n\n\nNotice that I did not say:\n\nanimals’ brains are proportional to a power of their bodies\namong these 62 mammals, average brain weight is approximately proportional to a power of average body weight"
  },
  {
    "objectID": "slides/week1-lifecycle.html#question-refinement",
    "href": "slides/week1-lifecycle.html#question-refinement",
    "title": "Data science lifecycle",
    "section": "Question refinement",
    "text": "Question refinement\nWe can now ask further, more specific questions:\n\nDo other types of animals exhibit the same power law relationship?\n\n\nTo investigate, we need richer data."
  },
  {
    "objectID": "slides/week1-lifecycle.html#more-data-acquisition",
    "href": "slides/week1-lifecycle.html#more-data-acquisition",
    "title": "Data science lifecycle",
    "section": "(More) data acquisition",
    "text": "(More) data acquisition\nA number of authors have compiled and published ‘meta-analysis’ datasets by combining the results of multiple studies.\nBelow we’ll import a few of these for three different animal classes.\n\n# import metaanalysis datasets\nreptiles = pd.read_csv('data/reptile_meta.csv')\nbirds = pd.read_csv('data/bird_meta.csv', encoding = 'latin1')\nmammals = pd.read_csv('data/mammal_meta.csv', encoding = 'latin1')"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment-2",
    "href": "slides/week1-lifecycle.html#data-assessment-2",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nWhere does this data come from? It’s kind of a convenience sample of scientific data:\n\nMultiple studies \\(\\rightarrow\\) possibly different sampling and measurement protocols\nCriteria for inclusion unknown \\(\\rightarrow\\) probably neither comprehensive nor representative of all such measurements taken\n\n\nSo these data, while richer, are still relatively narrow in terms of generalizability."
  },
  {
    "objectID": "slides/week1-lifecycle.html#a-comment-on-scope-of-inference",
    "href": "slides/week1-lifecycle.html#a-comment-on-scope-of-inference",
    "title": "Data science lifecycle",
    "section": "A comment on scope of inference",
    "text": "A comment on scope of inference\nThese data don’t support general inferences (e.g., to all animals, all mammals, etc.) because they weren’t collected for the purpose to which we’re putting them.\n\nUsually, if data are not collected for the explicit purpose of the question you’re trying to answer, they won’t constitute a representative sample."
  },
  {
    "objectID": "slides/week1-lifecycle.html#tidying",
    "href": "slides/week1-lifecycle.html#tidying",
    "title": "Data science lifecycle",
    "section": "Tidying",
    "text": "Tidying\nIn order to comine the datasets one must:\n\nSelect columns of interest;\nPut in consistent order;\nGive consistent names;\nConcatenate row-wise.\n\n\nWe’ll skip the details."
  },
  {
    "objectID": "slides/week1-lifecycle.html#inspection",
    "href": "slides/week1-lifecycle.html#inspection",
    "title": "Data science lifecycle",
    "section": "Inspection",
    "text": "Inspection\nThis dataset has quite a lot of missing brain weight measurements: many of the studies combined to form these datasets did not include that particular measurement.\n\n# missing values?\ndata.isna().mean(axis = 0)\n\nOrder      0.00000\nFamily     0.00000\nGenus      0.00000\nSpecies    0.00000\nSex        0.00000\nbody       0.00000\nbrain      0.57404\nclass      0.00000\ndtype: float64"
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration-2",
    "href": "slides/week1-lifecycle.html#exploration-2",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nFocusing on the nonmissing values, we see the same power law relationship but with different proportionality constants for the three classes of animals."
  },
  {
    "objectID": "slides/week1-lifecycle.html#analysis-1",
    "href": "slides/week1-lifecycle.html#analysis-1",
    "title": "Data science lifecycle",
    "section": "Analysis",
    "text": "Analysis\nSo we might hypothesize that:\n\\[\n(\\text{brain}) = \\beta_1(\\text{body})^{\\alpha_1} \\qquad \\text{(mammal)} \\\\\n(\\text{brain}) = \\beta_2(\\text{body})^{\\alpha_2} \\qquad \\text{(reptile)} \\\\\n(\\text{brain}) = \\beta_3(\\text{body})^{\\alpha_3} \\qquad \\text{(bird)} \\\\\n\\beta_i \\neq \\beta_j, \\alpha_i \\neq \\alpha_j \\quad \\text{for } i \\neq j\n\\]"
  },
  {
    "objectID": "slides/week1-lifecycle.html#interpretation-1",
    "href": "slides/week1-lifecycle.html#interpretation-1",
    "title": "Data science lifecycle",
    "section": "Interpretation",
    "text": "Interpretation\nIt seems that the average brain and body weights of the birds, mammals, and reptiles measured in these studies exhibit distinct power law relationships.\n\nWhat would you investigate next?\n\nCorrelates of body weight?\nAdjust for lifespan, habitat, predation, etc.?\nEstimate the \\(\\alpha_i\\)’s and \\(\\beta_i\\)’s?\nPredict brain weights for unobserved species?\nSomething else?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#a-comment",
    "href": "slides/week1-lifecycle.html#a-comment",
    "title": "Data science lifecycle",
    "section": "A comment",
    "text": "A comment\nNotice that I did not mention the word ‘model’ anywhere!\n\nThis was intentional – it is a common misconception that analyzing data always involves fitting models.\n\nModels are not not always necessary or appropriate\nYou can learn a lot from exploratory techniques\nModels approximate specific kinds of relationships in data\nExploratory analysis can reveal unexpected structure"
  },
  {
    "objectID": "slides/week1-lifecycle.html#next-week",
    "href": "slides/week1-lifecycle.html#next-week",
    "title": "Data science lifecycle",
    "section": "Next week",
    "text": "Next week\nTabular data operations with pandas; the ‘tidy data’ standard.\n\nLab 0 due Monday 11:59pm PST"
  },
  {
    "objectID": "content.html#getting-started-checklist",
    "href": "content.html#getting-started-checklist",
    "title": "Materials",
    "section": "Getting started checklist",
    "text": "Getting started checklist\n\nConfirm access to all course pages\nRead syllabus\nFill out intake survey"
  }
]