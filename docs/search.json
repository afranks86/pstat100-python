[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Instructor: Trevor Ruiz\nTeaching assistants: Mengye Liu, Harry Yu, Gabrielle Salo\nClass meetings: M-W 12:30pm – 1:45pm Buchanan 1920\nSection meetings:\n\nM 2:00pm – 2:50pm Phelps 1525 (Mengye)\nM 3:00pm – 3:50pm Phelps 1525 (Mengye)\nM 4:00pm – 4:50pm Phelps 1513 (Harry)\nM 5:00pm – 5:50pm Phelps 1525 (Harry)\nM 6:00pm – 6:50pm Phelps 1525 (Gabrielle)\n\nOffice hours:\n\nMengye M 9:00am – 11:00am on Zoom\nGabrielle Tu 7:00pm – 8:00pm on Zoom\nHarry W 2:00pm – 4:00pm Building 434 Room 113\nTrevor W 2:00pm – 3:00pm ILP 2207"
  },
  {
    "objectID": "about.html#content-and-materials",
    "href": "about.html#content-and-materials",
    "title": "Course syllabus",
    "section": "Content and materials",
    "text": "Content and materials\nData Science Concepts and Analysis (PSTAT100) is a hands-on introduction to data science intended for intermediate-level students from any discipline with some exposure to probability and basic computing skills, but few or no upper-division courses in statistics or computer science. The course introduces central concepts in statistics – such as sampling variation, uncertainty, and inference – in an applied setting together with techniques for data exploration and analysis. Applications emphasize end-to-end data analyses. Course activities model standard data science workflow practices by example, and successful students acquire programming skills, project management skills, and subject exposure that will serve them well in upper-division courses as well as in independent research or projects.\n\nCatalog description\nOverview of data science key concepts and the use of tools for data retrieval, analysis, visualization, and reproducible research. Topics include an introduction to inference and prediction, principles of measurement, missing data, and notions of causality, statistical “traps”, and concepts in data ethics and privacy. Case studies will illustrate the importance of domain knowledge. Credit units: 4.\n\n\nPrerequisites\n\nProbability and Statistics I (PSTAT 120A)\nLinear Algebra (MATH 4A)\nPrior experience with Python or another programming language (CMPSC 9 or CMPSC 16).\n\n\n\nLearning outcomes\nSuccessful students will establish foundational data science skills:\n\ncritical assessment of data quality and sampling design\nretrieval, inspection, and cleaning of raw data\nexploratory, descriptive, visual, and inferential techniques\ninterpretation and communication of results in context\n\nThese skills will be discussed in depth during course lectures; students will practice them through lab activities, homework assignments, and project work.\n\n\nAssessments\nAttainment of course learning outcomes will be measured by assessment of submitted work. Submitted work falls into four categories:\n\nLabs will be assigned weekly in most weeks. These are structured coding assignments with small exercises throughout that introduce the programming skills needed to complete homework assignments.\nHomeworks will be assigned biweekly. These are fairly involved assignments which apply concepts and techniques from the lectures and programming skills from the labs to real data sets in order to reproduce an analysis and answer substantive questions. Collaboration is encouraged, but students must write up and submit their own work individually.\nMini projects will be assigned biweekly in alternation with homeworks. These assignments prompt students to use skills from the course in an unstructured setting to answer high-level questions pertaining to one or more datasets. Mini projects should be completed collaboratively.\nA course project will be assigned requiring students to carry out an open-ended data analysis. This will be completed in teams. Each team will prepare a project plan for initial feedback a few weeks before the end of the quarter, and submit a final report of work and findings by the end of the quarter.\n\nOverall scores in the course will be calculated for each student as the weighted average of scores on all submitted work; the relative weighting and letter grade assignments will depend entirely on the score distribution of the class as a whole and as such reflect each student’s performance relative to their peers.\n\n\nSchedule\nThe tentative topic and assignment schedule is given below. Assignments are indicated by due date: all assignments are due by Monday 11:59pm in the week indicated. Late submissions are allowed, with a possible penalty, for up to 48 hours.\nThe schedule is subject to change based on the progress of the class.\n\n\n\nWeek\nTopic\nLab\nHomework\nProject\n\n\n\n\n1\nData science life cycle\n\n\n\n\n\n2\nTidy data\nL0\n\n\n\n\n3\nSampling and bias\nL1\n\n\n\n\n4\nStatistical graphics\nL2\nH1\n\n\n\n5\nKernel density estimation\nL3\n\nMP1\n\n\n6\nPrincipal components\nL4\nH2\n\n\n\n7\nSimple regression\n\n\nMP2\n\n\n8\nMultiple regression\nL5\nH3\n\n\n\n9\nClassification and clustering\n\n\nCP1\n\n\n10\nCase study\n\nH4\n\n\n\n11\nFinals\n\n\nCP2\n\n\n\n\nL: lab\nH: homework\nMP: mini project\nCP: course project\n\n\n\nMaterials\nThe course website ruizt.github.io/pstat100 will link to all course content and resources. Readings for the course will draw on multiple sources, including:\n\nPython Data Science Handbook (PDSH);\nLearning Data Science (LDS);\ncollected articles distributed as assigned.\n\n\n\nSoftware\nComputing will be hosted via the course LSIT server pstat100.lsit.ucsb.edu. Students need only a web browser and stable internet connection to complete all course work. It is strongly recommended that students download backup copies of their assignments from the LSIT server.\nInterested students are encouraged to install the software needed to open, edit, and execute notebooks on their own machine, in particular:\n\na Python install;\n(recommended) Miniconda\nJupyter;\npackages utilized in course materials (primarily numpy, pandas, altair, and scikit-learn).\n\nManaging package installations will require some (straightforward) use of the package installer pip or pip3 in the terminal to retrieve/install packages from the Python Package Index repository. Documentation for specific packages (or a Google search) will indicate the appropriate pip command."
  },
  {
    "objectID": "about.html#policies",
    "href": "about.html#policies",
    "title": "Course syllabus",
    "section": "Policies",
    "text": "Policies\n\nCommunication\nThere are two primary means of communication outside of scheduled class meetings: office hours and a discussion board.\nCourse staff have limited availability via email. Course staff will make every effort to respond to individual communication within 48 weekday-hours on the following (or similar) matters:\n\naccommodations/extensions due to personal circumstances;\nlogistical issues such as access to materials or missing scores;\ngeneral advising.\n\nEmail should not be used to ask content questions or submit assignments (unless specifically requested). Emails related to the following (or similar) matters may not receive replies and should be redirected:\n\n\n\nTopic\nRedirect to…\n\n\n\n\nTroubleshooting codes\nDiscussion board\n\n\nChecking answers\nOffice hours or discussion board\n\n\nClarifying assignment content\nOffice hours or discussion board\n\n\nAssignment submission\nGradescope\n\n\nRe-evaluation request\nGradescope\n\n\n\n\n\nExpected time commitment\nThe course is 4 credit units; each credit unit corresponds to an approximate time commitment of 3 hours. So, students should expect to allocate 12 hours per week to the course on average. Course staff are available to help any students spending considerably more time on the class balance the workload.\n\n\nScores and grades\nScores on submitted work can be monitored on Gradescope to ensure fair assignment of course grades. On any individual assignment, re-evaluation can be requested within one week of receiving a score. Requests for re-evaluation made beyond one week after publication of scores may or may not be considered on a discretionary basis.\nDetermination of letter grade assignments is made entirely at the discretion of the instructor based on the assessments outlined above and consistent with university policy. Students are not permitted to negotiate their grades, and are discouraged from requesting audits, recalculations, or verification of self-calculations after the course has concluded. The instructor is under no obligation to share the details of grade calculations with students or to respond to such requests.\nIf at the end of the course a student believes their grade was unfairly assigned, either due to discrimination or without basis in coursework, they are entitled to contest it according to the procedure outlined here.\n\n\nConduct\nStudents are expected to uphold the student code of conduct and to maintain integrity. All individually-submitted work must be an honest reflection of individual effort. Evidence of dishonest conduct will be discussed with the student(s) involved and reported to the Office of Student Conduct (OSC). Depending on the nature of the evidence and the violation, penalty in the course may range from a warning to loss of credit to automatic failure. For a definition and examples of dishonesty, a discussion of what constitutes an appropriate response from faculty, and an explanation of the reporting and investigation process, see the OSC page on academic integrity.\n\n\nDeadlines and late work\nThere is a one-hour grace period on all submission deadlines. After that, work may be submitted within 48 hours of the original deadline (not the deadline plus grace period) and will be considered late. Every student can submit two late assignments without penalty. Subsequent late submissions will be evaluated for 75% credit.\n\n\nAccommodations\nReasonable accommodations will be made for any student with a qualifying disability. Such requests should be made through the Disabled Students Program (DSP). More information, instructions on how to access accommodations, and information on related resources can be found on the DSP website.\n\n\nFeedback\nToward the end of the term students will be given an opportunity to provide feedback about the course via ESCI. This feedback is valuable for improvement of the course in future terms, and students are strongly encouraged to provide thoughtful course evaluations. The identities of student respondents to ESCI surveys are not disclosed to instructors."
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Materials",
    "section": "",
    "text": "Attendance form (fill out once per class meeting, including sections)\nGradescope (for assignment submissions)\nLSIT server (for course computing)"
  },
  {
    "objectID": "content.html#getting-started-checklist",
    "href": "content.html#getting-started-checklist",
    "title": "Materials",
    "section": "Getting started checklist",
    "text": "Getting started checklist\n\nConfirm access to all course pages\nRead syllabus\nFill out intake survey"
  },
  {
    "objectID": "content.html#week-1",
    "href": "content.html#week-1",
    "title": "Materials",
    "section": "Week 1",
    "text": "Week 1\nReadings:\n\nLDS1 The Data Science Lifecycle\nLDS5 Case Study: Why is my Bus Always Late?\nPDSH2.1 Understanding data types in python\nPDSH2.2 The basics of numpy arrays\nPDSH2.4 Aggregations: min, max, and everything in between\n\nMonday: Course introduction [slides]\nLab sections: Orientation to Jupyter notebooks [html] [notebook] [solutions]\nWednesday: Data science lifecycle [slides]"
  },
  {
    "objectID": "content.html#week-2",
    "href": "content.html#week-2",
    "title": "Materials",
    "section": "Week 2",
    "text": "Week 2\nReadings:\n\nWickham (2014). Tidy data. Journal of statistical software 59(10). [link to paper]\nPDSH3.1 Introducing pandas objects\nPDSH3.2 Data indexing and selection\nPDSH3.7 Merge and join\nPDSH3.8 Aggregation and grouping\n\nAssignments:\n\nHW1, BRFSS case study, due Monday, April 24 [html] [notebook]\n\nMonday: Tidy data [slides]\nLab sections: Pandas [html] [notebook]\nWednesday: Dataframe transformations [slides]"
  },
  {
    "objectID": "content.html#week-3",
    "href": "content.html#week-3",
    "title": "Materials",
    "section": "Week 3",
    "text": "Week 3\nReadings:\n\nLDS2.2 Population, frame, sample\nConcepts in incomplete data\nPDSH3.4 Handling missing data\n\nAssignments:\n\nMini project 1, due Monday, May 1 [html] [notebook]\n\nMonday: Sampling, bias, and missingness [slides]\nLab sections: Exploring sampling bias through simulation [html] [notebook]"
  },
  {
    "objectID": "hw/hw1-brfss/hw1-brfss-soln.html",
    "href": "hw/hw1-brfss/hw1-brfss-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport altair as alt\n\n\nBackground\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a long-term effort administered by the CDC to collect data on behaviors affecting physical and mental health, past and present health conditions, and access to healthcare among U.S. residents. The BRFSS comprises telephone surveys of U.S. residents conducted annually since 1984; in the last decade, over half a million interviews have been conducted each year. This is the largest such data collection effort in the world, and many countries have developed similar programs. The objective of the program is to support monitoring and analysis of factors influencing public health in the United States.\nEach year, a standard survey questionnaire is developed that includes a core component comprising questions about: demographic and household information; health-related perceptions, conditions, and behaviors; substance use; and diet. Trained interviewers in each state call randomly selected telephone (landline and cell) numbers and administer the questionnaire; the phone numbers are chosen so as to obtain a representative sample of all households with telephone numbers. Take a moment to read about the 2019 survey here.\nIn this assignment you’ll import and subsample the BRFSS 2019 data and perform a simple descriptive analysis exploring associations between adverse childhood experiences, health perceptions, tobacco use, and depressive disorders. This is an opportunity to practice:\n\nreview of data documentation\ndata assessment and critical thinking about data collection\ndataframe transformations in pandas\ncommunicating and interpreting grouped summaries\n\n\n\nData import and assessment\nThe cell below imports select columns from the 2019 dataset as a pandas DataFrame. The file is big, so this may take a few moments. Run the cell and then have a quick look at the first few rows and columns.\n\n# store variable names of interest\nselected_vars = ['_SEX', '_AGEG5YR', \n                 'GENHLTH', 'ACEPRISN', \n                 'ACEDRUGS', 'ACEDRINK', \n                 'ACEDEPRS', 'ADDEPEV3', \n                 '_SMOKER3', '_LLCPWT']\n\n# import full 2019 BRFSS dataset\nbrfss = pd.read_csv('data/brfss2019.zip', compression = 'zip', usecols = selected_vars)\n\n# invert sampling weights\nbrfss['_LLCPWT'] = 1/brfss._LLCPWT\n\n# print first few rows\nbrfss.head()\n\n\n\n\n\n  \n    \n      \n      GENHLTH\n      ADDEPEV3\n      ACEDEPRS\n      ACEDRINK\n      ACEDRUGS\n      ACEPRISN\n      _LLCPWT\n      _SEX\n      _AGEG5YR\n      _SMOKER3\n    \n  \n  \n    \n      0\n      3.0\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.007391\n      2.0\n      13.0\n      3.0\n    \n    \n      1\n      4.0\n      2.0\n      2.0\n      1.0\n      2.0\n      2.0\n      0.000687\n      2.0\n      11.0\n      4.0\n    \n    \n      2\n      3.0\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.004639\n      2.0\n      10.0\n      4.0\n    \n    \n      3\n      4.0\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.003827\n      2.0\n      13.0\n      9.0\n    \n    \n      4\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.001868\n      2.0\n      13.0\n      3.0\n    \n  \n\n\n\n\n\nQuestion 1: Data dimensions\nCheck the dimensions of the dataset. Store the dimensions as nrows and ncolumns.\n\nnrows, ncolumns = brfss.shape # SOLUTION\n\nprint(nrows, ncolumns)\n\n418268 10\n\n\n\ngrader.check(\"q1\")\n\n\n\n\nQuestion 2: Row and column information\nNow that you’ve imported the data, you should verify that the dimensions conform to the format you expect based on data documentation and ensure you understand what each row and each column represents.\nCheck the number of records (interviews conducted) reported and variables measured for 2019 by reviewing the surveillance summaries by year, and then answer the following questions in a few sentences:\n\nDoes the number of rows match the number of reported records?\nHow many columns were imported, and how many columns are reported in the full dataset?\nWhat does each row in the brfss dataframe represent?\nWhat does each column in the brfss dataframe represent\n\nType your answer here, replacing this text.\nSOLUTION:\nYes, there are exactly as many rows as reported records. The documentation reports 342 variables measured; we only imported 10 of those variables. Each row corresponds to a respondent, and each column corresponds to a respondent attribute or answer to one of the survey questions.\n\n\n\n\nQuestion 3: Sampling design and data collection\nSkim the overview documentation for the 2019 BRFSS data. Focus specifically the ‘Background’ and ‘Data Collection’ sections, read selectively for relevant details, and answer the following questions in a few sentences:\n\nWho conducts the interviews and how long does a typical interview last?\nWho does an interviewer speak to in each household?\nWhat criteria must a person meet to be interviewed?\nWho can’t appear in the survey? Give two examples.\nWhat is the study population (i.e., all individuals who could possibly be sampled)?\nDoes the data contain any identifying information?\n\nType your answer here, replacing this text.\nSOLUTION: State healthcare personell or trained contractors conduct interveiews, and they generally last 17-27 minutes. After speaking with whoever answers the phone, the interviewer determines a randomly selected adult in the household to survey. The respondent must be over 18, live in a private residence or college housing, and have a working phone. Anyone not meeting these criteria cannot participate, such as: anyone under 18; anyone living in residential care facilities or prisons; anyone without a permanent home. The study population is all adult U.S. residents with working phones and living in private or college housing. The data are de-identified and none of the variables allow for easy reconstruction of a respondent’s identity.\n\n\n\n\nQuestion 4: Variable descriptions\nYou’ll work with the small subset variables imported above: sex, age, general health self-assessment, smoking status, depressive disorder, and adverse childhood experiences (ACEs). The names of these variables as they appear in the raw dataset are defined in the cell in which you imported the data as selected_vars. It is often useful, and therefore good practice, to include a brief description of each variable at the outset of any reported analyses, both for your own clarity and for that of any potential readers. Open the 2019 BRFSS codebook in your browser and use text searching to locate each of the variable names of interest. Read the codebook entries and fill in the second column in the table below with a one-sentence description of each variable identified in selected_vars. Rephrase the descriptions in your own words – do not copy the codebook descriptions verbatim.\n\n\n\nVariable name\nDescription\n\n\n\n\nGENHLTH\n\n\n\n_SEX\n\n\n\n_AGEG5YR\n\n\n\nACEPRISN\n\n\n\nACEDRUGS\n\n\n\nACEDRINK\n\n\n\nACEDEPRS\n\n\n\nADDEPEV3\n\n\n\n_SMOKER3\n\n\n\n\nSOLUTION\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\nGENHLTH\nSelf-rated general health\n\n\n_SEX\nRespondent’s sex\n\n\n_AGEG5YR\nAge bracket in 5-year intervals\n\n\nACEPRISN\nLived with anyone who served prison time or was in prison?\n\n\nACEDRUGS\nLived with anyone abusing substances?\n\n\nACEDRINK\nLived with a problem drinker or alcoholic?\n\n\nACEDEPRS\nLived with anyone depressed, mentally ill, or suicidal?\n\n\nADDEPEV3\nEver diagnosed with a depressive disorder?\n\n\n_SMOKER3\nSmoking status\n\n\n\n\n\n\n\nSubsampling\nTo simplify life a little, we’ll draw a large random sample of the rows and work with that in place of the full dataset. This is known as subsampling.\nThe cell below draws a random subsample of 10k records. Because the subsample is randomly drawn, we should not expect it to vary in any systematic way from the overall dataset, and distinct subsamples should have similar properties – therefore, results downstream should be similar to an analysis of the full dataset, and should also be possible to replicate using distinct subsamples.\n\n# for reproducibility\nnp.random.seed(32221) \n\n# randomly sample 10k records\nsamp = brfss.sample(n = 10000, \n                    replace = False, \n                    weights = '_LLCPWT')\n\nAsides:\n\nNotice that the random number generator seed is set before carrying out this task – this ensures that every time the cell is run, the same subsample is drawn. As a result, the computations in this notebook are reproducible: when I run the notebook on my computer, I get the same results as you get when you run the notebook on your computer.\nNotice also that sampling weights provided with the dataset are used to draw a weighted sample. Some respondents are more likely to be selected than others from the general population of U.S. adults with phone numbers, so the BRFSS calculates derived weights that are inversely proportional to estimates of the probability that the respondent is included in the survey. This is a somewhat sophisticated calculation, however if you’re interested, you can read about how these weights are calculated and why in the overview documentation you used to answer the questions above. We use the sampling weights in drawing the subsample so that we get a representative sample of U.S. adults with phone numbers.\nNotice the missing values. How many entries are missing in each column? The cell below computes the proportion of missing values for each of the selected variables. We’ll return to this issue later on.\n\n\n# proportions of missingness \nsamp.isna().mean()\n\nGENHLTH     0.0000\nADDEPEV3    0.0000\nACEDEPRS    0.8086\nACEDRINK    0.8088\nACEDRUGS    0.8088\nACEPRISN    0.8088\n_LLCPWT     0.0000\n_SEX        0.0000\n_AGEG5YR    0.0000\n_SMOKER3    0.0000\ndtype: float64\n\n\n\n\nTidying\nIn the following series of questions you’ll tidy up the subsample by performing these steps:\n\nselecting columns of interest;\nreplacing coded values of question responses with responses;\ndefining new variables based on existing ones;\nrenaming columns.\n\nThe goal of this is to produce a clean version of the dataset that is well-organized, intuitive to navigate, and ready for analysis.\nThe variable entries are coded numerically to represent certain responses. These should be replaced by more informative entries. We can use the codebook to determine which number means what, and replace the values accordingly.\nThe cell below replaces the numeric values for _AGEG5YR by their meanings, illustrating how to use .replace() with a dictionary to convert the numeric coding to interpretable values. The basic strategy is:\n\nStore the variable coding for VAR as a dictionary var_codes.\nUse .replace({'VAR': var_codes}) to modify values.\n\nIf you need additional examples, check the pandas documentation for .replace().\n\n# dictionary representing variable coding\nage_codes = {\n    1: '18-24', 2: '25-29', 3: '30-34',\n    4: '35-39', 5: '40-44', 6: '45-49',\n    7: '50-54', 8: '55-59', 9: '60-64',\n    10: '65-69', 11: '70-74', 12: '75-79',\n    13: '80+', 14: 'Unsure/refused/missing'\n}\n\n# recode age categories\nsamp_mod1 = samp.replace({'_AGEG5YR': age_codes})\n\n# check result\nsamp_mod1.head()\n\n\n\n\n\n  \n    \n      \n      GENHLTH\n      ADDEPEV3\n      ACEDEPRS\n      ACEDRINK\n      ACEDRUGS\n      ACEPRISN\n      _LLCPWT\n      _SEX\n      _AGEG5YR\n      _SMOKER3\n    \n  \n  \n    \n      237125\n      5.0\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.057004\n      2.0\n      25-29\n      3.0\n    \n    \n      329116\n      5.0\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.108336\n      2.0\n      80+\n      3.0\n    \n    \n      178937\n      3.0\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.000998\n      1.0\n      18-24\n      4.0\n    \n    \n      410081\n      4.0\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.021973\n      2.0\n      45-49\n      2.0\n    \n    \n      184555\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.027175\n      2.0\n      80+\n      3.0\n    \n  \n\n\n\n\n\nQuestion 5: Recoding variables\nFollowing the example immediately above and referring to the 2019 BRFSS codebook, replace the numeric codings with response categories for each of the following variables:\n\n_SEX\nGENHLTH\n_SMOKER3\n\nNotice that above, the first modification (slicing) was stored as samp_mod1, and was a function of samp. You’ll follow this pattern, creating samp_mod2, samp_mod3, and so on so that each step (modification) of your data manipulations is stored separately, for easy troubleshooting.\n\nRecode _SEX: define a new dataframe samp_mod2 that is the same as samp_mod1 but with the _SEX variable recoded as M and F.\nRecode GENHLTH: define a new dataframe samp_mod3 that is the same as samp_mod2 but with the GENHLTH variable recoded as Excellent, Very good, Good, Fair, Poor, Unsure, and Refused.\nRecode _SMOKER3: define a new dataframe samp_mod4 that is the same as samp_mod3 but with _SMOKER3 recoded as Daily, Some days, Former, Never, and Unsure/refused/missing.\nPrint the first few rows of samp_mod4.\n\n\n# define dictionary for sex\nsex_codes = {1: 'M', 2: 'F'} # SOLUTION\n\n# recode sex\nsamp_mod2 = samp_mod1.replace({'_SEX': sex_codes}) # SOLUTION\n\n# define dictionary for health\nhealth_codes = { 1: 'Excellent', 2: 'Very good', 3: 'Good', 4: 'Fair', 5: 'Poor', 7: 'Unsure', 9: 'Refused'} # SOLUTION\n\n# recode health\nsamp_mod3 = samp_mod2.replace({'GENHLTH': health_codes}) # SOLUTION\n\n# define dictionary for smoking\nsmoke_codes = { 1: 'Daily', 2: 'Some days', 3: 'Former', 4: 'Never', 9: 'Unsure/refused/missing'} # SOLUTION\n\n# recode smoking\nsamp_mod4 = samp_mod3.replace({'_SMOKER3': smoke_codes}) # SOLUTION\n\n# print a few rows\nsamp_mod4.head() # SOLUTION\n\n\n\n\n\n  \n    \n      \n      GENHLTH\n      ADDEPEV3\n      ACEDEPRS\n      ACEDRINK\n      ACEDRUGS\n      ACEPRISN\n      _LLCPWT\n      _SEX\n      _AGEG5YR\n      _SMOKER3\n    \n  \n  \n    \n      237125\n      Poor\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.057004\n      F\n      25-29\n      Former\n    \n    \n      329116\n      Poor\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.108336\n      F\n      80+\n      Former\n    \n    \n      178937\n      Good\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.000998\n      M\n      18-24\n      Never\n    \n    \n      410081\n      Fair\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.021973\n      F\n      45-49\n      Some days\n    \n    \n      184555\n      Very good\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.027175\n      F\n      80+\n      Former\n    \n  \n\n\n\n\n\ngrader.check(\"q5\")\n\n\n\nQuestion 6: Value replacement\nNow all the variables except the adverse childhood experience and depressive disorder question responses are represented interpretably. In the codebook that the answer key is identical for these remaining variables.\nThe numeric codings can be replaced all at once by applying .replace() to the dataframe with an argument of the form\n\ndf.replace({'var1': varcodes1, 'var2': varcodes1, ..., 'varp': varcodesp})\n\nDefine a new dataframe samp_mod5 that is the same as samp_mod4 but with the remaining variables recoded according to the answer key Yes, No, Unsure, Refused. Print the first few rows of the result using .head().\n\n# define dictionary\nanswer_codes = {1: 'Yes', 2: 'No', 7: 'Unsure', 9: 'Refused'} #SOLUTION\n\n# recode\nsamp_mod5 =  samp_mod4.replace({'ACEPRISN': answer_codes, 'ACEDRUGS': answer_codes, 'ACEDRINK': answer_codes, 'ACEDEPRS': answer_codes, 'ADDEPEV3': answer_codes}) #SOLUTION\n\n# check using head()\nsamp_mod5.head() #SOLUTION\n\n\n\n\n\n  \n    \n      \n      GENHLTH\n      ADDEPEV3\n      ACEDEPRS\n      ACEDRINK\n      ACEDRUGS\n      ACEPRISN\n      _LLCPWT\n      _SEX\n      _AGEG5YR\n      _SMOKER3\n    \n  \n  \n    \n      237125\n      Poor\n      No\n      NaN\n      NaN\n      NaN\n      NaN\n      0.057004\n      F\n      25-29\n      Former\n    \n    \n      329116\n      Poor\n      No\n      NaN\n      NaN\n      NaN\n      NaN\n      0.108336\n      F\n      80+\n      Former\n    \n    \n      178937\n      Good\n      No\n      NaN\n      NaN\n      NaN\n      NaN\n      0.000998\n      M\n      18-24\n      Never\n    \n    \n      410081\n      Fair\n      Yes\n      NaN\n      NaN\n      NaN\n      NaN\n      0.021973\n      F\n      45-49\n      Some days\n    \n    \n      184555\n      Very good\n      No\n      No\n      No\n      No\n      No\n      0.027175\n      F\n      80+\n      Former\n    \n  \n\n\n\n\n\ngrader.check(\"q6\")\n\nFinally, all the variables in the dataset are categorical. Notice that the current data types do not reflect this.\n\nsamp_mod5.dtypes\n\nGENHLTH      object\nADDEPEV3     object\nACEDEPRS     object\nACEDRINK     object\nACEDRUGS     object\nACEPRISN     object\n_LLCPWT     float64\n_SEX         object\n_AGEG5YR     object\n_SMOKER3     object\ndtype: object\n\n\nLet’s coerce the variables to category data types using .astype().\n\n# coerce to categorical\nsamp_mod6 = samp_mod5.astype('category')\n\n# check new data types\nsamp_mod6.dtypes\n\nGENHLTH     category\nADDEPEV3    category\nACEDEPRS    category\nACEDRINK    category\nACEDRUGS    category\nACEPRISN    category\n_LLCPWT     category\n_SEX        category\n_AGEG5YR    category\n_SMOKER3    category\ndtype: object\n\n\n\n\nQuestion 7: Define ACE indicator variable\nDownstream analysis of ACEs will be facilitated by having an indicator variable that is a 1 if the respondent answered ‘Yes’ to any ACE question, and a 0 otherwise – that way, you can easily count the number of respondents reporting ACEs by summing up the indicator or compute the proportion by taking an average.\nTo this end, define a new logical variable:\n\nadverse_conditions: did the respondent answer yes to any of the adverse childhood condition questions?\n\nYou can accomplish this task in several steps:\n\nObtain a logical array indicating the positions of the ACE variables (hint: use .columns to obtain the column index and operate on the result with .str.startswith(...).). Store this as ace_positions.\nUse the logical array ace_positions to select the ACE columns via .loc[]. Store this as ace_data.\nObtain a dataframe that indicates whether each entry is a ‘Yes’ (hint: use the boolean operator ==, which is a vectorized operation). Store this as ace_yes.\nCompute the row sums using .sum(). Store this as ace_numyes.\nDefine the new variable as ace_numyes > 0.\n\nStore the result as samp_mod7, and print the first few rows using .head().\n\n# BEGIN SOLUTION NO PROMPT\n# copy samp_mod6\nsamp_mod7 = samp_mod6.copy()\n\n# ace column positions\nace_positions = samp_mod7.columns.str.startswith('ACE')\n\n# ace data\nace_data = samp_mod7.loc[:, ace_positions]\n\n# ace yes indicators\nace_yes = (ace_data == 'Yes')\n\n# number of yesses\nace_numyes = ace_yes.sum(axis = 1)\n\n# assign new variable\nsamp_mod7['adverse_conditions'] = (ace_numyes > 0)\n\n# check result\nsamp_mod7.head()\n# END SOLUTION\n\n\"\"\" # BEGIN PROMPT\n# copy samp_mod6\nsamp_mod7 = samp_mod6.copy()\n\n# ace column positions\nace_positions = ...\n\n# ace data\nace_data = ...\n\n# ace yes indicators\nace_yes = ...\n\n# number of yesses\nace_numyes = ...\n\n# assign new variable\nsamp_mod7['adverse_conditions'] = ...\n\n# check result using .head()\n...\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q7\")\n\n\n\nQuestion 8: Define missingness indicator variable\nAs you saw earlier, there are some missing values for the ACE questions. These arise whenever a respondent is not asked these questions. In fact, answers are missing for nearly 80% of the respondents in our subsample. We should keep track of this information. Define a missing indicator:\n\nadverse_missing: is a response missing for at least one of the ACE questions?\n\n\n# BEGIN SOLUTION NO PROMPT\n# copy modification 7\nsamp_mod8 = samp_mod7.copy()\n\n# define missing indicator using loc\nsamp_mod8.loc[:, 'adverse_missing'] = samp_mod8.loc[:, samp_mod8.columns.str.startswith('ACE').tolist()].isna().sum(axis = 1) > 0\n\n# check\nsamp_mod8.head()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# copy modification 7\nsamp_mod8 = samp_mod7.copy()\n\n# define missing indicator using loc\n...\n\n# check using head()\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q8\")\n\n\n\nQuestion 9: Filter respondents who did not answer ACE questions\nSince values are missing for the ACE question if a respondent was not asked, we can remove these observations and do any analysis conditional on respondents having been asked the ACE questions. Use your indicator variable adverse_missing to filter out respondents who were not asked the ACE questions.\nNote that this dramatically limits the scope of inference for subsequent analyses to only those locations where the ACE module was included in the survey.\n\nsamp_mod9 = samp_mod8[~samp_mod8.adverse_missing] #SOLUTION\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10: Define depression indicator variable\nIt will prove similarly helpful to define an indicator for reported depression:\n\ndepression: did the respondent report having been diagnosed with a depressive disorder?\n\nFollow the same strategy as above for the ACE variables, and store the result as samp_mod10. See if you can perform the calculation of the new variable in a single line of code. Print the first few rows using .head().\n\n# BEGIN SOLUTION NO PROMPT\n# copy samp_mod9\nsamp_mod10 = samp_mod9.copy()\n\n# define missing indicator using loc\nsamp_mod10['depression'] = ((samp_mod10.loc[:, 'ADDEPEV3'] == 'Yes') > 0)\n\n# check\nsamp_mod10.head()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# copy samp_mod9\nsamp_mod10 = samp_mod9.copy()\n\n# define new variable using loc\n...\n\n# check using .head()\n...\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q10\")\n\n\n\nQuestion 11: Final dataset\nFor the final dataset, drop the respondent answers to individual questions, the missingness indicator, and select just the derived indicator variables along with general health, sex, age, and smoking status. Check the pandas documentation for .rename() and follow the examples to rename the latter variables:\n\ngeneral_health\nsex\nage\nsmoking\n\nSee if you can perform both operations (slicing and renaming) in a single chain. Store the result as data.\n\nsamp_mod10.columns\n\nIndex(['GENHLTH', 'ADDEPEV3', 'ACEDEPRS', 'ACEDRINK', 'ACEDRUGS', 'ACEPRISN',\n       '_LLCPWT', '_SEX', '_AGEG5YR', '_SMOKER3', 'adverse_conditions',\n       'adverse_missing', 'depression'],\n      dtype='object')\n\n\n\n# BEGIN SOLUTION NO PROMPT\n# slice and rename\ndata = samp_mod10.iloc[:, [0, 7, 8, 9, 10, 12]].rename( #dropping some variables is the same as only selecting the remaining variables\n    columns = {'GENHLTH': 'general_health',\n               '_SEX': 'sex',\n               '_AGEG5YR': 'age',\n               '_SMOKER3': 'smoking'}\n)\n\n# preview\ndata.head()\n\n# END SOLUTION\n\"\"\" # BEGIN PROMPT\n# slice and rename\ndata = ...\n\n# check using .head()\n\n\"\"\"; # END PROMPT\n\n\ngrader.check(\"q11\")\n\n\n\n\nDescriptive analysis\nNow that you have a clean dataset, you’ll use grouping and aggregation to compute several summary statistics that will help you explore whether there is an apparent association between experiencing adverse childhood conditions and self-reported health, smoking status, and depressive disorders in areas where the ACE module was administered.\nThe basic strategy will be to calculate the proportions of respondents who answered yes to one of the adverse experience questions when respondents are grouped by the other variables.\n\nQuestion 12: Proportion of respondents reporting ACEs\nCalculate the overall proportion of respondents in the subsample that reported experiencing at least one adverse condition (given that they answered the ACE questions). Use .mean(); store the result as mean_ace and print.\n\n# proportion of respondents reporting at least one adverse condition\nmean_ace = data.adverse_conditions.mean() #SOLUTION\n\n# print\nmean_ace\n\n0.3070083682008368\n\n\n\ngrader.check(\"q12\")\n\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by general health?\nThe cell below computes the porportion separately by general health self-rating. Notice that the depression variable is dropped so that the result doesn’t also report the proportion of respondents reporting having been diagnosed with a depressive disorder. Notice also that the proportion of missing values for respondents indicating each general health rating is shown.\n\n# proportions grouped by general health\ndata.drop(\n    columns = 'depression'\n).groupby(\n    'general_health'\n).mean(numeric_only = True)\n\n\n\n\n\n  \n    \n      \n      adverse_conditions\n    \n    \n      general_health\n      \n    \n  \n  \n    \n      Excellent\n      0.300000\n    \n    \n      Fair\n      0.355491\n    \n    \n      Good\n      0.299174\n    \n    \n      Poor\n      0.441667\n    \n    \n      Refused\n      0.000000\n    \n    \n      Unsure\n      0.000000\n    \n    \n      Very good\n      0.264957\n    \n  \n\n\n\n\nNotice that the row index lists the general health rating out of order. This can be fixed using a .loc[] call and the dictionary that was defined for the variable coding.\n\n# same as above, rearranging index\nace_health = data.drop(\n    columns = 'depression'\n).groupby(\n    'general_health'\n).mean(\n    numeric_only = True\n).loc[list(health_codes.values()), :]\n\n# print\nace_health\n\n\n\n\n\n  \n    \n      \n      adverse_conditions\n    \n    \n      general_health\n      \n    \n  \n  \n    \n      Excellent\n      0.300000\n    \n    \n      Very good\n      0.264957\n    \n    \n      Good\n      0.299174\n    \n    \n      Fair\n      0.355491\n    \n    \n      Poor\n      0.441667\n    \n    \n      Unsure\n      0.000000\n    \n    \n      Refused\n      0.000000\n    \n  \n\n\n\n\n\n\nQuestion 13: Association between smoking status and ACEs\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by smoking status?\nFollowing the example above for computing the proportion of respondents reporting ACEs by general health rating, calculate the proportion of respondents reporting ACEs by smoking status (be sure to arrange the rows in appropriate order of smoking status) and store as ace_smoking.\n\n# proportions grouped by smoking status\nace_smoking = data.drop(\n    columns = 'depression'\n).groupby(\n    'smoking'\n).mean(\n    numeric_only = True\n).loc[list(smoke_codes.values()), :] #SOLUTION\n\n# print\nace_smoking\n\n\n\n\n\n  \n    \n      \n      adverse_conditions\n    \n    \n      smoking\n      \n    \n  \n  \n    \n      Daily\n      0.453125\n    \n    \n      Some days\n      0.527778\n    \n    \n      Former\n      0.334459\n    \n    \n      Never\n      0.251434\n    \n    \n      Unsure/refused/missing\n      0.100000\n    \n  \n\n\n\n\n\ngrader.check(\"q13\")\n\n\n\nQuestion 14: Association between depression and ACEs\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by smoking status?\nCalculate the proportion of respondents reporting ACEs by whether respondents had been diagnosed with a depressive disorder and store as ace_depr.\n\n# proportions grouped by having experienced depression\nace_depr = data.groupby(\n    'depression'\n).mean(\n    numeric_only = True\n) #SOLUTION\n\n# print\nace_depr\n\n\n\n\n\n  \n    \n      \n      adverse_conditions\n    \n    \n      depression\n      \n    \n  \n  \n    \n      False\n      0.250975\n    \n    \n      True\n      0.537433\n    \n  \n\n\n\n\n\ngrader.check(\"q14\")\n\n\n\nQuestion 15: Exploring subgroupings\nDoes the apparent association between general health and ACEs persist after accounting for sex?\nRepeat the calculation of the proportion of respondents reporting ACEs by general health rating, but also group by sex. Store the result as ace_health_sex.\n\n# group by general health and sex\nace_health_sex = data.drop(\n    columns = 'depression'\n).groupby(\n    ['general_health', 'sex']\n).mean(numeric_only = True) #SOLUTION\n\n\ngrader.check(\"q15\")\n\nThe cell below rearranges the table a little for better readability.\n\n# pivot table for better display\nace_health_sex.reset_index().pivot(columns = 'sex', index = 'general_health', values = 'adverse_conditions').loc[list(health_codes.values()), :]\n\n\n\n\n\n  \n    \n      sex\n      F\n      M\n    \n    \n      general_health\n      \n      \n    \n  \n  \n    \n      Excellent\n      0.328671\n      0.261682\n    \n    \n      Very good\n      0.282123\n      0.237885\n    \n    \n      Good\n      0.308108\n      0.285106\n    \n    \n      Fair\n      0.367150\n      0.338129\n    \n    \n      Poor\n      0.549296\n      0.285714\n    \n    \n      Unsure\n      NaN\n      0.000000\n    \n    \n      Refused\n      0.000000\n      NaN\n    \n  \n\n\n\n\nEven after rearrangement, the table in the last question is a little tricky to read (few people like visually scanning tables). This information would be better displayed in a plot. The example below generates a bar chart showing the summaries you calculated in Q2(d), with the proportion on the y axis, the health rating on the x axis, and separate bars for the two sexes.\n\n# coerce indices to columns for plotting\nplot_df = ace_health_sex.reset_index()\n\n# specify order of general health categories\ngenhealth_order = list(health_codes.values())\nplot_df.general_health.cat.set_categories(genhealth_order, inplace=True)\nplot_df.sort_values([\"general_health\"], inplace=True)\n\n# plot\nalt.Chart(plot_df).mark_bar().encode(\n    x = alt.X('general_health', \n              sort = ['general_health'],\n              title = 'Self-rated general health'),\n    y = alt.Y('adverse_conditions',\n              title = 'Prop. of respondents reporting ACEs'),\n    color = 'sex',\n    column = 'sex'\n).properties(\n    width = 200, \n    height = 200\n)\n\nC:\\Users\\trdea\\AppData\\Local\\Temp\\ipykernel_10156\\2150558614.py:6: FutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object.\n  plot_df.general_health.cat.set_categories(genhealth_order, inplace=True)\n\n\n\n\n\n\n\n\n\n\nQuestion 16: Visualization\nUse the example above to plot the proportion of respondents reporting ACEs against smoking status for men and women.\nHint: you only need to modify the example by substituting smoking status for general health.\n\n# BEGIN SOLUTION NO PROMPT\n# proportions grouped by smoking status\nace_smoking_sex = data.drop(\n    columns = 'depression'\n).groupby(\n    ['smoking', 'sex']\n).mean(numeric_only = True).loc[list(smoke_codes.values()), :]\n\n# coerce indices to columns for plotting\nplot_df = ace_smoking_sex.reset_index()\n\n# specify order of general health categories\nsmoke_order = pd.CategoricalDtype(list(smoke_codes.values()), ordered = True)\nplot_df['smoking'] = plot_df.smoking.astype(smoke_order)\n\n# plot\nalt.Chart(plot_df).mark_bar().encode(\n    x = alt.X('smoking', \n              sort = list(health_codes.values()),\n              title = 'Smoking status'),\n    y = alt.Y('adverse_conditions',\n              title = 'Prop. of respondents reporting ACEs'),\n    color = 'sex',\n    column = 'sex'\n).properties(\n    width = 200, \n    height = 200\n)\n# END SOLUTION\n\n\n\n\n\n\n\n# BEGIN PROMPT\n# dataframe of proportions grouped by smoking status\nace_smoking_sex = ...\n\n# coerce indices to columns for plotting\n...\n\n# specify order of general health categories\n...\n\n# plot\n...\n# END PROMPT\n\nEllipsis\n\n\n\n\n\n\nCommunicating results\nHere you’ll be asked to reflect briefly on your findings.\n\n\nQuestion 17: Summary\nIs there an observed association between reporting ACEs and general health, smoking status, and depression among survey respondents who answered the ACE questions?\nWrite a two to three sentence answer to the above question summarizing your findings. State an answer to the question in your first sentence, and then in your second/third sentences describe exactly what you observed in the foregoing descriptive analysis of the BRFSS data. Be precise, but also concise. There is no need to describe any of the data manipulations, survey design, or the like.\nType your answer here, replacing this text.\nSOLUTION\nYes, there are observed associations between reported adverse childhood experiences and general health, smoking status, and depression. The proportion of respondents reporting ACEs generally increases with smoking frequency for both men and women; there are higher observed rates of ACE reports among respondents in poorer health for both men and women; and there are higher observed rates of ACE reports among respondents with a diagnosed depressive disorder.\n\n\n\n\nQuestion 18: Scope of inference\nRecall from the overview documentation all the care that the BRFSS dedicates to collecting a representative sample of the U.S. adult population with phone numbers. Do you think that your findings provide evidence of an association among the general public (not just the individuals survey)? Why or why not? Answer in two sentences.\nType your answer here, replacing this text.\nSOLUTION\nThe sample is a probability sample of the study population, so results are in principle generalizable; however, many ACE responses were missing because certain states did not ask those questions. As a result, the observed proportions are likely underestimates of the rates among the general public (U.S. adults with phone numbers in private or college housing) and may misrepresent the overall pattern of association. More narrowly, the findings do provide evidence of associations between adverse childhood experiences and health, depression, and smoking among a subset of states.\n\n\n\n\nQuestion 19: Bias\nWhat is a potential source of bias in the survey results, and how might this affect the proportions you’ve calculated?\nAnswer in one or two sentences.\nType your answer here, replacing this text.\nSOLUTION\nAdverse childhood experience is a sensitive matter; respondents may not be comfortable responding truthfully to some of these questions. This would likely produce negative bias – the sample proportions may be underestimates if this is common.\n\n\n\nComment\nNotice that the language ‘association’ is non-causual: we don’t say that ACEs cause (or don’t cause) poorer health outcomes. This is intentional, because the BRFSS data are what are known as ‘observational’ data, i.e. not originating from a controlled experiment. There could be unobserved factors that explain the association.\nTo take a simple example, dog owners live longer, but the reason is simply that dog owners walk more – so it’s the exercise, not the dogs, that cause an increase in longevity. An observational study that doesn’t measure exercise would show a positive association between dog ownership and lifespan, but it’s a non-causal relationship.\n(As an interesting/amusing aside, there is a well known study that established an association between birdkeeping and lung cancer; obviously this is non-causal, yet the study authors recommended that individuals at high risk for cancer avoid ‘avian exposure’, as they were unsure of the mechanism.)\nSo there could easily be unobserved factors that account for the observed association in the BRFSS data. We guard against over-interpreting the results by using causally-neutral language.\n\n\n\nSubmission\n\nSave the notebook.\nRestart the kernel and run all cells. (CAUTION: if your notebook is not saved, you will lose your work.)\nCarefully look through your notebook and verify that all computations execute correctly. You should see no errors; if there are any errors, make sure to correct them before you submit the notebook.\nDownload the notebook as an .ipynb file. This is your backup copy.\nExport the notebook as PDF and upload to Gradescope.\n\n\nTo double-check your work, the cell below will rerun all of the autograder tests.\n\ngrader.check_all()"
  },
  {
    "objectID": "hw/hw1-brfss/hw1-brfss.html",
    "href": "hw/hw1-brfss/hw1-brfss.html",
    "title": "PSTAT100",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport altair as alt\n\n\nBackground\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a long-term effort administered by the CDC to collect data on behaviors affecting physical and mental health, past and present health conditions, and access to healthcare among U.S. residents. The BRFSS comprises telephone surveys of U.S. residents conducted annually since 1984; in the last decade, over half a million interviews have been conducted each year. This is the largest such data collection effort in the world, and many countries have developed similar programs. The objective of the program is to support monitoring and analysis of factors influencing public health in the United States.\nEach year, a standard survey questionnaire is developed that includes a core component comprising questions about: demographic and household information; health-related perceptions, conditions, and behaviors; substance use; and diet. Trained interviewers in each state call randomly selected telephone (landline and cell) numbers and administer the questionnaire; the phone numbers are chosen so as to obtain a representative sample of all households with telephone numbers. Take a moment to read about the 2019 survey here.\nIn this assignment you’ll import and subsample the BRFSS 2019 data and perform a simple descriptive analysis exploring associations between adverse childhood experiences, health perceptions, tobacco use, and depressive disorders. This is an opportunity to practice:\n\nreview of data documentation\ndata assessment and critical thinking about data collection\ndataframe transformations in pandas\ncommunicating and interpreting grouped summaries\n\n\n\nData import and assessment\nThe cell below imports select columns from the 2019 dataset as a pandas DataFrame. The file is big, so this may take a few moments. Run the cell and then have a quick look at the first few rows and columns.\n\n# store variable names of interest\nselected_vars = ['_SEX', '_AGEG5YR', \n                 'GENHLTH', 'ACEPRISN', \n                 'ACEDRUGS', 'ACEDRINK', \n                 'ACEDEPRS', 'ADDEPEV3', \n                 '_SMOKER3', '_LLCPWT']\n\n# import full 2019 BRFSS dataset\nbrfss = pd.read_csv('data/brfss2019.zip', compression = 'zip', usecols = selected_vars)\n\n# invert sampling weights\nbrfss['_LLCPWT'] = 1/brfss._LLCPWT\n\n# print first few rows\nbrfss.head()\n\n\nQuestion 1: Data dimensions\nCheck the dimensions of the dataset. Store the dimensions as nrows and ncolumns.\n\nnrows, ncolumns = ...\n\nprint(nrows, ncolumns)\n\n\ngrader.check(\"q1\")\n\n\n\n\nQuestion 2: Row and column information\nNow that you’ve imported the data, you should verify that the dimensions conform to the format you expect based on data documentation and ensure you understand what each row and each column represents.\nCheck the number of records (interviews conducted) reported and variables measured for 2019 by reviewing the surveillance summaries by year, and then answer the following questions in a few sentences:\n\nDoes the number of rows match the number of reported records?\nHow many columns were imported, and how many columns are reported in the full dataset?\nWhat does each row in the brfss dataframe represent?\nWhat does each column in the brfss dataframe represent\n\nType your answer here, replacing this text.\n\n\n\n\nQuestion 3: Sampling design and data collection\nSkim the overview documentation for the 2019 BRFSS data. Focus specifically the ‘Background’ and ‘Data Collection’ sections, read selectively for relevant details, and answer the following questions in a few sentences:\n\nWho conducts the interviews and how long does a typical interview last?\nWho does an interviewer speak to in each household?\nWhat criteria must a person meet to be interviewed?\nWho can’t appear in the survey? Give two examples.\nWhat is the study population (i.e., all individuals who could possibly be sampled)?\nDoes the data contain any identifying information?\n\nType your answer here, replacing this text.\n\n\n\n\nQuestion 4: Variable descriptions\nYou’ll work with the small subset variables imported above: sex, age, general health self-assessment, smoking status, depressive disorder, and adverse childhood experiences (ACEs). The names of these variables as they appear in the raw dataset are defined in the cell in which you imported the data as selected_vars. It is often useful, and therefore good practice, to include a brief description of each variable at the outset of any reported analyses, both for your own clarity and for that of any potential readers. Open the 2019 BRFSS codebook in your browser and use text searching to locate each of the variable names of interest. Read the codebook entries and fill in the second column in the table below with a one-sentence description of each variable identified in selected_vars. Rephrase the descriptions in your own words – do not copy the codebook descriptions verbatim.\n\n\n\nVariable name\nDescription\n\n\n\n\nGENHLTH\n\n\n\n_SEX\n\n\n\n_AGEG5YR\n\n\n\nACEPRISN\n\n\n\nACEDRUGS\n\n\n\nACEDRINK\n\n\n\nACEDEPRS\n\n\n\nADDEPEV3\n\n\n\n_SMOKER3\n\n\n\n\n\n\n\n\nSubsampling\nTo simplify life a little, we’ll draw a large random sample of the rows and work with that in place of the full dataset. This is known as subsampling.\nThe cell below draws a random subsample of 10k records. Because the subsample is randomly drawn, we should not expect it to vary in any systematic way from the overall dataset, and distinct subsamples should have similar properties – therefore, results downstream should be similar to an analysis of the full dataset, and should also be possible to replicate using distinct subsamples.\n\n# for reproducibility\nnp.random.seed(32221) \n\n# randomly sample 10k records\nsamp = brfss.sample(n = 10000, \n                    replace = False, \n                    weights = '_LLCPWT')\n\nAsides:\n\nNotice that the random number generator seed is set before carrying out this task – this ensures that every time the cell is run, the same subsample is drawn. As a result, the computations in this notebook are reproducible: when I run the notebook on my computer, I get the same results as you get when you run the notebook on your computer.\nNotice also that sampling weights provided with the dataset are used to draw a weighted sample. Some respondents are more likely to be selected than others from the general population of U.S. adults with phone numbers, so the BRFSS calculates derived weights that are inversely proportional to estimates of the probability that the respondent is included in the survey. This is a somewhat sophisticated calculation, however if you’re interested, you can read about how these weights are calculated and why in the overview documentation you used to answer the questions above. We use the sampling weights in drawing the subsample so that we get a representative sample of U.S. adults with phone numbers.\nNotice the missing values. How many entries are missing in each column? The cell below computes the proportion of missing values for each of the selected variables. We’ll return to this issue later on.\n\n\n# proportions of missingness \nsamp.isna().mean()\n\n\n\nTidying\nIn the following series of questions you’ll tidy up the subsample by performing these steps:\n\nselecting columns of interest;\nreplacing coded values of question responses with responses;\ndefining new variables based on existing ones;\nrenaming columns.\n\nThe goal of this is to produce a clean version of the dataset that is well-organized, intuitive to navigate, and ready for analysis.\nThe variable entries are coded numerically to represent certain responses. These should be replaced by more informative entries. We can use the codebook to determine which number means what, and replace the values accordingly.\nThe cell below replaces the numeric values for _AGEG5YR by their meanings, illustrating how to use .replace() with a dictionary to convert the numeric coding to interpretable values. The basic strategy is:\n\nStore the variable coding for VAR as a dictionary var_codes.\nUse .replace({'VAR': var_codes}) to modify values.\n\nIf you need additional examples, check the pandas documentation for .replace().\n\n# dictionary representing variable coding\nage_codes = {\n    1: '18-24', 2: '25-29', 3: '30-34',\n    4: '35-39', 5: '40-44', 6: '45-49',\n    7: '50-54', 8: '55-59', 9: '60-64',\n    10: '65-69', 11: '70-74', 12: '75-79',\n    13: '80+', 14: 'Unsure/refused/missing'\n}\n\n# recode age categories\nsamp_mod1 = samp.replace({'_AGEG5YR': age_codes})\n\n# check result\nsamp_mod1.head()\n\n\nQuestion 5: Recoding variables\nFollowing the example immediately above and referring to the 2019 BRFSS codebook, replace the numeric codings with response categories for each of the following variables:\n\n_SEX\nGENHLTH\n_SMOKER3\n\nNotice that above, the first modification (slicing) was stored as samp_mod1, and was a function of samp. You’ll follow this pattern, creating samp_mod2, samp_mod3, and so on so that each step (modification) of your data manipulations is stored separately, for easy troubleshooting.\n\nRecode _SEX: define a new dataframe samp_mod2 that is the same as samp_mod1 but with the _SEX variable recoded as M and F.\nRecode GENHLTH: define a new dataframe samp_mod3 that is the same as samp_mod2 but with the GENHLTH variable recoded as Excellent, Very good, Good, Fair, Poor, Unsure, and Refused.\nRecode _SMOKER3: define a new dataframe samp_mod4 that is the same as samp_mod3 but with _SMOKER3 recoded as Daily, Some days, Former, Never, and Unsure/refused/missing.\nPrint the first few rows of samp_mod4.\n\n\n# define dictionary for sex\nsex_codes = ...\n\n# recode sex\nsamp_mod2 = ...\n\n# define dictionary for health\nhealth_codes = ...\n\n# recode health\nsamp_mod3 = ...\n\n# define dictionary for smoking\nsmoke_codes = ...\n\n# recode smoking\nsamp_mod4 = ...\n\n# print a few rows\n...\n\n\ngrader.check(\"q5\")\n\n\n\nQuestion 6: Value replacement\nNow all the variables except the adverse childhood experience and depressive disorder question responses are represented interpretably. In the codebook that the answer key is identical for these remaining variables.\nThe numeric codings can be replaced all at once by applying .replace() to the dataframe with an argument of the form\n\ndf.replace({'var1': varcodes1, 'var2': varcodes1, ..., 'varp': varcodesp})\n\nDefine a new dataframe samp_mod5 that is the same as samp_mod4 but with the remaining variables recoded according to the answer key Yes, No, Unsure, Refused. Print the first few rows of the result using .head().\n\n# define dictionary\nanswer_codes = ...\n\n# recode\nsamp_mod5 = ...\n\n# check using head()\n...\n\n\ngrader.check(\"q6\")\n\nFinally, all the variables in the dataset are categorical. Notice that the current data types do not reflect this.\n\nsamp_mod5.dtypes\n\nLet’s coerce the variables to category data types using .astype().\n\n# coerce to categorical\nsamp_mod6 = samp_mod5.astype('category')\n\n# check new data types\nsamp_mod6.dtypes\n\n\n\nQuestion 7: Define ACE indicator variable\nDownstream analysis of ACEs will be facilitated by having an indicator variable that is a 1 if the respondent answered ‘Yes’ to any ACE question, and a 0 otherwise – that way, you can easily count the number of respondents reporting ACEs by summing up the indicator or compute the proportion by taking an average.\nTo this end, define a new logical variable:\n\nadverse_conditions: did the respondent answer yes to any of the adverse childhood condition questions?\n\nYou can accomplish this task in several steps:\n\nObtain a logical array indicating the positions of the ACE variables (hint: use .columns to obtain the column index and operate on the result with .str.startswith(...).). Store this as ace_positions.\nUse the logical array ace_positions to select the ACE columns via .loc[]. Store this as ace_data.\nObtain a dataframe that indicates whether each entry is a ‘Yes’ (hint: use the boolean operator ==, which is a vectorized operation). Store this as ace_yes.\nCompute the row sums using .sum(). Store this as ace_numyes.\nDefine the new variable as ace_numyes > 0.\n\nStore the result as samp_mod7, and print the first few rows using .head().\n\n\n# copy samp_mod6\nsamp_mod7 = samp_mod6.copy()\n\n# ace column positions\nace_positions = ...\n\n# ace data\nace_data = ...\n\n# ace yes indicators\nace_yes = ...\n\n# number of yesses\nace_numyes = ...\n\n# assign new variable\nsamp_mod7['adverse_conditions'] = ...\n\n# check result using .head()\n...\n\n\ngrader.check(\"q7\")\n\n\n\nQuestion 8: Define missingness indicator variable\nAs you saw earlier, there are some missing values for the ACE questions. These arise whenever a respondent is not asked these questions. In fact, answers are missing for nearly 80% of the respondents in our subsample. We should keep track of this information. Define a missing indicator:\n\nadverse_missing: is a response missing for at least one of the ACE questions?\n\n\n# copy modification 7\nsamp_mod8 = samp_mod7.copy()\n\n# define missing indicator using loc\n...\n\n# check using head()\n\n\ngrader.check(\"q8\")\n\n\n\nQuestion 9: Filter respondents who did not answer ACE questions\nSince values are missing for the ACE question if a respondent was not asked, we can remove these observations and do any analysis conditional on respondents having been asked the ACE questions. Use your indicator variable adverse_missing to filter out respondents who were not asked the ACE questions.\nNote that this dramatically limits the scope of inference for subsequent analyses to only those locations where the ACE module was included in the survey.\n\nsamp_mod9 = ...\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10: Define depression indicator variable\nIt will prove similarly helpful to define an indicator for reported depression:\n\ndepression: did the respondent report having been diagnosed with a depressive disorder?\n\nFollow the same strategy as above for the ACE variables, and store the result as samp_mod10. See if you can perform the calculation of the new variable in a single line of code. Print the first few rows using .head().\n\n# copy samp_mod9\nsamp_mod10 = samp_mod9.copy()\n\n# define new variable using loc\n...\n\n# check using .head()\n...\n\n\ngrader.check(\"q10\")\n\n\n\nQuestion 11: Final dataset\nFor the final dataset, drop the respondent answers to individual questions, the missingness indicator, and select just the derived indicator variables along with general health, sex, age, and smoking status. Check the pandas documentation for .rename() and follow the examples to rename the latter variables:\n\ngeneral_health\nsex\nage\nsmoking\n\nSee if you can perform both operations (slicing and renaming) in a single chain. Store the result as data.\n\nsamp_mod10.columns\n\n\n# slice and rename\ndata = ...\n\n# check using .head()\n\n\ngrader.check(\"q11\")\n\n\n\n\nDescriptive analysis\nNow that you have a clean dataset, you’ll use grouping and aggregation to compute several summary statistics that will help you explore whether there is an apparent association between experiencing adverse childhood conditions and self-reported health, smoking status, and depressive disorders in areas where the ACE module was administered.\nThe basic strategy will be to calculate the proportions of respondents who answered yes to one of the adverse experience questions when respondents are grouped by the other variables.\n\nQuestion 12: Proportion of respondents reporting ACEs\nCalculate the overall proportion of respondents in the subsample that reported experiencing at least one adverse condition (given that they answered the ACE questions). Use .mean(); store the result as mean_ace and print.\n\n# proportion of respondents reporting at least one adverse condition\nmean_ace = ...\n\n# print\nmean_ace\n\n\ngrader.check(\"q12\")\n\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by general health?\nThe cell below computes the porportion separately by general health self-rating. Notice that the depression variable is dropped so that the result doesn’t also report the proportion of respondents reporting having been diagnosed with a depressive disorder. Notice also that the proportion of missing values for respondents indicating each general health rating is shown.\n\n# proportions grouped by general health\ndata.drop(\n    columns = 'depression'\n).groupby(\n    'general_health'\n).mean(numeric_only = True)\n\nNotice that the row index lists the general health rating out of order. This can be fixed using a .loc[] call and the dictionary that was defined for the variable coding.\n\n# same as above, rearranging index\nace_health = data.drop(\n    columns = 'depression'\n).groupby(\n    'general_health'\n).mean(\n    numeric_only = True\n).loc[list(health_codes.values()), :]\n\n# print\nace_health\n\n\n\nQuestion 13: Association between smoking status and ACEs\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by smoking status?\nFollowing the example above for computing the proportion of respondents reporting ACEs by general health rating, calculate the proportion of respondents reporting ACEs by smoking status (be sure to arrange the rows in appropriate order of smoking status) and store as ace_smoking.\n\n# proportions grouped by smoking status\nace_smoking = data.drop(\n    columns = 'depression'\n).groupby(\n    'smoking'\n).mean(\n    numeric_only = True\n...\n\n# print\nace_smoking\n\n\ngrader.check(\"q13\")\n\n\n\nQuestion 14: Association between depression and ACEs\nDoes the proportion of respondents who reported experiencing adverse childhood conditions vary by smoking status?\nCalculate the proportion of respondents reporting ACEs by whether respondents had been diagnosed with a depressive disorder and store as ace_depr.\n\n# proportions grouped by having experienced depression\nace_depr = data.groupby(\n    'depression'\n).mean(\n    numeric_only = True\n...\n\n# print\nace_depr\n\n\ngrader.check(\"q14\")\n\n\n\nQuestion 15: Exploring subgroupings\nDoes the apparent association between general health and ACEs persist after accounting for sex?\nRepeat the calculation of the proportion of respondents reporting ACEs by general health rating, but also group by sex. Store the result as ace_health_sex.\n\n# group by general health and sex\nace_health_sex = data.drop(\n    columns = 'depression'\n).groupby(\n    ['general_health', 'sex']\n...\n\n\ngrader.check(\"q15\")\n\nThe cell below rearranges the table a little for better readability.\n\n# pivot table for better display\nace_health_sex.reset_index().pivot(columns = 'sex', index = 'general_health', values = 'adverse_conditions').loc[list(health_codes.values()), :]\n\nEven after rearrangement, the table in the last question is a little tricky to read (few people like visually scanning tables). This information would be better displayed in a plot. The example below generates a bar chart showing the summaries you calculated in Q2(d), with the proportion on the y axis, the health rating on the x axis, and separate bars for the two sexes.\n\n# coerce indices to columns for plotting\nplot_df = ace_health_sex.reset_index()\n\n# specify order of general health categories\ngenhealth_order = list(health_codes.values())\nplot_df.general_health.cat.set_categories(genhealth_order, inplace=True)\nplot_df.sort_values([\"general_health\"], inplace=True)\n\n# plot\nalt.Chart(plot_df).mark_bar().encode(\n    x = alt.X('general_health', \n              sort = ['general_health'],\n              title = 'Self-rated general health'),\n    y = alt.Y('adverse_conditions',\n              title = 'Prop. of respondents reporting ACEs'),\n    color = 'sex',\n    column = 'sex'\n).properties(\n    width = 200, \n    height = 200\n)\n\n\n\n\nQuestion 16: Visualization\nUse the example above to plot the proportion of respondents reporting ACEs against smoking status for men and women.\nHint: you only need to modify the example by substituting smoking status for general health.\n\n# dataframe of proportions grouped by smoking status\nace_smoking_sex = ...\n\n# coerce indices to columns for plotting\n...\n\n# specify order of general health categories\n...\n\n# plot\n...\n\n\n\n\n\nCommunicating results\nHere you’ll be asked to reflect briefly on your findings.\n\n\nQuestion 17: Summary\nIs there an observed association between reporting ACEs and general health, smoking status, and depression among survey respondents who answered the ACE questions?\nWrite a two to three sentence answer to the above question summarizing your findings. State an answer to the question in your first sentence, and then in your second/third sentences describe exactly what you observed in the foregoing descriptive analysis of the BRFSS data. Be precise, but also concise. There is no need to describe any of the data manipulations, survey design, or the like.\nType your answer here, replacing this text.\n\n\n\n\nQuestion 18: Scope of inference\nRecall from the overview documentation all the care that the BRFSS dedicates to collecting a representative sample of the U.S. adult population with phone numbers. Do you think that your findings provide evidence of an association among the general public (not just the individuals survey)? Why or why not? Answer in two sentences.\nType your answer here, replacing this text.\n\n\n\n\nQuestion 19: Bias\nWhat is a potential source of bias in the survey results, and how might this affect the proportions you’ve calculated?\nAnswer in one or two sentences.\nType your answer here, replacing this text.\n\n\n\nComment\nNotice that the language ‘association’ is non-causual: we don’t say that ACEs cause (or don’t cause) poorer health outcomes. This is intentional, because the BRFSS data are what are known as ‘observational’ data, i.e. not originating from a controlled experiment. There could be unobserved factors that explain the association.\nTo take a simple example, dog owners live longer, but the reason is simply that dog owners walk more – so it’s the exercise, not the dogs, that cause an increase in longevity. An observational study that doesn’t measure exercise would show a positive association between dog ownership and lifespan, but it’s a non-causal relationship.\n(As an interesting/amusing aside, there is a well known study that established an association between birdkeeping and lung cancer; obviously this is non-causal, yet the study authors recommended that individuals at high risk for cancer avoid ‘avian exposure’, as they were unsure of the mechanism.)\nSo there could easily be unobserved factors that account for the observed association in the BRFSS data. We guard against over-interpreting the results by using causally-neutral language.\n\n\n\nSubmission\n\nSave the notebook.\nRestart the kernel and run all cells. (CAUTION: if your notebook is not saved, you will lose your work.)\nCarefully look through your notebook and verify that all computations execute correctly. You should see no errors; if there are any errors, make sure to correct them before you submit the notebook.\nDownload the notebook as an .ipynb file. This is your backup copy.\nExport the notebook as PDF and upload to Gradescope.\n\n\nTo double-check your work, the cell below will rerun all of the autograder tests.\n\ngrader.check_all()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Concepts and Analysis",
    "section": "",
    "text": "This is the course website for UCSB’s Data Science Concepts and Analysis class (PSTAT100). Content is directed towards currently enrolled students. Please ask permission before using course materials in any other capacity."
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab0-gettingstarted.ipynb\")"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#jupyter-notebooks",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#jupyter-notebooks",
    "title": "PSTAT100",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nJupyter notebooks are organized into ‘cells’ that can contain either text or codes. For example, this is a text cell.\nTechnically, Jupyter is an application/interface that runs atop a kernel – a programming-language-specific independent environment in which code cells are executed. This basic organization allows for interactive computing with text integration.\nSelecting a cell and pressing Enter will enter edit mode and allow you to edit the cell. From edit mode, pressing Esc will revert to command mode and allow you to navigate the notebook’s cells.\nIn edit mode, most of the keyboard is dedicated to typing into the cell’s editor. Thus, in edit mode there are relatively few shortcuts. In command mode, the entire keyboard is available for shortcuts, so there are many more. Here are a few useful ones:\n\nCtrl + Return : Evaluate the current cell\nShift + Return: Evaluate the current cell and move to the next\nSaving the notebook: s\nBasic navigation: up one cell k, down one cell j\na : create a cell above\nb : create a cell below\ndd : delete a cell\nz : undo the last cell operation\nm : convert a cell to markdown\ny : convert a cell to code\n\nTake a moment to find out what the following commands do:\n\nCell editing: x, c, v, z\nKernel operations: i, 0 (press twice)\n\n\n# Practice the above commands on this cell\n\n\nRunning Cells and Displaying Output\nRun the following cell.\n\nprint(\"Hello, World!\")\n\nHello, World!\n\n\nIn Jupyter notebooks, all print statements are displayed below the cell. Furthermore, the output of only the last line is displayed following the cell upon execution.\n\n\"Will this line be displayed?\"\n\nprint(\"Hello\" + \",\", \"world!\")\n\n5 + 3\n\nHello, world!\n\n\n8\n\n\n\n\nViewing Documentation\nTo output the documentation for a function, use the help() function.\n\nhelp(print)\n\nHelp on built-in function print in module builtins:\n\nprint(...)\n    print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\n    \n    Prints the values to a stream, or to sys.stdout by default.\n    Optional keyword arguments:\n    file:  a file-like object (stream); defaults to the current sys.stdout.\n    sep:   string inserted between values, default a space.\n    end:   string appended after the last value, default a newline.\n    flush: whether to forcibly flush the stream.\n\n\n\nYou can also use Jupyter to view function documentation inside your notebook. The function must already be defined in the kernel for this to work.\nBelow, click your mouse anywhere on print() and use Shift + Tab to view the function’s documentation.\n\nprint('Welcome to this course!')\n\nWelcome to this course!\n\n\n\n\nImporting Libraries\nIn this course, we will be using common Python libraries to help us retrieve, manipulate, and perform operations on data. By convention, we import all libraries at the very top of the notebook. There are also a set of standard aliases that are used to shorten the library names. Below are some of the libraries that you may encounter throughout the course, along with their respective aliases.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#practice-questions-and-numpy-review",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted-soln.html#practice-questions-and-numpy-review",
    "title": "PSTAT100",
    "section": "Practice questions and numpy review",
    "text": "Practice questions and numpy review\nMost assignments for this class will be given as notebooks organized into explanation and prompts followed by response cells; you will complete assignments by filling in all of the response cells.\nMany response cells are followed by a test cell that performs a few checks on your work. Please be aware that test cells don’t always confirm that your response is correct or incorrect. They are meant to give you some useful feedback, but it’s your responsibility to interpret the feedback – please be sure to read and think about test output if tests fail, and make your own assessment of whether you need to revise your response.\nBelow are a few practice questions for you to familiarize yourself with the process. These assume familiarity with basic python syntax and the numpy package.\n\nQuestion 1\nWrite a function summation that evaluates the following summation for \\(n \\geq 1\\):\n\\[\\sum_{i=1}^{n} \\left(i^3 + 5 i^3\\right)\\]\nHint: np.arange(5).sum() will generate an array comprising \\(1, 2, \\dots, 5\\) and then add up the elements of the array.\n\ndef summation(n):\n    \"\"\"Compute the summation i^3 + 5 * i^3 for 1 <= i <= n.\"\"\"\n    # BEGIN SOLUTION\n    out = (6*(np.arange(n + 1)**3)).sum()\n    return out\n    # END SOLUTION\n\n\ngrader.check(\"q1\")\n\nUse your function to compute the sum for…\n\n# n = 2\n...\n\n\n# n = 20\n...\n\n\n\nQuestion 2\nThe core of numpy is the array. Let’s use np.array to create an array. It takes a sequence, such as a list or range (remember that list elements are included between the square brackets [ and ], such as [1, 5, 3]).\nBelow, create an array containing the values 1, 2, 3, 4, and 5 (in that order) and assign it the name my_array.\n\nmy_array = np.array([1, 2, 3, 4, 5]) #SOLUTION\n\n\ngrader.check(\"q2\")\n\nNumpy arrays are integer-indexed by position, with the first element indexed as position 0. Elements can be retrieved by enclosing the desired positions in brackets [].\n\nmy_array[3]\n\n4\n\n\nTo retrieve consecutive positions, specify the starting index and the ending index separated by :, for instance, arr[from:to]. This syntax is non-inclusive of the left endpoint, meaning that the starting index is not included in the output.\n\nmy_array[2:4]\n\narray([3, 4])\n\n\nIn addition to values in the array, we can access attributes such as array’s shape and data type that can be retrieved by name using syntax of the form array.attr. Some useful attributes are:\n\n.shape, a tuple with the length of each array dimension\n.size, the length of the first array dimension\n.dtype, the data type of the entries (float, integer, etc.)\n\nA full list of attributes is here.\n\nmy_array.shape\n\n(5,)\n\n\n\nmy_array.size\n\n5\n\n\n\nmy_array.dtype\n\ndtype('int32')\n\n\nArrays, unlike Python lists, cannot store items of different data types.\n\n# A regular Python list can store items of different data types\n[1, '3']\n\n[1, '3']\n\n\n\n# Arrays will convert everything to the same data type\nnp.array([1, '3'])\n\narray(['1', '3'], dtype='<U11')\n\n\n\n# Another example of array type conversion\nnp.array([5, 8.3])\n\narray([5. , 8.3])\n\n\nArrays are also useful in performing vectorized operations. Given two or more arrays of equal length, arithmetic will perform element-wise computations across the arrays.\nFor example, observe the following:\n\n# Python list addition will concatenate the two lists\n[1, 2, 3] + [4, 5, 6]\n\n[1, 2, 3, 4, 5, 6]\n\n\n\n# NumPy array addition will add them element-wise\nnp.array([1, 2, 3]) + np.array([4, 5, 6])\n\narray([5, 7, 9])\n\n\nArrays can be subsetted by index position, as shown above, or by a logical vector of the same length. For example:\n\nexample_arr = np.arange(4, 10)\nexample_arr\n\narray([4, 5, 6, 7, 8, 9])\n\n\nSuppose we want the last three elements. One option is to use index position:\n\nexample_arr[3:6]\n\narray([7, 8, 9])\n\n\nOr a logical vector:\n\nexample_arr[np.array([False, False, False, True, True, True])]\n\narray([7, 8, 9])\n\n\nThe latter approach allows one to subset based on a condition defined by the values of the vector. For example, we can use the condition \\(x \\geq 7\\) to obtain the logical vector used above.\n\nexample_arr >= 7\n\narray([False, False, False,  True,  True,  True])\n\n\nAnd then we can subset just as before:\n\nexample_arr[example_arr >= 7]\n\narray([7, 8, 9])\n\n\nYou’ll see this done frequently, and it’s sometimes referred to as filtering, because we’re selectively removing values.\n\n\nQuestion 3\nGiven the array random_arr, create an array containing all values \\(x\\) such that \\(2x^4 > 1\\). Name the array valid_values.\n\n# for reproducibility - setting the seed will result in the same random draw each time\nnp.random.seed(42)\n\n# draw 60 uniformly random integers between 0 and 1\nrandom_arr = np.random.rand(60)\n\n# solution here\nvalid_values = random_arr[2*(random_arr**4) > 1] # SOLUTION\n\n\ngrader.check(\"q3\")\n\n\n\nA note on np.arange and np.linspace\nUsually we use np.arange to return an array that steps from a to b with a fixed step size s. While this is fine in some cases, we sometimes prefer to use np.linspace(a, b, N), which divides the interval [a, b] into N equally spaced points.\nnp.arange(start, stop, step) produces an array with all the numbers starting at start, incremendted up by step, stopping before stop is reached. For example, the value of np.arange(1, 6, 2) is an array with elements 1, 3, and 5 – it starts at 1 and counts up by 2, then stops before 6. np.arange(4, 9, 1) is an array with elements 4, 5, 6, 7, and 8. (It doesn’t contain 9 because np.arange stops before the stop value is reached.)\nnp.linspace always includes both end points while np.arange will not include the second end point b. For this reason, especially when we are plotting ranges of values we tend to prefer np.linspace.\nNotice how the following two statements have different parameters but return the same result.\n\nnp.arange(-5, 6, 1.0)\n\narray([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])\n\n\n\nnp.linspace(-5, 5, 11)\n\narray([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])\n\n\nCheck your understanding. Will np.arange(1, 10) produce an array that contains 10? Add a cell below and check to confirm your answer."
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab0-gettingstarted.ipynb\")"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html#jupyter-notebooks",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html#jupyter-notebooks",
    "title": "PSTAT100",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nJupyter notebooks are organized into ‘cells’ that can contain either text or codes. For example, this is a text cell.\nTechnically, Jupyter is an application/interface that runs atop a kernel – a programming-language-specific independent environment in which code cells are executed. This basic organization allows for interactive computing with text integration.\nSelecting a cell and pressing Enter will enter edit mode and allow you to edit the cell. From edit mode, pressing Esc will revert to command mode and allow you to navigate the notebook’s cells.\nIn edit mode, most of the keyboard is dedicated to typing into the cell’s editor. Thus, in edit mode there are relatively few shortcuts. In command mode, the entire keyboard is available for shortcuts, so there are many more. Here are a few useful ones:\n\nCtrl + Return : Evaluate the current cell\nShift + Return: Evaluate the current cell and move to the next\nSaving the notebook: s\nBasic navigation: up one cell k, down one cell j\na : create a cell above\nb : create a cell below\ndd : delete a cell\nz : undo the last cell operation\nm : convert a cell to markdown\ny : convert a cell to code\n\nTake a moment to find out what the following commands do:\n\nCell editing: x, c, v, z\nKernel operations: i, 0 (press twice)\n\n\n# Practice the above commands on this cell\n\n\nRunning Cells and Displaying Output\nRun the following cell.\n\nprint(\"Hello, World!\")\n\nIn Jupyter notebooks, all print statements are displayed below the cell. Furthermore, the output of only the last line is displayed following the cell upon execution.\n\n\"Will this line be displayed?\"\n\nprint(\"Hello\" + \",\", \"world!\")\n\n5 + 3\n\n\n\nViewing Documentation\nTo output the documentation for a function, use the help() function.\n\nhelp(print)\n\nYou can also use Jupyter to view function documentation inside your notebook. The function must already be defined in the kernel for this to work.\nBelow, click your mouse anywhere on print() and use Shift + Tab to view the function’s documentation.\n\nprint('Welcome to this course!')\n\n\n\nImporting Libraries\nIn this course, we will be using common Python libraries to help us retrieve, manipulate, and perform operations on data. By convention, we import all libraries at the very top of the notebook. There are also a set of standard aliases that are used to shorten the library names. Below are some of the libraries that you may encounter throughout the course, along with their respective aliases.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "labs/lab0-gettingstarted/lab0-gettingstarted.html#practice-questions-and-numpy-review",
    "href": "labs/lab0-gettingstarted/lab0-gettingstarted.html#practice-questions-and-numpy-review",
    "title": "PSTAT100",
    "section": "Practice questions and numpy review",
    "text": "Practice questions and numpy review\nMost assignments for this class will be given as notebooks organized into explanation and prompts followed by response cells; you will complete assignments by filling in all of the response cells.\nMany response cells are followed by a test cell that performs a few checks on your work. Please be aware that test cells don’t always confirm that your response is correct or incorrect. They are meant to give you some useful feedback, but it’s your responsibility to interpret the feedback – please be sure to read and think about test output if tests fail, and make your own assessment of whether you need to revise your response.\nBelow are a few practice questions for you to familiarize yourself with the process. These assume familiarity with basic python syntax and the numpy package.\n\nQuestion 1\nWrite a function summation that evaluates the following summation for \\(n \\geq 1\\):\n\\[\\sum_{i=1}^{n} \\left(i^3 + 5 i^3\\right)\\]\nHint: np.arange(5).sum() will generate an array comprising \\(1, 2, \\dots, 5\\) and then add up the elements of the array.\n\ndef summation(n):\n    \"\"\"Compute the summation i^3 + 5 * i^3 for 1 <= i <= n.\"\"\"\n    ...\n\n\ngrader.check(\"q1\")\n\nUse your function to compute the sum for…\n\n# n = 2\n...\n\n\n# n = 20\n...\n\n\n\nQuestion 2\nThe core of numpy is the array. Let’s use np.array to create an array. It takes a sequence, such as a list or range (remember that list elements are included between the square brackets [ and ], such as [1, 5, 3]).\nBelow, create an array containing the values 1, 2, 3, 4, and 5 (in that order) and assign it the name my_array.\n\nmy_array = ...\n\n\ngrader.check(\"q2\")\n\nNumpy arrays are integer-indexed by position, with the first element indexed as position 0. Elements can be retrieved by enclosing the desired positions in brackets [].\n\nmy_array[3]\n\nTo retrieve consecutive positions, specify the starting index and the ending index separated by :, for instance, arr[from:to]. This syntax is non-inclusive of the left endpoint, meaning that the starting index is not included in the output.\n\nmy_array[2:4]\n\nIn addition to values in the array, we can access attributes such as array’s shape and data type that can be retrieved by name using syntax of the form array.attr. Some useful attributes are:\n\n.shape, a tuple with the length of each array dimension\n.size, the length of the first array dimension\n.dtype, the data type of the entries (float, integer, etc.)\n\nA full list of attributes is here.\n\nmy_array.shape\n\n\nmy_array.size\n\n\nmy_array.dtype\n\nArrays, unlike Python lists, cannot store items of different data types.\n\n# A regular Python list can store items of different data types\n[1, '3']\n\n\n# Arrays will convert everything to the same data type\nnp.array([1, '3'])\n\n\n# Another example of array type conversion\nnp.array([5, 8.3])\n\nArrays are also useful in performing vectorized operations. Given two or more arrays of equal length, arithmetic will perform element-wise computations across the arrays.\nFor example, observe the following:\n\n# Python list addition will concatenate the two lists\n[1, 2, 3] + [4, 5, 6]\n\n\n# NumPy array addition will add them element-wise\nnp.array([1, 2, 3]) + np.array([4, 5, 6])\n\nArrays can be subsetted by index position, as shown above, or by a logical vector of the same length. For example:\n\nexample_arr = np.arange(4, 10)\nexample_arr\n\nSuppose we want the last three elements. One option is to use index position:\n\nexample_arr[3:6]\n\nOr a logical vector:\n\nexample_arr[np.array([False, False, False, True, True, True])]\n\nThe latter approach allows one to subset based on a condition defined by the values of the vector. For example, we can use the condition \\(x \\geq 7\\) to obtain the logical vector used above.\n\nexample_arr >= 7\n\nAnd then we can subset just as before:\n\nexample_arr[example_arr >= 7]\n\nYou’ll see this done frequently, and it’s sometimes referred to as filtering, because we’re selectively removing values.\n\n\nQuestion 3\nGiven the array random_arr, create an array containing all values \\(x\\) such that \\(2x^4 > 1\\). Name the array valid_values.\n\n# for reproducibility - setting the seed will result in the same random draw each time\nnp.random.seed(42)\n\n# draw 60 uniformly random integers between 0 and 1\nrandom_arr = np.random.rand(60)\n\n# solution here\nvalid_values = ...\n\n\ngrader.check(\"q3\")\n\n\n\nA note on np.arange and np.linspace\nUsually we use np.arange to return an array that steps from a to b with a fixed step size s. While this is fine in some cases, we sometimes prefer to use np.linspace(a, b, N), which divides the interval [a, b] into N equally spaced points.\nnp.arange(start, stop, step) produces an array with all the numbers starting at start, incremendted up by step, stopping before stop is reached. For example, the value of np.arange(1, 6, 2) is an array with elements 1, 3, and 5 – it starts at 1 and counts up by 2, then stops before 6. np.arange(4, 9, 1) is an array with elements 4, 5, 6, 7, and 8. (It doesn’t contain 9 because np.arange stops before the stop value is reached.)\nnp.linspace always includes both end points while np.arange will not include the second end point b. For this reason, especially when we are plotting ranges of values we tend to prefer np.linspace.\nNotice how the following two statements have different parameters but return the same result.\n\nnp.arange(-5, 6, 1.0)\n\n\nnp.linspace(-5, 5, 11)\n\nCheck your understanding. Will np.arange(1, 10) produce an array that contains 10? Add a cell below and check to confirm your answer."
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab1-pandas.ipynb\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#creating-dataframes-basic-manipulations",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#creating-dataframes-basic-manipulations",
    "title": "PSTAT100",
    "section": "Creating DataFrames & Basic Manipulations",
    "text": "Creating DataFrames & Basic Manipulations\nA dataframe is a table in which each column has a type; there is an index over the columns (typically string labels) and an index over the rows (typically ordinal numbers). An index is represented by a series object, which is a one-dimensional labeled array. Here you’ll cover:\n\ncreating dataframes from scratch;\nretrieving attributes;\ndataframe indexing;\nadding, removing, and renaming columns.\n\n\nCreating dataframes from scratch\nThe documentation for the pandas DataFrame class provide two primary syntaxes to create a data frame from scratch:\n\nfrom a dictionary\nrow-wise tuples\n\nSyntax 1 (dictionary): You can create a data frame by specifying the columns and values using a dictionary (a concatenation of named lists) as shown below.\nThe keys of the dictionary are the column names, and the values of the dictionary are lists containing the row entries.\n\n# define a dataframe using dictionary syntax\nfruit_info = pd.DataFrame( \n    data = { 'fruit': ['apple', 'orange', 'banana', 'raspberry'],\n             'color': ['red', 'orange', 'yellow', 'pink']\n           })\n\n# print\nfruit_info\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      0\n      apple\n      red\n    \n    \n      1\n      orange\n      orange\n    \n    \n      2\n      banana\n      yellow\n    \n    \n      3\n      raspberry\n      pink\n    \n  \n\n\n\n\nSyntax 2 (row tuples): You can also define a dataframe by specifying the rows as tuples.\nEach row corresponds to a distinct tuple, and the column indices are specified separately.\n\n# define the same dataframe using tuple syntax \nfruit_info2 = pd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"]\n)\n\n# print\nfruit_info2\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      0\n      apple\n      red\n    \n    \n      1\n      orange\n      orange\n    \n    \n      2\n      banana\n      yellow\n    \n    \n      3\n      raspberry\n      pink\n    \n  \n\n\n\n\n\n\nDataframe Attributes\nDataFrames have several basic attributes:\n\n.shape contains dimensions;\n.dtypes contains data types (float, integer, object, etc.)\n.size first (row) dimension;\n.values contains an array comprising each entry in the dataframe.\n.columns contains the column index;\n.index contains the row index.\n\nYou can obtain these attributes by appending the attribute name to the dataframe name. For instance, the dimensions of a dataframe df can be retrieved by df.shape.\n\n# dimensions\nfruit_info.shape\n\n(4, 2)\n\n\nTo retrieve a two-dimensional numpy array with the values of the dataframe, use df.values. It is sometimes useful to extract this data structure in order to perform vectorized operations, linear algebra, and the like.\n\n# as array\nfruit_info.values\n\narray([['apple', 'red'],\n       ['orange', 'orange'],\n       ['banana', 'yellow'],\n       ['raspberry', 'pink']], dtype=object)\n\n\n\n\nDataframe Indexing\nThe entries in a dataframe are indexed. Indices for rows and columns are stored as the .index. and .columns attributes, respectively.\n\nfruit_info.columns\n\nIndex(['fruit', 'color'], dtype='object')\n\n\n\nfruit_info.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\nBy default, the row indexing is simply numbering by consecutive integers.\n\nfruit_info.index.values\n\narray([0, 1, 2, 3], dtype=int64)\n\n\nHowever, rows can alternatively be indexed by labels:\n\n# define with a row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = [\"fruit 1\", \"fruit 2\", \"fruit 3\", 'fruit 4']\n)\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      fruit 1\n      apple\n      red\n    \n    \n      fruit 2\n      orange\n      orange\n    \n    \n      fruit 3\n      banana\n      yellow\n    \n    \n      fruit 4\n      raspberry\n      pink\n    \n  \n\n\n\n\nUnlike data frames in R, the row index label figures prominently in certain operations. The elements of the dataframe can be retrived using .loc[ROW-INDEX, COL-INDEX] which specifies the location of data values by name (not by position).\n\n# retrieve row 0, column 'fruit'\nfruit_info.loc[0, 'fruit']\n\n'apple'\n\n\nMost of the time rows are indexed numerically, and somewhat confusingly, the syntax for .loc does not require putting the row index 0 in quotes, even though it refers to the row label and not the row number. This is important to remember, because often operations will scramble the order of rows. To see the difference, consider the following:\n\n# non-consecutive row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n)\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      8\n      apple\n      red\n    \n    \n      6\n      orange\n      orange\n    \n    \n      4\n      banana\n      yellow\n    \n    \n      2\n      raspberry\n      pink\n    \n  \n\n\n\n\nNow adding .loc[2, 'color'] will retrieve pink, the last row of the dataframe.\n\n# subset\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n).loc[2, 'color']\n\n'pink'\n\n\nTo retrieve values by position, use .iloc. For many, this is more intuitive, as it is most similar to matrix or array indexing in mathematical notation.\n\n# retrieve 0, 0 entry\nfruit_info.iloc[0, 0]\n\n'apple'\n\n\n\n\nAdding, removing, and renaming columns\nThere are two ways to add new columns:\n\ndirect specification;\nusing .loc[].\n\nDirect specification: For a dataFrame df, you can add a column with df['new column name'] = ... and assign a list or array of values to the column.\nUsing .loc[]: For a dataframe df, you can add a column with df.loc[:, 'new column name'] = ... and assign a list or array of values to the column.\nBoth accomplish the same task – adding a new column index and populating values for each row – but .loc[] is a little faster.\n\n\nQuestion 1\nUsing direct specification, add to the fruit_info table a new column called rank1 containing integers 1, 2, 3, and 4, which express your personal preference about the taste ordering for each fruit (1 is tastiest; 4 is least tasty). Make sure that the numbers utilized are unique - no ties are allowed.\n\n\nfruit_info['rank1'] = [1, 3, 4, 2] # SOLUTION\n\n# print\nfruit_info\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n      rank1\n    \n  \n  \n    \n      0\n      apple\n      red\n      1\n    \n    \n      1\n      orange\n      orange\n      3\n    \n    \n      2\n      banana\n      yellow\n      4\n    \n    \n      3\n      raspberry\n      pink\n      2\n    \n  \n\n\n\n\n\ngrader.check(\"q1\")\n\nNow, create a new dataframe fruit_info_mod1 with the same information as fruit_info_original, but has the additional column rank2. Let’s start off with making fruit_info_mod1 as a copy of fruit_info:\n\nfruit_info_mod1 = fruit_info.copy()\n\n\n\nQuestion 2\nUsing .loc[], add a column called rank2 to the fruit_info_mod1 table that contains the same values in the same order as the rank1 column.\nHint: .loc will parse : as shorthand for ‘all indices’.\n\n\nfruit_info_mod1.loc[:, 'rank2']  = [1, 3, 4, 2] #SOLUTION\n\n# print\nfruit_info_mod1\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n      rank1\n      rank2\n    \n  \n  \n    \n      0\n      apple\n      red\n      1\n      1\n    \n    \n      1\n      orange\n      orange\n      3\n      3\n    \n    \n      2\n      banana\n      yellow\n      4\n      4\n    \n    \n      3\n      raspberry\n      pink\n      2\n      2\n    \n  \n\n\n\n\n\ngrader.check(\"q2\")\n\nWhen using the .loc[] approach, the : specifies that values are assigned to all rows of the data frame, so the array assigned to the new variable must be the same length as the data frame. What if we only assign values to certain rows? Try running the cell below.\n\n# define new variable just for rows 1 and 2\nfruit_info_mod1.loc[1:2, 'rank3'] = [1, 2]\n\n# check result\nfruit_info_mod1\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n      rank1\n      rank2\n      rank3\n    \n  \n  \n    \n      0\n      apple\n      red\n      1\n      1\n      NaN\n    \n    \n      1\n      orange\n      orange\n      3\n      3\n      1.0\n    \n    \n      2\n      banana\n      yellow\n      4\n      4\n      2.0\n    \n    \n      3\n      raspberry\n      pink\n      2\n      2\n      NaN\n    \n  \n\n\n\n\nThe remaining rows are assigned missing values. Notice what this does to the data type:\n\n# check data types\nfruit_info_mod1.dtypes\n\nfruit     object\ncolor     object\nrank1      int64\nrank2      int64\nrank3    float64\ndtype: object\n\n\nWe can detect these missing values using .isna():\n\n# returns a logical data frame indicating whether each entry is missing or not\nfruit_info_mod1.isna()\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n      rank1\n      rank2\n      rank3\n    \n  \n  \n    \n      0\n      False\n      False\n      False\n      False\n      True\n    \n    \n      1\n      False\n      False\n      False\n      False\n      False\n    \n    \n      2\n      False\n      False\n      False\n      False\n      False\n    \n    \n      3\n      False\n      False\n      False\n      False\n      True\n    \n  \n\n\n\n\nIt would be more helpful to simply see by column whether there are missing values. Appending a .any() to the above command will do the trick:\n\n# detects whether any column has missing entries\nfruit_info_mod1.isna().any()\n\nfruit    False\ncolor    False\nrank1    False\nrank2    False\nrank3     True\ndtype: bool\n\n\nNow that we’ve had a bit of fun let’s remove those rank variables. Columns can be removed using .drop() with a list of column names to drop as its argument. For example:\n\n# first syntax for .drop()\nfruit_info_mod1.drop(columns = 'color')\n\n\n\n\n\n  \n    \n      \n      fruit\n      rank1\n      rank2\n      rank3\n    \n  \n  \n    \n      0\n      apple\n      1\n      1\n      NaN\n    \n    \n      1\n      orange\n      3\n      3\n      1.0\n    \n    \n      2\n      banana\n      4\n      4\n      2.0\n    \n    \n      3\n      raspberry\n      2\n      2\n      NaN\n    \n  \n\n\n\n\nThere is an alternate syntax to that shown above, which involves specifying the axis (row vs. column) and index name to drop:\n\n# second syntax for .drop()\nfruit_info_mod1.drop('color', axis = 1)\n\n\n\n\n\n  \n    \n      \n      fruit\n      rank1\n      rank2\n      rank3\n    \n  \n  \n    \n      0\n      apple\n      1\n      1\n      NaN\n    \n    \n      1\n      orange\n      3\n      3\n      1.0\n    \n    \n      2\n      banana\n      4\n      4\n      2.0\n    \n    \n      3\n      raspberry\n      2\n      2\n      NaN\n    \n  \n\n\n\n\n\n\nQuestion 3\nUse the .drop() method to drop all rank columns you created in fruit_info_mod1. Note that drop does not change the table, but instead returns a new table with fewer columns or rows. To store the result, assign a new name (or write over the old dataframe). Here, assign the result to fruit_info_original.\nHint: Look through the documentation to see how you can drop multiple columns of a Pandas dataframe at once using a list of column names.\n\n\nfruit_info_original = fruit_info_mod1.drop(columns = ['rank1', 'rank2', 'rank3']) #SOLUTION\n\n# print\nfruit_info_original\n\n\n\n\n\n  \n    \n      \n      fruit\n      color\n    \n  \n  \n    \n      0\n      apple\n      red\n    \n    \n      1\n      orange\n      orange\n    \n    \n      2\n      banana\n      yellow\n    \n    \n      3\n      raspberry\n      pink\n    \n  \n\n\n\n\n\ngrader.check(\"q3\")\n\nNifty trick: Use df.columns[df.columns.str.startswith('STRING')] to retrieve all indices starting with STRING and ix.values.tolist() to convert an index to an array of index names to obtain a list of column names to drop. Combining these gives df.columns[df.columns.str.startswith('STRING')].values.tolist(), and will return a list of all column names starting with STRING. This can be used in conjunction with the hint to remove all columns starting with rank.\n\n# try the nifty trick here\n\nNow create a new dataframe fruit_info_mod2with the same information as fruit_info_original, but has the column names capitalized. Begin by creating a copy fruit_info_mod2 of fruit_info_original:\n\nfruit_info_mod2 = fruit_info_original.copy()\n\n\n\nQuestion 4\nReview the documentation for .rename(). Based on the examples, rename the columns of fruit_info_mod2 so they begin with capital letters.\nFor many operations, you can change the dataframe ‘in place’ without reassigning the result of the operation to a new name by setting the inplace parameter to True. Use that strategy here.\n\n\nfruit_info_mod2.rename(columns = {'fruit': 'Fruit', 'color': 'Color'}, inplace = True) #SOLUTION\n\n# print\nfruit_info_mod2\n\n\n\n\n\n  \n    \n      \n      Fruit\n      Color\n    \n  \n  \n    \n      0\n      apple\n      red\n    \n    \n      1\n      orange\n      orange\n    \n    \n      2\n      banana\n      yellow\n    \n    \n      3\n      raspberry\n      pink\n    \n  \n\n\n\n\n\ngrader.check(\"q4\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#operations-on-data-frames",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#operations-on-data-frames",
    "title": "PSTAT100",
    "section": "Operations on Data Frames",
    "text": "Operations on Data Frames\nWith some basics in place, here you’ll see how to perform subsetting operations on data frames that are useful for tidying up datasets.\n\nSlicing: selecting columns or rows in chunks or by position.\n\nOften imported data contain columns that are either superfluous or not of interest for a particular project.\nYou may also want to examine particular portions of a data frame.\n\nFiltering: selecting rows that meet certain criteria\n\nOften you’ll want to remove duplicate rows, filter missing observations, or select a structured subset of a data frame.\nAlso helpful for inspection.\n\n\nTo illustrate these operations, you’ll use a dataset comprising counts of the given names of babies born in California each year from 1990 - 2018. The cell below imports the baby names data as a data frame from a .csv file. .head() prints the first few rows of the dataset.\n\n# import baby names data\nbaby_names = pd.read_csv('data/baby_names.csv')\n\n# preview first few rows\nbaby_names.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1990\n      Jessica\n      6635\n    \n    \n      1\n      CA\n      F\n      1990\n      Ashley\n      4537\n    \n    \n      2\n      CA\n      F\n      1990\n      Stephanie\n      4001\n    \n    \n      3\n      CA\n      F\n      1990\n      Amanda\n      3856\n    \n    \n      4\n      CA\n      F\n      1990\n      Jennifer\n      3611\n    \n  \n\n\n\n\nYour focus here isn’t on analyzing this data, so we won’t ask you to spend too much effort getting acquainted with it. However, a brief inspection is always a good idea. Let’s check:\n\ndimensions (number of rows and columns);\nhow many distinct states, sexes, and years.\n\nNote that the above dataframe displayed is a preview of the full dataframe.\n\nQuestion 5\nYou’ve already seen how to examine dimensions using dataframe attributes. Check the dimensions of baby_names and store them in dimensions_baby_names.\n\n\ndimensions_baby_names = baby_names.shape #SOLUTION\n\n\ngrader.check(\"q5\")\n\nYou haven’t yet seen how to retrieve the distinct values of an array or series, without duplicates. There are a few different ways to go about this, but one is to count the number of occurrences of each distinct entry in a column. This can be done by retrieving the column as a series using syntax of the form df.colname, and then pass the result to .value_counts():\n\n# count distinct values\nbaby_names.Sex.value_counts()\n\nF    112196\nM     78566\nName: Sex, dtype: int64\n\n\n\n\nQuestion 6\nCount the number of occurences of each distinct year. Create a series occur_per_year that displays the number of occurrences, ordered by year (so that the years are displayed in order). If you add sort = False as an argument to value_counts, the distinct values will be displayed in the order they appear in the dataset.\nHow many years are represented in the dataset? Store your answer as num_years.\n\n\noccur_per_year = baby_names.Year.value_counts(sort = False) #SOLUTION \n\nnum_years = len(occur_per_year) #SOLUTION\n\nprint(occur_per_year)\nprint(num_years)\n\n1990    6261\n1991    6226\n1992    6304\n1993    6314\n1994    6241\n1995    6092\n1996    6036\n1997    5961\n1998    5976\n1999    6052\n2000    6284\n2001    6333\n2002    6414\n2003    6533\n2004    6708\n2005    6874\n2006    7075\n2007    7250\n2008    7158\n2009    7119\n2010    7010\n2011    6880\n2012    7007\n2013    6861\n2014    6952\n2015    6871\n2016    6770\n2017    6684\n2018    6516\nName: Year, dtype: int64\n29\n\n\n\ngrader.check(\"q6\")\n\n\n\nSlicing: selecting rows and columns\nThere are two fast and simple ways to slice dataframes:\n\nusing .loc to specify rows and columns by index;\nusing .iloc to specify rows and columns by position.\n\nYou have seen simple examples of both of these above. Here we’ll show how to use these two commands to retrieve multiple rows and columns.\n\nSlicing with .loc: specifying index names\nThis method retrieves entries by specifying row and column indexes using syntax of the form df.loc[rows, cols]. The rows and columns can be single indices, a list of indices, or a set of adjacent indices using a colon :. Examples of these usages are shown below.\n\n# single indices -- small slice\nbaby_names.loc[2, 'Name']\n\n'Stephanie'\n\n\n\n# a list of indices -- larger slice\nbaby_names.loc[[2, 3], ['Name', 'Count']]\n\n\n\n\n\n  \n    \n      \n      Name\n      Count\n    \n  \n  \n    \n      2\n      Stephanie\n      4001\n    \n    \n      3\n      Amanda\n      3856\n    \n  \n\n\n\n\n\n# consecutive indices -- a chunk\nbaby_names.loc[2:10, 'Year':'Count']\n\n\n\n\n\n  \n    \n      \n      Year\n      Name\n      Count\n    \n  \n  \n    \n      2\n      1990\n      Stephanie\n      4001\n    \n    \n      3\n      1990\n      Amanda\n      3856\n    \n    \n      4\n      1990\n      Jennifer\n      3611\n    \n    \n      5\n      1990\n      Elizabeth\n      3170\n    \n    \n      6\n      1990\n      Sarah\n      2843\n    \n    \n      7\n      1990\n      Brittany\n      2737\n    \n    \n      8\n      1990\n      Samantha\n      2720\n    \n    \n      9\n      1990\n      Michelle\n      2453\n    \n    \n      10\n      1990\n      Melissa\n      2442\n    \n  \n\n\n\n\n\n\n\nSlicing with .iloc: specifying entry positions\nAn alternative to specifying the indices in order to slice a dataframe is to specify the entry positions using .iloc (‘integer location’). You have seen an example of this too. As with .loc, .iloc can be used to select multiple rows/columns using either lists of positions or a consecutive set with from:to syntax.\n\n# single position\nbaby_names.iloc[2, 3]\n\n'Stephanie'\n\n\n\n# a list of positions\nbaby_names.iloc[[2, 3], [3, 4]]\n\n\n\n\n\n  \n    \n      \n      Name\n      Count\n    \n  \n  \n    \n      2\n      Stephanie\n      4001\n    \n    \n      3\n      Amanda\n      3856\n    \n  \n\n\n\n\n\n# consecutive positions\nbaby_names.iloc[2:11, 2:5]\n\n\n\n\n\n  \n    \n      \n      Year\n      Name\n      Count\n    \n  \n  \n    \n      2\n      1990\n      Stephanie\n      4001\n    \n    \n      3\n      1990\n      Amanda\n      3856\n    \n    \n      4\n      1990\n      Jennifer\n      3611\n    \n    \n      5\n      1990\n      Elizabeth\n      3170\n    \n    \n      6\n      1990\n      Sarah\n      2843\n    \n    \n      7\n      1990\n      Brittany\n      2737\n    \n    \n      8\n      1990\n      Samantha\n      2720\n    \n    \n      9\n      1990\n      Michelle\n      2453\n    \n    \n      10\n      1990\n      Melissa\n      2442\n    \n  \n\n\n\n\nWhile these commands may look very similar to their .loc analogs, there are some subtle but important differences. The row selection looks nearly identical, but recall that .loc uses the index and .iloc uses the position; they look so similar because typically index and position coincide.\nHowever, sorting the baby_names dataframe helps to reveal how the position of a row is not necessarily equal to the index of a row. For example, the first row is not necessarily the row associated with index 1. This distinction is important in understanding the difference between .loc[] and .iloc[].\n\n# sort and display\nsorted_baby_names = baby_names.sort_values(by=['Name'])\nsorted_baby_names.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      160797\n      CA\n      M\n      2008\n      Aadan\n      7\n    \n    \n      178791\n      CA\n      M\n      2014\n      Aadan\n      5\n    \n    \n      163914\n      CA\n      M\n      2009\n      Aadan\n      6\n    \n    \n      171112\n      CA\n      M\n      2012\n      Aaden\n      38\n    \n    \n      179928\n      CA\n      M\n      2015\n      Aaden\n      34\n    \n  \n\n\n\n\nHere is an example of how we would get the 2nd, 3rd, and 4th rows with only the Name column of the baby_names dataframe using both iloc[] and loc[]. Observe the difference, especially after sorting baby_names by name.\n\n# example iloc usage\nsorted_baby_names.iloc[1:4, 3]\n\n178791    Aadan\n163914    Aadan\n171112    Aaden\nName: Name, dtype: object\n\n\nNotice that using loc[] with 1:4 gives different results, since it selects using the index. The index gets moved around when you perform an operation like sort on the dataframe.\n\n# same syntax, different result\nsorted_baby_names.loc[1:4, \"Name\"]\n\n1           Ashley\n22219       Ashley\n138598      Ashley\n151978      Ashley\n120624      Ashley\n            ...   \n74380       Jennie\n19395       Jennie\n23061       Jennie\n91825       Jennie\n4         Jennifer\nName: Name, Length: 68640, dtype: object\n\n\nAbove, the .loc method retrieves all indexes between index 1 and index 4 in the order they appear in the sorted dataset. If instead we want to retrieve the same rows returned by the .iloc command, we need to specify the row indices explicitly as a list:\n\n# retrieve the same rows as iloc using loc\nsorted_baby_names.loc[[178791, 163914, 171112], 'Name']\n\n178791    Aadan\n163914    Aadan\n171112    Aaden\nName: Name, dtype: object\n\n\nSometimes it’s useful for slicing (and other operations) to set one of the columns to be a row index, effectively treating one column as a collection of row labels. This can be accomplished using set_index.\n\n# change the (row) index from 0,1,2,... to the name column\nbaby_names_nameindexed = baby_names.set_index(\"Name\") \nbaby_names_nameindexed.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Count\n    \n    \n      Name\n      \n      \n      \n      \n    \n  \n  \n    \n      Jessica\n      CA\n      F\n      1990\n      6635\n    \n    \n      Ashley\n      CA\n      F\n      1990\n      4537\n    \n    \n      Stephanie\n      CA\n      F\n      1990\n      4001\n    \n    \n      Amanda\n      CA\n      F\n      1990\n      3856\n    \n    \n      Jennifer\n      CA\n      F\n      1990\n      3611\n    \n  \n\n\n\n\nWe can now slice by name directly:\n\n# slice rows for ashley and jennifer\nbaby_names_nameindexed.loc[['Ashley', 'Jennifer'], :]\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Count\n    \n    \n      Name\n      \n      \n      \n      \n    \n  \n  \n    \n      Ashley\n      CA\n      F\n      1990\n      4537\n    \n    \n      Ashley\n      CA\n      F\n      1991\n      4233\n    \n    \n      Ashley\n      CA\n      F\n      1992\n      3966\n    \n    \n      Ashley\n      CA\n      F\n      1993\n      3591\n    \n    \n      Ashley\n      CA\n      F\n      1994\n      3202\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Jennifer\n      CA\n      M\n      1998\n      10\n    \n    \n      Jennifer\n      CA\n      M\n      1999\n      12\n    \n    \n      Jennifer\n      CA\n      M\n      2000\n      10\n    \n    \n      Jennifer\n      CA\n      M\n      2001\n      8\n    \n    \n      Jennifer\n      CA\n      M\n      2002\n      7\n    \n  \n\n88 rows Ã— 4 columns\n\n\n\n\n\nQuestion 7\nLook up the name of a friend! Store the name as friend_name. Use the name-indexed data frame to slice rows for the name of your choice and the Count, Sex, and Year columns in that order, and store the data frame as friend_slice.\n\n\n# if your friend's name is not in the database, use another name\n\nfriend_name = \"Trevor\" # SOLUTION\nfriend_slice = baby_names_nameindexed.loc[\"Trevor\", ['Count', 'Sex', 'Year']] #SOLUTION\n\n#print\nfriend_slice\n\n\n\n\n\n  \n    \n      \n      Count\n      Sex\n      Year\n    \n    \n      Name\n      \n      \n      \n    \n  \n  \n    \n      Trevor\n      5\n      F\n      1990\n    \n    \n      Trevor\n      823\n      M\n      1990\n    \n    \n      Trevor\n      836\n      M\n      1991\n    \n    \n      Trevor\n      897\n      M\n      1992\n    \n    \n      Trevor\n      737\n      M\n      1993\n    \n    \n      Trevor\n      675\n      M\n      1994\n    \n    \n      Trevor\n      682\n      M\n      1995\n    \n    \n      Trevor\n      609\n      M\n      1996\n    \n    \n      Trevor\n      590\n      M\n      1997\n    \n    \n      Trevor\n      647\n      M\n      1998\n    \n    \n      Trevor\n      673\n      M\n      1999\n    \n    \n      Trevor\n      545\n      M\n      2000\n    \n    \n      Trevor\n      535\n      M\n      2001\n    \n    \n      Trevor\n      488\n      M\n      2002\n    \n    \n      Trevor\n      425\n      M\n      2003\n    \n    \n      Trevor\n      369\n      M\n      2004\n    \n    \n      Trevor\n      372\n      M\n      2005\n    \n    \n      Trevor\n      335\n      M\n      2006\n    \n    \n      Trevor\n      302\n      M\n      2007\n    \n    \n      Trevor\n      281\n      M\n      2008\n    \n    \n      Trevor\n      252\n      M\n      2009\n    \n    \n      Trevor\n      219\n      M\n      2010\n    \n    \n      Trevor\n      194\n      M\n      2011\n    \n    \n      Trevor\n      161\n      M\n      2012\n    \n    \n      Trevor\n      142\n      M\n      2013\n    \n    \n      Trevor\n      126\n      M\n      2014\n    \n    \n      Trevor\n      114\n      M\n      2015\n    \n    \n      Trevor\n      103\n      M\n      2016\n    \n    \n      Trevor\n      90\n      M\n      2017\n    \n    \n      Trevor\n      78\n      M\n      2018\n    \n  \n\n\n\n\n\ngrader.check(\"q7\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#filtering",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#filtering",
    "title": "PSTAT100",
    "section": "Filtering",
    "text": "Filtering\nFiltering is sifting out rows according to a criterion, and can be accomplished using an array or series of Trues and Falses defined by a comparison. To take a simple example, say you wanted to filter out all names with fewer than 1000 occurrences. First you could define a logical series:\n\n# true if filtering criterion is met, false otherwise\narr = baby_names.Count > 1000\n\nThen you can filter using that array:\n\n# filter\nbaby_names_filtered = baby_names[arr]\nbaby_names_filtered.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1990\n      Jessica\n      6635\n    \n    \n      1\n      CA\n      F\n      1990\n      Ashley\n      4537\n    \n    \n      2\n      CA\n      F\n      1990\n      Stephanie\n      4001\n    \n    \n      3\n      CA\n      F\n      1990\n      Amanda\n      3856\n    \n    \n      4\n      CA\n      F\n      1990\n      Jennifer\n      3611\n    \n  \n\n\n\n\nNotice that the filtered array is much smaller than the overall array – only about 2000 rows correspond to a name occurring more than 1000 times in a year for a gender.\n\n# compare dimensions\nprint(baby_names_filtered.shape)\nprint(baby_names.shape)\n\n(2517, 5)\n(190762, 5)\n\n\nYou have already encountered this concept in lab 0 when subsetting an array. For your reference, some commonly used comparison operators are given below.\n\n\n\nSymbol\nUsage\nMeaning\n\n\n\n\n==\na == b\nDoes a equal b?\n\n\n<=\na <= b\nIs a less than or equal to b?\n\n\n>=\na >= b\nIs a greater than or equal to b?\n\n\n<\na < b\nIs a less than b?\n\n\n>\na > b\nIs a greater than b?\n\n\n~\n~p\nReturns negation of p\n\n\n|\np | q\np OR q\n\n\n&\np & q\np AND q\n\n\n^\np ^ q\np XOR q (exclusive or)\n\n\n\nWhat if instead you wanted to filter using multiple conditions? Here’s an example of retrieving rows with counts exceeding 1000 for only the year 2001:\n\n# filter using two conditions\nbaby_names[(baby_names.Year == 2000) & (baby_names.Count > 1000)]\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      36416\n      CA\n      F\n      2000\n      Emily\n      2958\n    \n    \n      36417\n      CA\n      F\n      2000\n      Ashley\n      2831\n    \n    \n      36418\n      CA\n      F\n      2000\n      Samantha\n      2579\n    \n    \n      36419\n      CA\n      F\n      2000\n      Jessica\n      2484\n    \n    \n      36420\n      CA\n      F\n      2000\n      Jennifer\n      2263\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      137298\n      CA\n      M\n      2000\n      Oscar\n      1089\n    \n    \n      137299\n      CA\n      M\n      2000\n      Thomas\n      1061\n    \n    \n      137300\n      CA\n      M\n      2000\n      Cameron\n      1052\n    \n    \n      137301\n      CA\n      M\n      2000\n      Austin\n      1010\n    \n    \n      137302\n      CA\n      M\n      2000\n      Richard\n      1001\n    \n  \n\n98 rows Ã— 5 columns\n\n\n\n\nQuestion 8\nSelect the girl names in 2010 that were given more than 3000 times, and store them as common_girl_names_2010.\nNote: Any time you use p & q to filter the dataframe, make sure to use df[df[(p) & (q)]] or df.loc[df[(p) & (q)]]). That is, make sure to wrap conditions with parentheses to ensure the intended order of operations.\n\n\ncommon_girl_names_2010 = baby_names[(baby_names.Sex == 'F') & (baby_names.Year == 2010) & (baby_names.Count > 3000)] #SOLUTION\n\ncommon_girl_names_2010\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      76793\n      CA\n      F\n      2010\n      Isabella\n      3368\n    \n    \n      76794\n      CA\n      F\n      2010\n      Sophia\n      3361\n    \n  \n\n\n\n\n\ngrader.check(\"q8\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas-soln.html#grouping-and-aggregation",
    "href": "labs/lab1-pandas/lab1-pandas-soln.html#grouping-and-aggregation",
    "title": "PSTAT100",
    "section": "Grouping and aggregation",
    "text": "Grouping and aggregation\nGrouping and aggregation are useful in generating data summaries, which are often important starting points in exploring a dataset.\n\nAggregation\nAggregation literally means ‘putting together’ (etymologically the word means ‘joining the herd’) – in statistics and data science, this refers to data summaries like an average, a minimum, or a measure of spread such as the sample variance or mean absolute deviation (data herding!). From a technical point of view, operations that take multiple values as inputs and return a single output are considered summaries – in other words, statistics. Some of the most common aggregations are:\n\nsum\nproduct\ncount\nnumber of distinct values\nmean\nmedian\nvariance\nstandard deviation\nminimum/maximum\nquantiles\n\nPandas has built-in dataframe operations that compute most of these summaries across either axis (column-wise or row-wise):\n\n.sum()\n.prod()\n.mean()\n.median()\n.var()\n.std()\n.nunique()\n.min() and .max()\n.quantile()\n\nTo illustrate these operations, let’s filter out all names in 1995.\n\n# filter 1995 names\nnames_95 = baby_names[baby_names.Year == 1995]\n\nHow many individuals were counted in total in 1995? We can address that by computing a sum of the counts:\n\n# n for 1995\nnames_95.Count.sum()\n\n494580\n\n\nWhat is the typical frequency of all names in 1995? We can address that by computing the average count:\n\n# average count for a name in 1995\nnames_95.Count.mean()\n\n81.18516086671043\n\n\n\n\nQuestion 9\nFind how often the most common name 1995 was given and store this as names_95_max_count. Use this value to filter names_95 and find which name was most common that year. Store the filtered dataframe as names_95_most_common_name.\n\n\nnames_95_max_count = names_95.Count.max() #SOLUTION\nnames_95_most_common_name = (names_95.loc[names_95.Count == names_95.Count.max(),  'Name']) #SOLUTION\n\n\nprint(\"Number of people with the most frequent name in 1995 is :\", names_95_max_count, \"people\")\nprint(\"Most frequent name in 1995 is:\", names_95_most_common_name.values[0])\n\nNumber of people with the most frequent name in 1995 is : 5003 people\nMost frequent name in 1995 is: Daniel\n\n\n\ngrader.check(\"q9\")\n\nCaution! If applied to the entire dataframe, the operation df.max() (or any other aggregation) will return the maximum of each column. Notice that the cell below does not return the row you found just now, but could easily be misinterpreted as such. The cell does tell you that the maximum value of sex (alphabetically last) is M and the maximum name (alphabetically last) is Zyanya and the maximum count is 5003; it does not tell you that 5003 boys were named Zyanya in 1995.\n\n# maximum of each variable\nnames_95.max()\n\nState        CA\nSex           M\nYear       1995\nName     Zyanya\nCount      5003\ndtype: object\n\n\n\n\nGrouping\nWhat if you want to know the most frequent male and female names? If so, you’ll need to repeat the above operations group-wise by sex.\nIn general, any variable in a dataframe can be used to define a grouping structure on the rows (or, less commonly, columns). After grouping, any dataframe operations will be executed within each group, but not across groups. This can be used to generate grouped summaries, such as the maximum count for boys and girls; as a point of terminology, we’d describe this summary as ‘maximum count by sex’.\nThe .groupby() function defines such a structure; here is the documentation. The cell below groups the names_95 dataframe by sex. Notice that when the grouped dataframe is previewed with .head(), the first few rows are returned for each group.\n\n# grouped dataframe\nnames_95_bysex = names_95.groupby('Sex')\n\n# print\nnames_95_bysex.head(2)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      18604\n      CA\n      F\n      1995\n      Jessica\n      4620\n    \n    \n      18605\n      CA\n      F\n      1995\n      Ashley\n      2903\n    \n    \n      124938\n      CA\n      M\n      1995\n      Daniel\n      5003\n    \n    \n      124939\n      CA\n      M\n      1995\n      Michael\n      4783\n    \n  \n\n\n\n\nAny aggregation operations applied to the grouped dataframe will be applied separately to the rows where Sex == M and the rows where Sex == F. For example, computing .sum() on the grouped dataframe will show the total number of individuals in the data for 1995 by sex:\n\n# number of individuals by sex\nnames_95_bysex.Count.sum()\n\nSex\nF    234552\nM    260028\nName: Count, dtype: int64\n\n\nThe most frequent boy and girl names can be found using .idxmax() groupwise to obtain the index of the first occurence of the maximum count for each sex, and then slicing with .loc:\n\n# first most common names by sex\nnames_95.loc[names_95_bysex.Count.idxmax(), :]\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      18604\n      CA\n      F\n      1995\n      Jessica\n      4620\n    \n    \n      124938\n      CA\n      M\n      1995\n      Daniel\n      5003\n    \n  \n\n\n\n\nSince .idxmax() gives the index of the first occurrence, these are the alphabetically first most common names; there could be ties. You know from your work so far that there are no ties for the male names; another filtering step can be used to check for ties among the female names.\n\n# ties?\nnames_95[names_95_bysex.Count.max().values[0] == names_95['Count']]\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      18604\n      CA\n      F\n      1995\n      Jessica\n      4620\n    \n  \n\n\n\n\nSo, no ties.\n\n\nQuestion 10\nAre there more girl names or boy names in 1995? Use the grouped dataframe names_95_bysex with the .count() aggregation to find the total number of names for each sex. Store the female and male counts as girl_name_count and boy_name_count, respectfully.\n\n\ngirl_name_count = names_95_bysex.Count.count()['F'] #SOLUTION\nboy_name_count = names_95_bysex.Count.count()['M'] #SOLUTION\n\n#print\nprint(girl_name_count)\nprint(boy_name_count)\n\n3614\n2478\n\n\n\ngrader.check(\"q10\")\n\n\n\nChaining operations\nYou have already seen examples of this, but pandas and numpy operations can be chained together in sequence. For example, names_95.Count.max() is a chain with two steps: first select the Count column (.count); then compute the maximum (.max()).\nGrouped summaries are often convenient to compute in a chained fashion, rather than by assigning the grouped dataframe a new name and performing operations on the resulting object. For example, finding the total number of boys and girls recorded in the 1995 data can be done with the following chain:\n\n# repeating previous calculation, more streamlined\nnames_95.groupby('Sex').Count.sum()\n\nSex\nF    234552\nM    260028\nName: Count, dtype: int64\n\n\nWe can take this even one step further and also perform the filtering in sequence as part of the chain:\n\n# longer chain\nbaby_names[baby_names.Year == 1995].groupby('Sex').Count.sum()\n\nSex\nF    234552\nM    260028\nName: Count, dtype: int64\n\n\nChains can get somewhat long, but they have the advantage of making codes more efficient, and often more readable. We did above in one step what took several lines before. Further, this chain can almost be read aloud:\n“Take baby names, filter on year, then group by sex, then select name counts, then compute the sum.”\nLet’s now consider computing the average counts of boy and girl names for each year 1990-1995. This can be accomplished by the following chain (notice it is possible to group by multiple variables).\n\n# average counts by sex and year\nbaby_names[baby_names.Year <= 1995].groupby(['Year', 'Sex']).mean(numeric_only = True)\n\n\n\n\n\n  \n    \n      \n      \n      Count\n    \n    \n      Year\n      Sex\n      \n    \n  \n  \n    \n      1990\n      F\n      70.085760\n    \n    \n      M\n      115.231930\n    \n    \n      1991\n      F\n      70.380888\n    \n    \n      M\n      114.608124\n    \n    \n      1992\n      F\n      68.744510\n    \n    \n      M\n      110.601556\n    \n    \n      1993\n      F\n      66.330675\n    \n    \n      M\n      107.896552\n    \n    \n      1994\n      F\n      66.426301\n    \n    \n      M\n      102.967966\n    \n    \n      1995\n      F\n      64.900941\n    \n    \n      M\n      104.934625\n    \n  \n\n\n\n\nThis display is not ideal. We can ‘pivot’ the table into a wide format by adding a few extra steps in the chain: change the indices to columns; then define a new shape by specifying which column should be the new row index, which should be the new column index, and which values should populate the table.\n\n# average counts by sex and year\nbaby_names[baby_names.Year <= 1995].groupby(\n    ['Year', 'Sex']\n    ).mean(\n    numeric_only = True\n    ).reset_index().pivot(\n    index = 'Sex', columns = 'Year', values = 'Count'\n    )\n\n\n\n\n\n  \n    \n      Year\n      1990\n      1991\n      1992\n      1993\n      1994\n      1995\n    \n    \n      Sex\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      F\n      70.08576\n      70.380888\n      68.744510\n      66.330675\n      66.426301\n      64.900941\n    \n    \n      M\n      115.23193\n      114.608124\n      110.601556\n      107.896552\n      102.967966\n      104.934625\n    \n  \n\n\n\n\nStyle comment: break long chains over multiple lines with indentation. The above chain is too long to be readable in one line. To balance the readability of codes with the efficiency of chaining, it is good practice to break long chains over several lines, with appropriate indentations.\nHere are some rules of thumb on style.\n\nSeparate comparisons by spaces (a<b as a < b)\nSplit chains longer than 30-40 characters over multiple lines\nSplit lines between delimiters (, )\nIncrease indentation for lines between delimiters\nFor chained operations, try to get each step in the chain shown on a separate line\nFor functions with multiple arguments, split lines so that each argument is on its own line\n\n\n\nQuestion 11\nWrite a chain with appropriate style to display the (first) most common boy and girl names in each of the years 2005-2015. Do this in two steps:\n\nFirst filter baby_names by year, then group by year and sex, and then find the indices of first occurence of the largest counts. Store these indices as ind.\nThen use .loc[] with your stored indices to slice baby_names so as to retrieve the rows corresponding to each most frequent name each year and for each sex; then pivot this table so that the columns are years, the rows are sexes, and the entries are names. Store this as pivot_names.\n\n\n\n# BEGIN SOLUTION\nind =  baby_names[(baby_names.Year <= 2015) & (baby_names.Year >= 2005)].groupby(\n    ['Sex', 'Year']\n).Count.idxmax(\n).values\n\npivot_names = baby_names.loc[ind, :].pivot(\n    index = 'Sex', \n    columns = 'Year', \n    values = 'Name'\n)\n\n# END SOLUTION\nprint(ind)\npivot_names\n\n[ 55767  59866  64073  68355  72602  76793  80890  84883  88981  92944\n  96958 150164 152939 155807 158775 161686 164614 167527 170414 173323\n 176221 179159]\n\n\n\n\n\n\n  \n    \n      Year\n      2005\n      2006\n      2007\n      2008\n      2009\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n    \n    \n      Sex\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      F\n      Emily\n      Emily\n      Emily\n      Isabella\n      Isabella\n      Isabella\n      Sophia\n      Sophia\n      Sophia\n      Sophia\n      Sophia\n    \n    \n      M\n      Daniel\n      Daniel\n      Daniel\n      Daniel\n      Daniel\n      Jacob\n      Jacob\n      Jacob\n      Jacob\n      Noah\n      Noah\n    \n  \n\n\n\n\n\ngrader.check(\"q11\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html",
    "href": "labs/lab1-pandas/lab1-pandas.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab1-pandas.ipynb\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#creating-dataframes-basic-manipulations",
    "href": "labs/lab1-pandas/lab1-pandas.html#creating-dataframes-basic-manipulations",
    "title": "PSTAT100",
    "section": "Creating DataFrames & Basic Manipulations",
    "text": "Creating DataFrames & Basic Manipulations\nA dataframe is a table in which each column has a type; there is an index over the columns (typically string labels) and an index over the rows (typically ordinal numbers). An index is represented by a series object, which is a one-dimensional labeled array. Here you’ll cover:\n\ncreating dataframes from scratch;\nretrieving attributes;\ndataframe indexing;\nadding, removing, and renaming columns.\n\n\nCreating dataframes from scratch\nThe documentation for the pandas DataFrame class provide two primary syntaxes to create a data frame from scratch:\n\nfrom a dictionary\nrow-wise tuples\n\nSyntax 1 (dictionary): You can create a data frame by specifying the columns and values using a dictionary (a concatenation of named lists) as shown below.\nThe keys of the dictionary are the column names, and the values of the dictionary are lists containing the row entries.\n\n# define a dataframe using dictionary syntax\nfruit_info = pd.DataFrame( \n    data = { 'fruit': ['apple', 'orange', 'banana', 'raspberry'],\n             'color': ['red', 'orange', 'yellow', 'pink']\n           })\n\n# print\nfruit_info\n\nSyntax 2 (row tuples): You can also define a dataframe by specifying the rows as tuples.\nEach row corresponds to a distinct tuple, and the column indices are specified separately.\n\n# define the same dataframe using tuple syntax \nfruit_info2 = pd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"]\n)\n\n# print\nfruit_info2\n\n\n\nDataframe Attributes\nDataFrames have several basic attributes:\n\n.shape contains dimensions;\n.dtypes contains data types (float, integer, object, etc.)\n.size first (row) dimension;\n.values contains an array comprising each entry in the dataframe.\n.columns contains the column index;\n.index contains the row index.\n\nYou can obtain these attributes by appending the attribute name to the dataframe name. For instance, the dimensions of a dataframe df can be retrieved by df.shape.\n\n# dimensions\nfruit_info.shape\n\nTo retrieve a two-dimensional numpy array with the values of the dataframe, use df.values. It is sometimes useful to extract this data structure in order to perform vectorized operations, linear algebra, and the like.\n\n# as array\nfruit_info.values\n\n\n\nDataframe Indexing\nThe entries in a dataframe are indexed. Indices for rows and columns are stored as the .index. and .columns attributes, respectively.\n\nfruit_info.columns\n\n\nfruit_info.index\n\nBy default, the row indexing is simply numbering by consecutive integers.\n\nfruit_info.index.values\n\nHowever, rows can alternatively be indexed by labels:\n\n# define with a row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = [\"fruit 1\", \"fruit 2\", \"fruit 3\", 'fruit 4']\n)\n\nUnlike data frames in R, the row index label figures prominently in certain operations. The elements of the dataframe can be retrived using .loc[ROW-INDEX, COL-INDEX] which specifies the location of data values by name (not by position).\n\n# retrieve row 0, column 'fruit'\nfruit_info.loc[0, 'fruit']\n\nMost of the time rows are indexed numerically, and somewhat confusingly, the syntax for .loc does not require putting the row index 0 in quotes, even though it refers to the row label and not the row number. This is important to remember, because often operations will scramble the order of rows. To see the difference, consider the following:\n\n# non-consecutive row index\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n)\n\nNow adding .loc[2, 'color'] will retrieve pink, the last row of the dataframe.\n\n# subset\npd.DataFrame(\n    [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"banana\", \"yellow\"), (\"raspberry\", \"pink\")],\n    columns = [\"fruit\", \"color\"],\n    index = np.array([8, 6, 4, 2])\n).loc[2, 'color']\n\nTo retrieve values by position, use .iloc. For many, this is more intuitive, as it is most similar to matrix or array indexing in mathematical notation.\n\n# retrieve 0, 0 entry\nfruit_info.iloc[0, 0]\n\n\n\nAdding, removing, and renaming columns\nThere are two ways to add new columns:\n\ndirect specification;\nusing .loc[].\n\nDirect specification: For a dataFrame df, you can add a column with df['new column name'] = ... and assign a list or array of values to the column.\nUsing .loc[]: For a dataframe df, you can add a column with df.loc[:, 'new column name'] = ... and assign a list or array of values to the column.\nBoth accomplish the same task – adding a new column index and populating values for each row – but .loc[] is a little faster.\n\n\nQuestion 1\nUsing direct specification, add to the fruit_info table a new column called rank1 containing integers 1, 2, 3, and 4, which express your personal preference about the taste ordering for each fruit (1 is tastiest; 4 is least tasty). Make sure that the numbers utilized are unique - no ties are allowed.\n\n\n...\n\n# print\nfruit_info\n\n\ngrader.check(\"q1\")\n\nNow, create a new dataframe fruit_info_mod1 with the same information as fruit_info_original, but has the additional column rank2. Let’s start off with making fruit_info_mod1 as a copy of fruit_info:\n\nfruit_info_mod1 = fruit_info.copy()\n\n\n\nQuestion 2\nUsing .loc[], add a column called rank2 to the fruit_info_mod1 table that contains the same values in the same order as the rank1 column.\nHint: .loc will parse : as shorthand for ‘all indices’.\n\n\n...\n\n# print\nfruit_info_mod1\n\n\ngrader.check(\"q2\")\n\nWhen using the .loc[] approach, the : specifies that values are assigned to all rows of the data frame, so the array assigned to the new variable must be the same length as the data frame. What if we only assign values to certain rows? Try running the cell below.\n\n# define new variable just for rows 1 and 2\nfruit_info_mod1.loc[1:2, 'rank3'] = [1, 2]\n\n# check result\nfruit_info_mod1\n\nThe remaining rows are assigned missing values. Notice what this does to the data type:\n\n# check data types\nfruit_info_mod1.dtypes\n\nWe can detect these missing values using .isna():\n\n# returns a logical data frame indicating whether each entry is missing or not\nfruit_info_mod1.isna()\n\nIt would be more helpful to simply see by column whether there are missing values. Appending a .any() to the above command will do the trick:\n\n# detects whether any column has missing entries\nfruit_info_mod1.isna().any()\n\nNow that we’ve had a bit of fun let’s remove those rank variables. Columns can be removed using .drop() with a list of column names to drop as its argument. For example:\n\n# first syntax for .drop()\nfruit_info_mod1.drop(columns = 'color')\n\nThere is an alternate syntax to that shown above, which involves specifying the axis (row vs. column) and index name to drop:\n\n# second syntax for .drop()\nfruit_info_mod1.drop('color', axis = 1)\n\n\n\nQuestion 3\nUse the .drop() method to drop all rank columns you created in fruit_info_mod1. Note that drop does not change the table, but instead returns a new table with fewer columns or rows. To store the result, assign a new name (or write over the old dataframe). Here, assign the result to fruit_info_original.\nHint: Look through the documentation to see how you can drop multiple columns of a Pandas dataframe at once using a list of column names.\n\n\nfruit_info_original = ...\n\n# print\nfruit_info_original\n\n\ngrader.check(\"q3\")\n\nNifty trick: Use df.columns[df.columns.str.startswith('STRING')] to retrieve all indices starting with STRING and ix.values.tolist() to convert an index to an array of index names to obtain a list of column names to drop. Combining these gives df.columns[df.columns.str.startswith('STRING')].values.tolist(), and will return a list of all column names starting with STRING. This can be used in conjunction with the hint to remove all columns starting with rank.\n\n# try the nifty trick here\n\nNow create a new dataframe fruit_info_mod2with the same information as fruit_info_original, but has the column names capitalized. Begin by creating a copy fruit_info_mod2 of fruit_info_original:\n\nfruit_info_mod2 = fruit_info_original.copy()\n\n\n\nQuestion 4\nReview the documentation for .rename(). Based on the examples, rename the columns of fruit_info_mod2 so they begin with capital letters.\nFor many operations, you can change the dataframe ‘in place’ without reassigning the result of the operation to a new name by setting the inplace parameter to True. Use that strategy here.\n\n\n...\n\n# print\nfruit_info_mod2\n\n\ngrader.check(\"q4\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#operations-on-data-frames",
    "href": "labs/lab1-pandas/lab1-pandas.html#operations-on-data-frames",
    "title": "PSTAT100",
    "section": "Operations on Data Frames",
    "text": "Operations on Data Frames\nWith some basics in place, here you’ll see how to perform subsetting operations on data frames that are useful for tidying up datasets.\n\nSlicing: selecting columns or rows in chunks or by position.\n\nOften imported data contain columns that are either superfluous or not of interest for a particular project.\nYou may also want to examine particular portions of a data frame.\n\nFiltering: selecting rows that meet certain criteria\n\nOften you’ll want to remove duplicate rows, filter missing observations, or select a structured subset of a data frame.\nAlso helpful for inspection.\n\n\nTo illustrate these operations, you’ll use a dataset comprising counts of the given names of babies born in California each year from 1990 - 2018. The cell below imports the baby names data as a data frame from a .csv file. .head() prints the first few rows of the dataset.\n\n# import baby names data\nbaby_names = pd.read_csv('data/baby_names.csv')\n\n# preview first few rows\nbaby_names.head()\n\nYour focus here isn’t on analyzing this data, so we won’t ask you to spend too much effort getting acquainted with it. However, a brief inspection is always a good idea. Let’s check:\n\ndimensions (number of rows and columns);\nhow many distinct states, sexes, and years.\n\nNote that the above dataframe displayed is a preview of the full dataframe.\n\nQuestion 5\nYou’ve already seen how to examine dimensions using dataframe attributes. Check the dimensions of baby_names and store them in dimensions_baby_names.\n\n\ndimensions_baby_names = ...\n\n\ngrader.check(\"q5\")\n\nYou haven’t yet seen how to retrieve the distinct values of an array or series, without duplicates. There are a few different ways to go about this, but one is to count the number of occurrences of each distinct entry in a column. This can be done by retrieving the column as a series using syntax of the form df.colname, and then pass the result to .value_counts():\n\n# count distinct values\nbaby_names.Sex.value_counts()\n\n\n\nQuestion 6\nCount the number of occurences of each distinct year. Create a series occur_per_year that displays the number of occurrences, ordered by year (so that the years are displayed in order). If you add sort = False as an argument to value_counts, the distinct values will be displayed in the order they appear in the dataset.\nHow many years are represented in the dataset? Store your answer as num_years.\n\n\noccur_per_year = ...\n\nnum_years = ...\n\nprint(occur_per_year)\nprint(num_years)\n\n\ngrader.check(\"q6\")\n\n\n\nSlicing: selecting rows and columns\nThere are two fast and simple ways to slice dataframes:\n\nusing .loc to specify rows and columns by index;\nusing .iloc to specify rows and columns by position.\n\nYou have seen simple examples of both of these above. Here we’ll show how to use these two commands to retrieve multiple rows and columns.\n\nSlicing with .loc: specifying index names\nThis method retrieves entries by specifying row and column indexes using syntax of the form df.loc[rows, cols]. The rows and columns can be single indices, a list of indices, or a set of adjacent indices using a colon :. Examples of these usages are shown below.\n\n# single indices -- small slice\nbaby_names.loc[2, 'Name']\n\n\n# a list of indices -- larger slice\nbaby_names.loc[[2, 3], ['Name', 'Count']]\n\n\n# consecutive indices -- a chunk\nbaby_names.loc[2:10, 'Year':'Count']\n\n\n\n\nSlicing with .iloc: specifying entry positions\nAn alternative to specifying the indices in order to slice a dataframe is to specify the entry positions using .iloc (‘integer location’). You have seen an example of this too. As with .loc, .iloc can be used to select multiple rows/columns using either lists of positions or a consecutive set with from:to syntax.\n\n# single position\nbaby_names.iloc[2, 3]\n\n\n# a list of positions\nbaby_names.iloc[[2, 3], [3, 4]]\n\n\n# consecutive positions\nbaby_names.iloc[2:11, 2:5]\n\nWhile these commands may look very similar to their .loc analogs, there are some subtle but important differences. The row selection looks nearly identical, but recall that .loc uses the index and .iloc uses the position; they look so similar because typically index and position coincide.\nHowever, sorting the baby_names dataframe helps to reveal how the position of a row is not necessarily equal to the index of a row. For example, the first row is not necessarily the row associated with index 1. This distinction is important in understanding the difference between .loc[] and .iloc[].\n\n# sort and display\nsorted_baby_names = baby_names.sort_values(by=['Name'])\nsorted_baby_names.head()\n\nHere is an example of how we would get the 2nd, 3rd, and 4th rows with only the Name column of the baby_names dataframe using both iloc[] and loc[]. Observe the difference, especially after sorting baby_names by name.\n\n# example iloc usage\nsorted_baby_names.iloc[1:4, 3]\n\nNotice that using loc[] with 1:4 gives different results, since it selects using the index. The index gets moved around when you perform an operation like sort on the dataframe.\n\n# same syntax, different result\nsorted_baby_names.loc[1:4, \"Name\"]\n\nAbove, the .loc method retrieves all indexes between index 1 and index 4 in the order they appear in the sorted dataset. If instead we want to retrieve the same rows returned by the .iloc command, we need to specify the row indices explicitly as a list:\n\n# retrieve the same rows as iloc using loc\nsorted_baby_names.loc[[178791, 163914, 171112], 'Name']\n\nSometimes it’s useful for slicing (and other operations) to set one of the columns to be a row index, effectively treating one column as a collection of row labels. This can be accomplished using set_index.\n\n# change the (row) index from 0,1,2,... to the name column\nbaby_names_nameindexed = baby_names.set_index(\"Name\") \nbaby_names_nameindexed.head()\n\nWe can now slice by name directly:\n\n# slice rows for ashley and jennifer\nbaby_names_nameindexed.loc[['Ashley', 'Jennifer'], :]\n\n\n\nQuestion 7\nLook up the name of a friend! Store the name as friend_name. Use the name-indexed data frame to slice rows for the name of your choice and the Count, Sex, and Year columns in that order, and store the data frame as friend_slice.\n\n\n# if your friend's name is not in the database, use another name\n\nfriend_name = ...\nfriend_slice = ...\n\n#print\nfriend_slice\n\n\ngrader.check(\"q7\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#filtering",
    "href": "labs/lab1-pandas/lab1-pandas.html#filtering",
    "title": "PSTAT100",
    "section": "Filtering",
    "text": "Filtering\nFiltering is sifting out rows according to a criterion, and can be accomplished using an array or series of Trues and Falses defined by a comparison. To take a simple example, say you wanted to filter out all names with fewer than 1000 occurrences. First you could define a logical series:\n\n# true if filtering criterion is met, false otherwise\narr = baby_names.Count > 1000\n\nThen you can filter using that array:\n\n# filter\nbaby_names_filtered = baby_names[arr]\nbaby_names_filtered.head()\n\nNotice that the filtered array is much smaller than the overall array – only about 2000 rows correspond to a name occurring more than 1000 times in a year for a gender.\n\n# compare dimensions\nprint(baby_names_filtered.shape)\nprint(baby_names.shape)\n\nYou have already encountered this concept in lab 0 when subsetting an array. For your reference, some commonly used comparison operators are given below.\n\n\n\nSymbol\nUsage\nMeaning\n\n\n\n\n==\na == b\nDoes a equal b?\n\n\n<=\na <= b\nIs a less than or equal to b?\n\n\n>=\na >= b\nIs a greater than or equal to b?\n\n\n<\na < b\nIs a less than b?\n\n\n>\na > b\nIs a greater than b?\n\n\n~\n~p\nReturns negation of p\n\n\n|\np | q\np OR q\n\n\n&\np & q\np AND q\n\n\n^\np ^ q\np XOR q (exclusive or)\n\n\n\nWhat if instead you wanted to filter using multiple conditions? Here’s an example of retrieving rows with counts exceeding 1000 for only the year 2001:\n\n# filter using two conditions\nbaby_names[(baby_names.Year == 2000) & (baby_names.Count > 1000)]\n\n\nQuestion 8\nSelect the girl names in 2010 that were given more than 3000 times, and store them as common_girl_names_2010.\nNote: Any time you use p & q to filter the dataframe, make sure to use df[df[(p) & (q)]] or df.loc[df[(p) & (q)]]). That is, make sure to wrap conditions with parentheses to ensure the intended order of operations.\n\n\ncommon_girl_names_2010 = ...\n\ncommon_girl_names_2010\n\n\ngrader.check(\"q8\")"
  },
  {
    "objectID": "labs/lab1-pandas/lab1-pandas.html#grouping-and-aggregation",
    "href": "labs/lab1-pandas/lab1-pandas.html#grouping-and-aggregation",
    "title": "PSTAT100",
    "section": "Grouping and aggregation",
    "text": "Grouping and aggregation\nGrouping and aggregation are useful in generating data summaries, which are often important starting points in exploring a dataset.\n\nAggregation\nAggregation literally means ‘putting together’ (etymologically the word means ‘joining the herd’) – in statistics and data science, this refers to data summaries like an average, a minimum, or a measure of spread such as the sample variance or mean absolute deviation (data herding!). From a technical point of view, operations that take multiple values as inputs and return a single output are considered summaries – in other words, statistics. Some of the most common aggregations are:\n\nsum\nproduct\ncount\nnumber of distinct values\nmean\nmedian\nvariance\nstandard deviation\nminimum/maximum\nquantiles\n\nPandas has built-in dataframe operations that compute most of these summaries across either axis (column-wise or row-wise):\n\n.sum()\n.prod()\n.mean()\n.median()\n.var()\n.std()\n.nunique()\n.min() and .max()\n.quantile()\n\nTo illustrate these operations, let’s filter out all names in 1995.\n\n# filter 1995 names\nnames_95 = baby_names[baby_names.Year == 1995]\n\nHow many individuals were counted in total in 1995? We can address that by computing a sum of the counts:\n\n# n for 1995\nnames_95.Count.sum()\n\nWhat is the typical frequency of all names in 1995? We can address that by computing the average count:\n\n# average count for a name in 1995\nnames_95.Count.mean()\n\n\n\nQuestion 9\nFind how often the most common name 1995 was given and store this as names_95_max_count. Use this value to filter names_95 and find which name was most common that year. Store the filtered dataframe as names_95_most_common_name.\n\n\nnames_95_max_count = ...\nnames_95_most_common_name = ...\n\n\nprint(\"Number of people with the most frequent name in 1995 is :\", names_95_max_count, \"people\")\nprint(\"Most frequent name in 1995 is:\", names_95_most_common_name.values[0])\n\n\ngrader.check(\"q9\")\n\nCaution! If applied to the entire dataframe, the operation df.max() (or any other aggregation) will return the maximum of each column. Notice that the cell below does not return the row you found just now, but could easily be misinterpreted as such. The cell does tell you that the maximum value of sex (alphabetically last) is M and the maximum name (alphabetically last) is Zyanya and the maximum count is 5003; it does not tell you that 5003 boys were named Zyanya in 1995.\n\n# maximum of each variable\nnames_95.max()\n\n\n\nGrouping\nWhat if you want to know the most frequent male and female names? If so, you’ll need to repeat the above operations group-wise by sex.\nIn general, any variable in a dataframe can be used to define a grouping structure on the rows (or, less commonly, columns). After grouping, any dataframe operations will be executed within each group, but not across groups. This can be used to generate grouped summaries, such as the maximum count for boys and girls; as a point of terminology, we’d describe this summary as ‘maximum count by sex’.\nThe .groupby() function defines such a structure; here is the documentation. The cell below groups the names_95 dataframe by sex. Notice that when the grouped dataframe is previewed with .head(), the first few rows are returned for each group.\n\n# grouped dataframe\nnames_95_bysex = names_95.groupby('Sex')\n\n# print\nnames_95_bysex.head(2)\n\nAny aggregation operations applied to the grouped dataframe will be applied separately to the rows where Sex == M and the rows where Sex == F. For example, computing .sum() on the grouped dataframe will show the total number of individuals in the data for 1995 by sex:\n\n# number of individuals by sex\nnames_95_bysex.Count.sum()\n\nThe most frequent boy and girl names can be found using .idxmax() groupwise to obtain the index of the first occurence of the maximum count for each sex, and then slicing with .loc:\n\n# first most common names by sex\nnames_95.loc[names_95_bysex.Count.idxmax(), :]\n\nSince .idxmax() gives the index of the first occurrence, these are the alphabetically first most common names; there could be ties. You know from your work so far that there are no ties for the male names; another filtering step can be used to check for ties among the female names.\n\n# ties?\nnames_95[names_95_bysex.Count.max().values[0] == names_95['Count']]\n\nSo, no ties.\n\n\nQuestion 10\nAre there more girl names or boy names in 1995? Use the grouped dataframe names_95_bysex with the .count() aggregation to find the total number of names for each sex. Store the female and male counts as girl_name_count and boy_name_count, respectfully.\n\n\ngirl_name_count = ...\nboy_name_count = ...\n\n#print\nprint(girl_name_count)\nprint(boy_name_count)\n\n\ngrader.check(\"q10\")\n\n\n\nChaining operations\nYou have already seen examples of this, but pandas and numpy operations can be chained together in sequence. For example, names_95.Count.max() is a chain with two steps: first select the Count column (.count); then compute the maximum (.max()).\nGrouped summaries are often convenient to compute in a chained fashion, rather than by assigning the grouped dataframe a new name and performing operations on the resulting object. For example, finding the total number of boys and girls recorded in the 1995 data can be done with the following chain:\n\n# repeating previous calculation, more streamlined\nnames_95.groupby('Sex').Count.sum()\n\nWe can take this even one step further and also perform the filtering in sequence as part of the chain:\n\n# longer chain\nbaby_names[baby_names.Year == 1995].groupby('Sex').Count.sum()\n\nChains can get somewhat long, but they have the advantage of making codes more efficient, and often more readable. We did above in one step what took several lines before. Further, this chain can almost be read aloud:\n“Take baby names, filter on year, then group by sex, then select name counts, then compute the sum.”\nLet’s now consider computing the average counts of boy and girl names for each year 1990-1995. This can be accomplished by the following chain (notice it is possible to group by multiple variables).\n\n# average counts by sex and year\nbaby_names[baby_names.Year <= 1995].groupby(['Year', 'Sex']).mean(numeric_only = True)\n\nThis display is not ideal. We can ‘pivot’ the table into a wide format by adding a few extra steps in the chain: change the indices to columns; then define a new shape by specifying which column should be the new row index, which should be the new column index, and which values should populate the table.\n\n# average counts by sex and year\nbaby_names[baby_names.Year <= 1995].groupby(\n    ['Year', 'Sex']\n    ).mean(\n    numeric_only = True\n    ).reset_index().pivot(\n    index = 'Sex', columns = 'Year', values = 'Count'\n    )\n\nStyle comment: break long chains over multiple lines with indentation. The above chain is too long to be readable in one line. To balance the readability of codes with the efficiency of chaining, it is good practice to break long chains over several lines, with appropriate indentations.\nHere are some rules of thumb on style.\n\nSeparate comparisons by spaces (a<b as a < b)\nSplit chains longer than 30-40 characters over multiple lines\nSplit lines between delimiters (, )\nIncrease indentation for lines between delimiters\nFor chained operations, try to get each step in the chain shown on a separate line\nFor functions with multiple arguments, split lines so that each argument is on its own line\n\n\n\nQuestion 11\nWrite a chain with appropriate style to display the (first) most common boy and girl names in each of the years 2005-2015. Do this in two steps:\n\nFirst filter baby_names by year, then group by year and sex, and then find the indices of first occurence of the largest counts. Store these indices as ind.\nThen use .loc[] with your stored indices to slice baby_names so as to retrieve the rows corresponding to each most frequent name each year and for each sex; then pivot this table so that the columns are years, the rows are sexes, and the entries are names. Store this as pivot_names.\n\n\n\n...\nprint(ind)\npivot_names\n\n\ngrader.check(\"q11\")"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab2-sampling.ipynb\")"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#sampling-designs",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#sampling-designs",
    "title": "PSTAT100",
    "section": "Sampling designs",
    "text": "Sampling designs\nThe sampling design of a study refers to the way observational units are selected from the collection of all observational units. Any design can be expressed by the probability that each unit is included in the sample. In a random sample, all units are equally likely to be included.\nFor example, you might want to learn about U.S. residents (population), but only be able for ethical or practical reasons to study adults (sampling frame), and decide to do a mail survey of 2000 randomly selected addresses in each state (sampling design). Each collection of 2000 addresses may constitute a random sample of households, but even with a 100% response rate the survey results will not be a random sample of adult U.S. residents because individuals share addresses and the population sizes are different from state to state."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#bias",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#bias",
    "title": "PSTAT100",
    "section": "Bias",
    "text": "Bias\nFormally, bias describes the ‘typical’ deviation of a sample statistic the correspongind population value.\nFor example, if a particular sampling design tends to produce an average measurement around 1.5 units, but the true average in the population is 2 units, then the estimate has a bias of -0.5 units. The language ‘typical’ and ‘tends to’ is important here. Estimates are never perfect, so just because an estimate is off by -0.5 units for one sample doesn’t make it biased – it is only biased if it is consistently off.\nAlthough bias is technically a property of a sample statistic (like the sample average), it’s common to talk about a biased sample – this term refers to a dataset collected using a sampling design that produces biased statistics.\nThis is exactly what you’ll explore in this lab – the relationship between sampling design and bias."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#simulated-data",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#simulated-data",
    "title": "PSTAT100",
    "section": "Simulated data",
    "text": "Simulated data\nYou will be simulating data in this lab. Simulation is a great means of exploration because you can control the population properties, which are generally unknown in practice.\nWhen working with real data, you just have one dataset, and you don’t know any of the properties of the population or what might have happened if a different sample were collected. That makes it difficult to understand sampling variation and impossible to directly compare the sample properties to the population properties.\nWith simulated data, by contrast, you control how data are generated with exact precision – so by extension, you know everything there is to know about the population. In addition, repeated simulation of data makes it possible to explore the typical behavior of a particular sampling design, so you can learn ‘what usually happens’ for a particular sampling design by direct observation."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#hypothetical-population",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#hypothetical-population",
    "title": "PSTAT100",
    "section": "Hypothetical population",
    "text": "Hypothetical population\nTo provide a little context to this scenario, imagine that you’re measuring eucalyptus seeds to determine their typical diameter. The cell below simulates diameter measurements for a hypothetical population of 5000 seeds; imagine that this is the total number of seeds in a small grove at some point in time.\n\n# simulate seed diameters\nnp.random.seed(40221) # for reproducibility\npopulation = pd.DataFrame(\n    data = {'diameter': np.random.gamma(shape = 2, scale = 1/2, size = 5000), \n            'seed': np.arange(5000)}\n).set_index('seed')\n\n# check first few rows\npopulation.head(3)\n\n\n\n\n\n  \n    \n      \n      diameter\n    \n    \n      seed\n      \n    \n  \n  \n    \n      0\n      0.831973\n    \n    \n      1\n      1.512187\n    \n    \n      2\n      0.977392\n    \n  \n\n\n\n\n\nQuestion 1\nCalculate the mean diameter for the hypothetical population and store it as mean_diameter.\n\n# solution\nmean_pop_diameter = population.mean() #SOLUTION\n\nmean_pop_diameter\n\ndiameter    1.018929\ndtype: float64\n\n\n\ngrader.check(\"q1\")\n\n\n\nQuestion 2\nCalculate the standard deviation of diameters for the hypothetical population and store it as std_dev_pop_diameter.\n\n# solution\nstd_dev_pop_diameter = population.std() \nstd_dev_pop_diameter\n\ndiameter    0.72393\ndtype: float64\n\n\n\ngrader.check(\"q2\")\n\nThe cell below produces a histogram of the population values – the distribution of diameter measurements among the hypothetical population – with a vertical line indicating the population mean.\n\n# base layer\nbase_pop = alt.Chart(population).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_pop = base_pop.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20), \n              title = 'Diameter (mm)', \n              scale = alt.Scale(domain = (0, 6))),\n    y = alt.Y('count()', title = 'Number of seeds in population')\n)\n\n# vertical line for population mean\nmean_pop = base_pop.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# display\nhist_pop + mean_pop"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#random-sampling",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#random-sampling",
    "title": "PSTAT100",
    "section": "Random sampling",
    "text": "Random sampling\nImagine that your sampling design involves collecting bunches of plant material from several locations in the grove and sifting out the seeds with a fine sieve until you obtaining 250 seeds. We’ll suppose that using your collection method, any of the 5000 seeds is equally likely to be obtained, so that your 250 seeds comprise a random sample of the population.\nWe can simulate samples obtained using your hypothetical design by drawing values without replacement from the population.\n\n# draw a random sample of seeds\nnp.random.seed(40221) # for reproducibility\nsample = population.sample(n = 250, replace = False)\n\n\nQuestion 3\nCalculate the mean diameter of seeds in the simulated sample and set the result to mean_sample_diameter.\n\nmean_sample_diameter = sample.mean()\nmean_sample_diameter\n\ndiameter    0.977722\ndtype: float64\n\n\n\ngrader.check(\"q3\")\n\nYou should see above that the sample mean is close to the population mean. In fact, all sample statistics are close to the population; this can be seen by comparing the distribution of sample values with the distribution of population values.\n\n# base layer\nbase_samp = alt.Chart(sample).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_samp = base_samp.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20),\n              scale = alt.Scale(domain = (0, 6)),\n              title = 'Diameter (mm)'),\n    y = alt.Y('count()', title = 'Number of seeds in sample')\n)\n\n# vertical line for population mean\nmean_samp = base_samp.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# display\nhist_samp + mean_samp | hist_pop + mean_pop\n\n\n\n\n\n\nWhile there are some small differences, the overall shape is similar and the sample mean is almost exactly the same as the population mean. So with this sampling design, you obtained a dataset with few distortions of the population properties, and the sample mean is a good estimate of the population mean.\n\n\nAssessing bias through simulation\nYou may wonder: does that happen all the time, or was this just a lucky draw? This question can be answered by simulating a large number of samples and checking the average behavior to see whether the undistorted representation of the population is typical for this sampling design.\nThe cell below estimates the bias of the sample mean by:\n\ndrawing 1000 samples of size 300;\nstoring the sample mean from each sample;\ncomputing the average difference between the sample means and the population mean.\n\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means[i] = population.sample(n = 250, replace = False).mean()\n\nThe bias of the sample mean is its average distance from the population mean. We can estimate this using our simulation results as follows:\n\n# bias\nsamp_means.mean() - population.diameter.mean()\n\n-0.0012458197406362004\n\n\nSo the average error observed in 1000 simulations was about 0.001 mm! This suggests that the sample mean is unbiased: on average, there is no error. Therefore, at least with respect to estimating the population mean, random samples appear to be unbiased samples.\nHowever, unbiasedness does not mean that you won’t observe estimation error. There is a natural amount of variability from sample to sample, because in each sample a different collection of seeds is measured. We can estimate this as well using the simulation results by checking the standard deviation of the sample means across all 1000 samples:\n\nsamp_means.std()\n\n0.04285044949708931\n\n\nSo on average, the sample mean varies by about 0.04 mm from sample to sample.\nWe could also check how much the sample mean deviates from the population mean on average by computing root mean squared error:\n\nnp.sqrt(np.sum((samp_means - population.diameter.mean())**2)/1000)\n\n0.0428685559463899\n\n\nNote that this is very close to the variance of the sample mean across simulations, but not exactly the same; this latter calculation measures the spread around the population mean, and is a conventional measure of estimation accuracy.\nThe cell below plots a histogram representing the distribution of values of the sample mean across the 1000 samples you simulated (this is known as the sampling distribution of the sample mean). It shows a peak right at the population mean (blue vertical line) but some symmetric variation to either side – most values are between about 0.93 and 1.12.\n\n# plot the simulated sampling distribution\nsampling_dist = alt.Chart(pd.DataFrame({'sample mean': samp_means})).mark_bar().encode(\n    x = alt.X('sample mean', bin = alt.Bin(maxbins = 30), title = 'Value of sample mean'),\n    y = alt.Y('count()', title = 'Number of simulations')\n)\n\nsampling_dist + mean_pop"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#biased-sampling",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#biased-sampling",
    "title": "PSTAT100",
    "section": "Biased sampling",
    "text": "Biased sampling\nIn this scenario, you’ll use the same hypothetical population of eucalyptus seed diameter measurements and explore the impact of a biased sampling design.\nIn the first design, you were asked to imagine that you collected and sifted plant material to obtain seeds. Suppose you didn’t know that the typical seed is about 1mm in diameter and decided to use a sieve that is a little too coarse, tending only to sift out larger seeds and letting smaller seeds pass through. As a result, small seeds have a lower probability of being included in the sample and large seeds have a higher probability of being included in the sample.\nThis kind of sampling design can be described by assigning differential sampling weights \\(w_1, \\dots, w_N\\) to each observation. The cell below defines some hypothetical weights such that larger diameters are more likely to be sampled.\n\npopulation_mod1 = population.copy()\n\n\n# inclusion weight as a function of seed diameter\ndef weight_fn(x, r = 10, c = 1.5):\n    out = 1/(1 + np.e**(-r*(x - c)))\n    return out\n\n# create a grid of values to use in plotting the function\ngrid = np.linspace(0, 6, 100)\nweight_df = pd.DataFrame(\n    {'seed diameter': grid,\n     'weight': weight_fn(grid)}\n)\n\n# plot of inclusion probability against diameter\nweight_plot = alt.Chart(weight_df).mark_area(opacity = 0.3, line = True).encode(\n    x = 'seed diameter',\n    y = 'weight'\n).properties(height = 100)\n\n# show plot\nweight_plot\n\n\n\n\n\n\nThe actual probability that a seed is included in the sample – its inclusion probability – is proportional to the sampling weight. These inclusion probabilities \\(\\pi_i\\) can be calculated by normalizing the weights \\(w_i\\) over all seeds in the population \\(i = 1, \\dots, 5000\\):\n\\[\\pi_i = \\frac{w_i}{\\sum_i w_i}\\]\nIt may help you to picture how the weights will be used in sampling to line up this plot with the population distribution. In effect, we will sample more from the right tail of the population distribution, where the weight is nearest to 1.\n\nhist_pop & weight_plot\n\n\n\n\n\n\nThe following cell draws a sample with replacement from the hypothetical seed population with seeds weighted according to the inclusion probability given by the function above.\n\n# assign weight to each seed\npopulation_mod1['weight'] = weight_fn(population_mod1.diameter)\n\n# draw weighted sample\nnp.random.seed(40721)\nsample2 = population_mod1.sample(n = 250, replace = False, weights = 'weight').drop(columns = 'weight')\n\n\nQuestion 4\nCalculate the mean diameter of seeds in the simulated sample and store as mean_sample2_diameter.\n\n# solution\nmean_sample2_diameter = sample2.mean() #SOLUTION\n\nmean_sample2_diameter\n\ndiameter    2.02672\ndtype: float64\n\n\n\ngrader.check(\"q4\")\n\n\n\n\nQuestion 5\nShow side-by-side plots of the distribution of sample values and the distribution of population values, with vertical lines indicating the corresponding mean on each plot.\nHint: copy the cell that produced this plot in scenario 1 and replace sample with sample2. Utilizing different methods is also welcome.\n\n# base layer\nbase_samp = alt.Chart(sample2).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_samp = base_samp.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20),\n              scale = alt.Scale(domain = (0, 6)),\n              title = 'Diameter (mm)'),\n    y = alt.Y('count()', title = 'Number of seeds in sample')\n)\n\n# vertical line for population mean\nmean_samp = base_samp.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# display\nhist_samp + mean_samp | hist_pop + mean_pop\n\n\n\n\n\n\n\n# base layer\nbase_samp = ...\n\n# histogram of diameter measurements\nhist_samp = ...\n\n# vertical line for population mean\nmean_samp = ...\n\n# combine layers\n\n\n\n\nAssessing bias through simulation\nHere you’ll mimic the simulation done in scenario 1 to assess the bias of the sample mean under this new sampling design.\n\npopulation_mod1.head()\n\n\n\n\n\n  \n    \n      \n      diameter\n      weight\n    \n    \n      seed\n      \n      \n    \n  \n  \n    \n      0\n      0.831973\n      0.001254\n    \n    \n      1\n      1.512187\n      0.530430\n    \n    \n      2\n      0.977392\n      0.005346\n    \n    \n      3\n      2.874944\n      0.999999\n    \n    \n      4\n      0.506508\n      0.000048\n    \n  \n\n\n\n\n\n\nQuestion 6\nInvestigate the bias of the sample mean by:\n\ndrawing 1000 samples with observations weighted by inclusion probability;\nstoring the collection of sample means from each sample as samp_means;\ncomputing the average difference between the sample means and the population mean (in that order!) and storing the result as avg_diff.\n\n(Hint: copy the cell that performs this simulation in scenario 1, and be sure to replace population with population_mod1 and adjust the sampling step to include weights = ... with the appropriate argument.)\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means[i] = population_mod1.sample(\n        n = 250, \n        replace = False, \n        weights = 'weight'\n    ).drop(columns = 'weight').mean()\n\n# bias\navg_diff = samp_means.mean() - population_mod1.diameter.mean()\n\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\n\n# storage for the sample means\n\n# repeatedly sample and store the sample mean in the samp_means array\n\n# bias\n\n\ngrader.check(\"q6\")\n\n\n\n\nQuestion 7\nDoes this sampling design seem to introduce bias? If so, does the sample mean tend to over-estimate or under-estimate the population mean?\nType your answer here, replacing this text.\nSOLUTION: Yes, this sampling design seems to introduce bias, where the sample mean tends to over-estimate the population mean."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#hypothetical-population-1",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#hypothetical-population-1",
    "title": "PSTAT100",
    "section": "Hypothetical population",
    "text": "Hypothetical population\nSuppose you’re interested in determining the average beak-to-tail length of red-tailed hawks to help differentiate them from other hawks by sight at a distance. Females and males differ slightly in length – females are generally larger than males. The cell below generates length measurements for a hypothetical population of 3000 females and 2000 males.\n\n# for reproducibility\nnp.random.seed(40721)\n\n# simulate hypothetical population\nfemale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 57.5, scale = 3, size = 3000),\n            'sex': np.repeat('female', 3000)}\n)\n\nmale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 50.5, scale = 3, size = 2000),\n            'sex': np.repeat('male', 2000)}\n)\n\npopulation_hawks = pd.concat([female_hawks, male_hawks], axis = 0)\n\n# preview\npopulation_hawks.groupby('sex').head(2)\n\n\n\n\n\n  \n    \n      \n      length\n      sex\n    \n  \n  \n    \n      0\n      53.975230\n      female\n    \n    \n      1\n      60.516768\n      female\n    \n    \n      0\n      53.076663\n      male\n    \n    \n      1\n      49.933166\n      male\n    \n  \n\n\n\n\nThe cell below produces a histogram of the lengths in the population overall (bottom panel) and when distinguished by sex (top panel).\n\nbase = alt.Chart(population_hawks).properties(height = 200)\n\nhist = base.mark_bar(opacity = 0.5, color = 'red').encode(\n    x = alt.X('length', \n              bin = alt.Bin(maxbins = 40), \n              scale = alt.Scale(domain = (40, 70)),\n              title = 'length (cm)'),\n    y = alt.Y('count()', \n              stack = None,\n              title = 'number of birds')\n)\n\nhist_bysex = hist.encode(color = 'sex').properties(height = 100)\n\nhist_bysex & hist\n\n\n\n\n\n\nThe population mean – average length of both female and male red-tailed hawks – is shown below.\n\n# population mean\npopulation_hawks.mean(numeric_only = True)\n\nlength    54.737717\ndtype: float64\n\n\nFirst try drawing a random sample from the population:\n\n# for reproducibility\nnp.random.seed(40821)\n\n# randomly sample\nsample_hawks = population_hawks.sample(n = 300, replace = False)\n\n\nQuestion 8\nDo you expect that the sample will contain equal numbers of male and female hawks? Think about this for a moment (you don’t have to provide a written answer), and then compute the proportions of individuals in the sample of each sex and store the result as proportion_hawks_sample.\nHint: group by sex, use .count(), and divide by the sample size. Be sure to rename the output column appropriately, as the default behavior produces a column called length.\n\n# solution\nproportion_hawks_sample = sample_hawks.groupby('sex').count().rename(columns = {'length': 'proportion'})/300 #SOLUTION\n\nproportion_hawks_sample\n\n\n\n\n\n  \n    \n      \n      proportion\n    \n    \n      sex\n      \n    \n  \n  \n    \n      female\n      0.596667\n    \n    \n      male\n      0.403333\n    \n  \n\n\n\n\n\ngrader.check(\"q8\")\n\nThe sample mean is shown below, and is fairly close to the population mean. This should be expected, since you already saw in scenario 1 that random sampling is an unbiased sampling design with respect to the mean.\n\nsample_hawks.mean(numeric_only = True)\n\nlength    54.952103\ndtype: float64"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling-soln.html#biased-sampling-1",
    "href": "labs/lab2-sampling/lab2-sampling-soln.html#biased-sampling-1",
    "title": "PSTAT100",
    "section": "Biased sampling",
    "text": "Biased sampling\nLet’s now consider a biased sampling design. Usually, length measurements are collected from dead specimens collected opportunistically. Imagine that male mortality is higher, so there are better chances of finding dead males than dead females. Suppose in particular that specimens are five times as likely to be male; to represent this situation, we’ll assign sampling weights of 5/6 to all male hawks and weights of 1/6 to all female hawks.\n\ndef weight_fn(sex, p = 5/6):\n    if sex == 'male':\n        out = p\n    else:\n        out = 1 - p\n    return out\n\nweight_df = pd.DataFrame(\n    {'length': [50.5, 57.5],\n     'weight': [5/6, 1/6],\n     'sex': ['male', 'female']})\n\nwt = alt.Chart(weight_df).mark_bar(opacity = 0.5).encode(\n    x = alt.X('length', scale = alt.Scale(domain = (40, 70))),\n    y = alt.Y('weight', scale = alt.Scale(domain = (0, 1))),\n    color = 'sex'\n).properties(height = 70)\n\nhist_bysex & wt\n\n\n\n\n\n\n\nQuestion 9\nDraw a weighted sample sample_hawks_biased from the population population_hawks using the weights defined by weight_fn, and compute and store the sample mean as sample_hawks_biased_mean.\n\n# for reproducibility\nnp.random.seed(40821)\n\n# assign weights\npopulation_hawks['weight'] = population_hawks.sex.aggregate(func = weight_fn)\n\n# randomly sample\nsample_hawks_biased = population_hawks.sample(n = 300, replace = False, weights = 'weight').drop(columns = 'weight')\n\n# compute mean\nsample_hawks_biased_mean = sample_hawks_biased.mean(numeric_only = True)\n\n\n# for reproducibility\nnp.random.seed(40821)\n\n# assign weights\n\n# randomly sample\n\n# compute mean\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10\nInvestigate the bias of the sample mean by:\n\ndrawing 1000 samples with observations weighted by weight_fn;\nstoring the sample mean from each sample as samp_means_hawks;\ncomputing the average difference between the sample means and the population mean and storing the result as avg_diff_hawks.\n\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means_hawks = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means_hawks[i] = population_hawks.sample(\n        n = 300, \n        replace = False, \n        weights = 'weight'\n    ).drop(columns = 'weight').mean(numeric_only = True)\n\n# bias\navg_diff_hawks = samp_means_hawks.mean() - population_hawks.length.mean()\n\navg_diff_hawks\n\n-2.5720649894415715\n\n\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\n\n# storage for the sample means\n\n# repeatedly sample and store the sample mean in the samp_means array\n\n# bias\n\n\ngrader.check(\"q10\")\n\n\n\n\nQuestion 11\nReflect a moment on your simulation result in question 3c. If instead female mortality is higher and specimens for measurement are collected opportunistically, as described in the previous sampling design, do you expect that the average length in the sample will be an underestimate or an overestimate of the population mean? Explain why in 1-2 sentences, and carry out a simulation to check your intuition.\nType your answer here, replacing this text.\nSOLUTION: If female mortality was higher, then it would be an overestimate.\n\nnp.random.seed(40221) # for reproducibility\n\n# invert weights\npopulation_hawks['weight_inv'] = 1 - population_hawks.weight\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means_hawks = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means_hawks[i] = population_hawks.sample(\n        n = 300, \n        replace = False, \n        weights = 'weight_inv'\n    ).length.mean(numeric_only = True)\n\n# bias\nsamp_means_hawks.mean() - population_hawks.length.mean()\n\n1.9796123471243803"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html",
    "href": "labs/lab2-sampling/lab2-sampling.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"lab2-sampling.ipynb\")"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#sampling-designs",
    "href": "labs/lab2-sampling/lab2-sampling.html#sampling-designs",
    "title": "PSTAT100",
    "section": "Sampling designs",
    "text": "Sampling designs\nThe sampling design of a study refers to the way observational units are selected from the collection of all observational units. Any design can be expressed by the probability that each unit is included in the sample. In a random sample, all units are equally likely to be included.\nFor example, you might want to learn about U.S. residents (population), but only be able for ethical or practical reasons to study adults (sampling frame), and decide to do a mail survey of 2000 randomly selected addresses in each state (sampling design). Each collection of 2000 addresses may constitute a random sample of households, but even with a 100% response rate the survey results will not be a random sample of adult U.S. residents because individuals share addresses and the population sizes are different from state to state."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#bias",
    "href": "labs/lab2-sampling/lab2-sampling.html#bias",
    "title": "PSTAT100",
    "section": "Bias",
    "text": "Bias\nFormally, bias describes the ‘typical’ deviation of a sample statistic the correspongind population value.\nFor example, if a particular sampling design tends to produce an average measurement around 1.5 units, but the true average in the population is 2 units, then the estimate has a bias of -0.5 units. The language ‘typical’ and ‘tends to’ is important here. Estimates are never perfect, so just because an estimate is off by -0.5 units for one sample doesn’t make it biased – it is only biased if it is consistently off.\nAlthough bias is technically a property of a sample statistic (like the sample average), it’s common to talk about a biased sample – this term refers to a dataset collected using a sampling design that produces biased statistics.\nThis is exactly what you’ll explore in this lab – the relationship between sampling design and bias."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#simulated-data",
    "href": "labs/lab2-sampling/lab2-sampling.html#simulated-data",
    "title": "PSTAT100",
    "section": "Simulated data",
    "text": "Simulated data\nYou will be simulating data in this lab. Simulation is a great means of exploration because you can control the population properties, which are generally unknown in practice.\nWhen working with real data, you just have one dataset, and you don’t know any of the properties of the population or what might have happened if a different sample were collected. That makes it difficult to understand sampling variation and impossible to directly compare the sample properties to the population properties.\nWith simulated data, by contrast, you control how data are generated with exact precision – so by extension, you know everything there is to know about the population. In addition, repeated simulation of data makes it possible to explore the typical behavior of a particular sampling design, so you can learn ‘what usually happens’ for a particular sampling design by direct observation."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#hypothetical-population",
    "href": "labs/lab2-sampling/lab2-sampling.html#hypothetical-population",
    "title": "PSTAT100",
    "section": "Hypothetical population",
    "text": "Hypothetical population\nTo provide a little context to this scenario, imagine that you’re measuring eucalyptus seeds to determine their typical diameter. The cell below simulates diameter measurements for a hypothetical population of 5000 seeds; imagine that this is the total number of seeds in a small grove at some point in time.\n\n# simulate seed diameters\nnp.random.seed(40221) # for reproducibility\npopulation = pd.DataFrame(\n    data = {'diameter': np.random.gamma(shape = 2, scale = 1/2, size = 5000), \n            'seed': np.arange(5000)}\n).set_index('seed')\n\n# check first few rows\npopulation.head(3)\n\n\nQuestion 1\nCalculate the mean diameter for the hypothetical population and store it as mean_diameter.\n\n# solution\nmean_pop_diameter = ...\n\nmean_pop_diameter\n\n\ngrader.check(\"q1\")\n\n\n\nQuestion 2\nCalculate the standard deviation of diameters for the hypothetical population and store it as std_dev_pop_diameter.\n\n# solution\nstd_dev_pop_diameter = population.std() \nstd_dev_pop_diameter\n\n\ngrader.check(\"q2\")\n\nThe cell below produces a histogram of the population values – the distribution of diameter measurements among the hypothetical population – with a vertical line indicating the population mean.\n\n# base layer\nbase_pop = alt.Chart(population).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_pop = base_pop.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20), \n              title = 'Diameter (mm)', \n              scale = alt.Scale(domain = (0, 6))),\n    y = alt.Y('count()', title = 'Number of seeds in population')\n)\n\n# vertical line for population mean\nmean_pop = base_pop.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# display\nhist_pop + mean_pop"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#random-sampling",
    "href": "labs/lab2-sampling/lab2-sampling.html#random-sampling",
    "title": "PSTAT100",
    "section": "Random sampling",
    "text": "Random sampling\nImagine that your sampling design involves collecting bunches of plant material from several locations in the grove and sifting out the seeds with a fine sieve until you obtaining 250 seeds. We’ll suppose that using your collection method, any of the 5000 seeds is equally likely to be obtained, so that your 250 seeds comprise a random sample of the population.\nWe can simulate samples obtained using your hypothetical design by drawing values without replacement from the population.\n\n# draw a random sample of seeds\nnp.random.seed(40221) # for reproducibility\nsample = population.sample(n = 250, replace = False)\n\n\nQuestion 3\nCalculate the mean diameter of seeds in the simulated sample and set the result to mean_sample_diameter.\n\nmean_sample_diameter = sample.mean()\nmean_sample_diameter\n\n\ngrader.check(\"q3\")\n\nYou should see above that the sample mean is close to the population mean. In fact, all sample statistics are close to the population; this can be seen by comparing the distribution of sample values with the distribution of population values.\n\n# base layer\nbase_samp = alt.Chart(sample).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_samp = base_samp.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20),\n              scale = alt.Scale(domain = (0, 6)),\n              title = 'Diameter (mm)'),\n    y = alt.Y('count()', title = 'Number of seeds in sample')\n)\n\n# vertical line for population mean\nmean_samp = base_samp.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# display\nhist_samp + mean_samp | hist_pop + mean_pop\n\nWhile there are some small differences, the overall shape is similar and the sample mean is almost exactly the same as the population mean. So with this sampling design, you obtained a dataset with few distortions of the population properties, and the sample mean is a good estimate of the population mean.\n\n\nAssessing bias through simulation\nYou may wonder: does that happen all the time, or was this just a lucky draw? This question can be answered by simulating a large number of samples and checking the average behavior to see whether the undistorted representation of the population is typical for this sampling design.\nThe cell below estimates the bias of the sample mean by:\n\ndrawing 1000 samples of size 300;\nstoring the sample mean from each sample;\ncomputing the average difference between the sample means and the population mean.\n\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means[i] = population.sample(n = 250, replace = False).mean()\n\nThe bias of the sample mean is its average distance from the population mean. We can estimate this using our simulation results as follows:\n\n# bias\nsamp_means.mean() - population.diameter.mean()\n\nSo the average error observed in 1000 simulations was about 0.001 mm! This suggests that the sample mean is unbiased: on average, there is no error. Therefore, at least with respect to estimating the population mean, random samples appear to be unbiased samples.\nHowever, unbiasedness does not mean that you won’t observe estimation error. There is a natural amount of variability from sample to sample, because in each sample a different collection of seeds is measured. We can estimate this as well using the simulation results by checking the standard deviation of the sample means across all 1000 samples:\n\nsamp_means.std()\n\nSo on average, the sample mean varies by about 0.04 mm from sample to sample.\nWe could also check how much the sample mean deviates from the population mean on average by computing root mean squared error:\n\nnp.sqrt(np.sum((samp_means - population.diameter.mean())**2)/1000)\n\nNote that this is very close to the variance of the sample mean across simulations, but not exactly the same; this latter calculation measures the spread around the population mean, and is a conventional measure of estimation accuracy.\nThe cell below plots a histogram representing the distribution of values of the sample mean across the 1000 samples you simulated (this is known as the sampling distribution of the sample mean). It shows a peak right at the population mean (blue vertical line) but some symmetric variation to either side – most values are between about 0.93 and 1.12.\n\n# plot the simulated sampling distribution\nsampling_dist = alt.Chart(pd.DataFrame({'sample mean': samp_means})).mark_bar().encode(\n    x = alt.X('sample mean', bin = alt.Bin(maxbins = 30), title = 'Value of sample mean'),\n    y = alt.Y('count()', title = 'Number of simulations')\n)\n\nsampling_dist + mean_pop"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#biased-sampling",
    "href": "labs/lab2-sampling/lab2-sampling.html#biased-sampling",
    "title": "PSTAT100",
    "section": "Biased sampling",
    "text": "Biased sampling\nIn this scenario, you’ll use the same hypothetical population of eucalyptus seed diameter measurements and explore the impact of a biased sampling design.\nIn the first design, you were asked to imagine that you collected and sifted plant material to obtain seeds. Suppose you didn’t know that the typical seed is about 1mm in diameter and decided to use a sieve that is a little too coarse, tending only to sift out larger seeds and letting smaller seeds pass through. As a result, small seeds have a lower probability of being included in the sample and large seeds have a higher probability of being included in the sample.\nThis kind of sampling design can be described by assigning differential sampling weights \\(w_1, \\dots, w_N\\) to each observation. The cell below defines some hypothetical weights such that larger diameters are more likely to be sampled.\n\npopulation_mod1 = population.copy()\n\n\n# inclusion weight as a function of seed diameter\ndef weight_fn(x, r = 10, c = 1.5):\n    out = 1/(1 + np.e**(-r*(x - c)))\n    return out\n\n# create a grid of values to use in plotting the function\ngrid = np.linspace(0, 6, 100)\nweight_df = pd.DataFrame(\n    {'seed diameter': grid,\n     'weight': weight_fn(grid)}\n)\n\n# plot of inclusion probability against diameter\nweight_plot = alt.Chart(weight_df).mark_area(opacity = 0.3, line = True).encode(\n    x = 'seed diameter',\n    y = 'weight'\n).properties(height = 100)\n\n# show plot\nweight_plot\n\nThe actual probability that a seed is included in the sample – its inclusion probability – is proportional to the sampling weight. These inclusion probabilities \\(\\pi_i\\) can be calculated by normalizing the weights \\(w_i\\) over all seeds in the population \\(i = 1, \\dots, 5000\\):\n\\[\\pi_i = \\frac{w_i}{\\sum_i w_i}\\]\nIt may help you to picture how the weights will be used in sampling to line up this plot with the population distribution. In effect, we will sample more from the right tail of the population distribution, where the weight is nearest to 1.\n\nhist_pop & weight_plot\n\nThe following cell draws a sample with replacement from the hypothetical seed population with seeds weighted according to the inclusion probability given by the function above.\n\n# assign weight to each seed\npopulation_mod1['weight'] = weight_fn(population_mod1.diameter)\n\n# draw weighted sample\nnp.random.seed(40721)\nsample2 = population_mod1.sample(n = 250, replace = False, weights = 'weight').drop(columns = 'weight')\n\n\nQuestion 4\nCalculate the mean diameter of seeds in the simulated sample and store as mean_sample2_diameter.\n\n# solution\nmean_sample2_diameter = ...\n\nmean_sample2_diameter\n\n\ngrader.check(\"q4\")\n\n\n\n\nQuestion 5\nShow side-by-side plots of the distribution of sample values and the distribution of population values, with vertical lines indicating the corresponding mean on each plot.\nHint: copy the cell that produced this plot in scenario 1 and replace sample with sample2. Utilizing different methods is also welcome.\n\n# base layer\nbase_samp = alt.Chart(sample2).properties(width = 400, height = 300)\n\n# histogram of diameter measurements\nhist_samp = base_samp.mark_bar(opacity = 0.8).encode(\n    x = alt.X('diameter', \n              bin = alt.Bin(maxbins = 20),\n              scale = alt.Scale(domain = (0, 6)),\n              title = 'Diameter (mm)'),\n    y = alt.Y('count()', title = 'Number of seeds in sample')\n)\n\n# vertical line for population mean\nmean_samp = base_samp.mark_rule(color='blue').encode(\n    x = 'mean(diameter)'\n)\n\n# display\nhist_samp + mean_samp | hist_pop + mean_pop\n\n\n# base layer\nbase_samp = ...\n\n# histogram of diameter measurements\nhist_samp = ...\n\n# vertical line for population mean\nmean_samp = ...\n\n# combine layers\n\n\n\n\nAssessing bias through simulation\nHere you’ll mimic the simulation done in scenario 1 to assess the bias of the sample mean under this new sampling design.\n\npopulation_mod1.head()\n\n\n\nQuestion 6\nInvestigate the bias of the sample mean by:\n\ndrawing 1000 samples with observations weighted by inclusion probability;\nstoring the collection of sample means from each sample as samp_means;\ncomputing the average difference between the sample means and the population mean (in that order!) and storing the result as avg_diff.\n\n(Hint: copy the cell that performs this simulation in scenario 1, and be sure to replace population with population_mod1 and adjust the sampling step to include weights = ... with the appropriate argument.)\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means[i] = population_mod1.sample(\n        n = 250, \n        replace = False, \n        weights = 'weight'\n    ).drop(columns = 'weight').mean()\n\n# bias\navg_diff = samp_means.mean() - population_mod1.diameter.mean()\n\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\n\n# storage for the sample means\n\n# repeatedly sample and store the sample mean in the samp_means array\n\n# bias\n\n\ngrader.check(\"q6\")\n\n\n\n\nQuestion 7\nDoes this sampling design seem to introduce bias? If so, does the sample mean tend to over-estimate or under-estimate the population mean?\nType your answer here, replacing this text."
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#hypothetical-population-1",
    "href": "labs/lab2-sampling/lab2-sampling.html#hypothetical-population-1",
    "title": "PSTAT100",
    "section": "Hypothetical population",
    "text": "Hypothetical population\nSuppose you’re interested in determining the average beak-to-tail length of red-tailed hawks to help differentiate them from other hawks by sight at a distance. Females and males differ slightly in length – females are generally larger than males. The cell below generates length measurements for a hypothetical population of 3000 females and 2000 males.\n\n# for reproducibility\nnp.random.seed(40721)\n\n# simulate hypothetical population\nfemale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 57.5, scale = 3, size = 3000),\n            'sex': np.repeat('female', 3000)}\n)\n\nmale_hawks = pd.DataFrame(\n    data = {'length': np.random.normal(loc = 50.5, scale = 3, size = 2000),\n            'sex': np.repeat('male', 2000)}\n)\n\npopulation_hawks = pd.concat([female_hawks, male_hawks], axis = 0)\n\n# preview\npopulation_hawks.groupby('sex').head(2)\n\nThe cell below produces a histogram of the lengths in the population overall (bottom panel) and when distinguished by sex (top panel).\n\nbase = alt.Chart(population_hawks).properties(height = 200)\n\nhist = base.mark_bar(opacity = 0.5, color = 'red').encode(\n    x = alt.X('length', \n              bin = alt.Bin(maxbins = 40), \n              scale = alt.Scale(domain = (40, 70)),\n              title = 'length (cm)'),\n    y = alt.Y('count()', \n              stack = None,\n              title = 'number of birds')\n)\n\nhist_bysex = hist.encode(color = 'sex').properties(height = 100)\n\nhist_bysex & hist\n\nThe population mean – average length of both female and male red-tailed hawks – is shown below.\n\n# population mean\npopulation_hawks.mean(numeric_only = True)\n\nFirst try drawing a random sample from the population:\n\n# for reproducibility\nnp.random.seed(40821)\n\n# randomly sample\nsample_hawks = population_hawks.sample(n = 300, replace = False)\n\n\nQuestion 8\nDo you expect that the sample will contain equal numbers of male and female hawks? Think about this for a moment (you don’t have to provide a written answer), and then compute the proportions of individuals in the sample of each sex and store the result as proportion_hawks_sample.\nHint: group by sex, use .count(), and divide by the sample size. Be sure to rename the output column appropriately, as the default behavior produces a column called length.\n\n# solution\nproportion_hawks_sample = ...\n\nproportion_hawks_sample\n\n\ngrader.check(\"q8\")\n\nThe sample mean is shown below, and is fairly close to the population mean. This should be expected, since you already saw in scenario 1 that random sampling is an unbiased sampling design with respect to the mean.\n\nsample_hawks.mean(numeric_only = True)"
  },
  {
    "objectID": "labs/lab2-sampling/lab2-sampling.html#biased-sampling-1",
    "href": "labs/lab2-sampling/lab2-sampling.html#biased-sampling-1",
    "title": "PSTAT100",
    "section": "Biased sampling",
    "text": "Biased sampling\nLet’s now consider a biased sampling design. Usually, length measurements are collected from dead specimens collected opportunistically. Imagine that male mortality is higher, so there are better chances of finding dead males than dead females. Suppose in particular that specimens are five times as likely to be male; to represent this situation, we’ll assign sampling weights of 5/6 to all male hawks and weights of 1/6 to all female hawks.\n\ndef weight_fn(sex, p = 5/6):\n    if sex == 'male':\n        out = p\n    else:\n        out = 1 - p\n    return out\n\nweight_df = pd.DataFrame(\n    {'length': [50.5, 57.5],\n     'weight': [5/6, 1/6],\n     'sex': ['male', 'female']})\n\nwt = alt.Chart(weight_df).mark_bar(opacity = 0.5).encode(\n    x = alt.X('length', scale = alt.Scale(domain = (40, 70))),\n    y = alt.Y('weight', scale = alt.Scale(domain = (0, 1))),\n    color = 'sex'\n).properties(height = 70)\n\nhist_bysex & wt\n\n\nQuestion 9\nDraw a weighted sample sample_hawks_biased from the population population_hawks using the weights defined by weight_fn, and compute and store the sample mean as sample_hawks_biased_mean.\n\n# for reproducibility\nnp.random.seed(40821)\n\n# assign weights\npopulation_hawks['weight'] = population_hawks.sex.aggregate(func = weight_fn)\n\n# randomly sample\nsample_hawks_biased = population_hawks.sample(n = 300, replace = False, weights = 'weight').drop(columns = 'weight')\n\n# compute mean\nsample_hawks_biased_mean = sample_hawks_biased.mean(numeric_only = True)\n\n\n# for reproducibility\nnp.random.seed(40821)\n\n# assign weights\n\n# randomly sample\n\n# compute mean\n\n\ngrader.check(\"q9\")\n\n\n\nQuestion 10\nInvestigate the bias of the sample mean by:\n\ndrawing 1000 samples with observations weighted by weight_fn;\nstoring the sample mean from each sample as samp_means_hawks;\ncomputing the average difference between the sample means and the population mean and storing the result as avg_diff_hawks.\n\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means_hawks = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means_hawks[i] = population_hawks.sample(\n        n = 300, \n        replace = False, \n        weights = 'weight'\n    ).drop(columns = 'weight').mean(numeric_only = True)\n\n# bias\navg_diff_hawks = samp_means_hawks.mean() - population_hawks.length.mean()\n\navg_diff_hawks\n\n\nnp.random.seed(40221) # for reproducibility\n\n# number of samples to simulate\n\n# storage for the sample means\n\n# repeatedly sample and store the sample mean in the samp_means array\n\n# bias\n\n\ngrader.check(\"q10\")\n\n\n\n\nQuestion 11\nReflect a moment on your simulation result in question 3c. If instead female mortality is higher and specimens for measurement are collected opportunistically, as described in the previous sampling design, do you expect that the average length in the sample will be an underestimate or an overestimate of the population mean? Explain why in 1-2 sentences, and carry out a simulation to check your intuition.\nType your answer here, replacing this text.\n\nnp.random.seed(40221) # for reproducibility\n\n# invert weights\npopulation_hawks['weight_inv'] = 1 - population_hawks.weight\n\n# number of samples to simulate\nnsim = 1000\n\n# storage for the sample means\nsamp_means_hawks = np.zeros(nsim)\n\n# repeatedly sample and store the sample mean\nfor i in range(0, nsim):\n    samp_means_hawks[i] = population_hawks.sample(\n        n = 300, \n        replace = False, \n        weights = 'weight_inv'\n    ).length.mean(numeric_only = True)\n\n# bias\nsamp_means_hawks.mean() - population_hawks.length.mean()"
  },
  {
    "objectID": "miscellany.html",
    "href": "miscellany.html",
    "title": "Miscellany",
    "section": "",
    "text": "Automated tests in assignment notebooks are a guide, not a confirmation or refutation of your answer. Don’t rely too heavily on them, but do read the output message if they fail and think about what the message is telling you. On some occasions they will fail despite a correct answer; on others they will pass despite an incorrect answer. Furthermore, they will guide you to one particular strategy for obtaining the solution; most problems admit a few possible strategies.\nAll assignments are due on Mondays. You get two free late assignments. Late submissions are due Wednesdays.\nStart your homeworks and mini-projects early.\nTake your own notes during class; don’t simply rely on lecture slides."
  },
  {
    "objectID": "miscellany.html#troubleshooting",
    "href": "miscellany.html#troubleshooting",
    "title": "Miscellany",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nIf you need to recover the distribution copy of any assignment notebook, perhaps due to accidentally deleting cells or similar issues, simply rename the notebook containing your work on the LSIT server and then redeploy the notebook from the course website link.\nIf you try to open a notebook and the server fails at the ‘synchronizing git repository’ stage, open the LSIT server separately, rename the pstat100-content directory, and then try opening the notebook from the website link again. If successfull, you will need to migrate all of your previous work into the new pstat100-content directory."
  },
  {
    "objectID": "projects/mp1/mp1-airquality-soln.html",
    "href": "projects/mp1/mp1-airquality-soln.html",
    "title": "PSTAT100",
    "section": "",
    "text": "# packages\nimport numpy as np\nimport pandas as pd\n\n# read in raw data file\nair_raw = pd.read_csv('air-raw.csv')\n\n# split off city info\ncbsa_info = air_raw.iloc[:, 0:2].dropna().set_index('CBSA')\ncbsa_info.to_csv('cbsa-info.csv')\n\n# remove city, state from air quality data\nair_quality = air_raw.drop(columns = 'Core Based Statistical Area').set_index('CBSA')\nair_quality.to_csv('air-quality.csv')"
  },
  {
    "objectID": "projects/mp1/mp1-airquality-soln.html#part-i-dataset",
    "href": "projects/mp1/mp1-airquality-soln.html#part-i-dataset",
    "title": "PSTAT100",
    "section": "Part I: Dataset",
    "text": "Part I: Dataset\nMerge the city information with the air quality data and tidy the dataset (see notes below). Write a brief description of the data.\nIn your description, answer the following questions:\n\nWhat is a CBSA (the geographic unit of measurement)?\nHow many CBSA’s are included in the data?\nIn how many states and territories do the CBSA’s reside?\nIn which years were data values recorded?\nHow many observations are recorded?\nHow many variables are measured?\nWhich variables are non-missing most of the time (i.e., in at least 50% of instances)?\nWhat is PM 2.5 and why is it important?\n\nPlease write your description in narrative fashion; please do not list answers to the questions above one by one. A few brief paragraphs should suffice; please limit your data description to three paragraphs or less.\n\nAir quality data\nWrite your description here."
  },
  {
    "objectID": "projects/mp1/mp1-airquality-soln.html#part-ii-descriptive-analysis",
    "href": "projects/mp1/mp1-airquality-soln.html#part-ii-descriptive-analysis",
    "title": "PSTAT100",
    "section": "Part II: Descriptive analysis",
    "text": "Part II: Descriptive analysis\nFocus on the PM2.5 measurements that are non-missing most of the time. Answer each of the following questions in a brief paragraph or two. Your paragraph(s) should indicate both your answer and a description of how you obtained it; please do not include codes with your answers.\n\nHas PM 2.5 air pollution improved in the average U.S. city since 2000?\nWrite your answer here.\n\n\nOver time, has PM 2.5 pollution become more variable, less variable, or about the same from city to city?\nWrite your answer here.\n\n\nWhich state has seen the greatest improvement over time?\nWrite your answer here.\n\n\nChoose a location with some meaning to you (e.g. hometown, family lives there, took a vacation there, etc.). Was that location in compliance with EPA primary standards as of the most recent measurement?\nWrite your answer here."
  },
  {
    "objectID": "projects/mp1/mp1-airquality.html",
    "href": "projects/mp1/mp1-airquality.html",
    "title": "PSTAT100",
    "section": "",
    "text": "In a way, this project is simple: you are given some data on air quality in U.S. metropolitan areas over time together with several questions of interest, and your objective is to answer the questions.\nHowever, unlike the homeworks and labs, there is no explicit instruction provided about how to answer the questions or where exactly to begin. Thus, you will need to discern for yourself how to manipulate and summarize the data in order to answer the questions of interest, and you will need to write your own codes from scratch to obtain results. It is recommended that you examine the data, consider the questions, and plan a rough approach before you begin doing any computations.\nYou have some latitude for creativity: although there are accurate answers to each question – namely, those that are consistent with the data – there is no singularly correct answer. Most students will perform similar operations and obtain similar answers, but there’s no specific result that must be considered to answer the questions accurately. As a result, your approaches and answers may differ from those of your classmates. If you choose to discuss your work with others, you may even find that disagreements prove to be fertile learning opportunities.\nThe questions can be answered using computing skills taught in class so far and basic internet searches for domain background; for this project, you may wish to refer to HW1 and Lab1 for code examples and the EPA website on PM pollution for background. However, you are also encouraged to refer to external resources (package documentation, vignettes, stackexchange, internet searches, etc.) as needed – this may be an especially good idea if you find yourself thinking, ‘it would be really handy to do X, but I haven’t seen that in class anywhere’.\nThe broader goal of these mini projects is to cultivate your problem-solving ability in an unstructured setting. Your work will be evaluated based on the following: - choice of method(s) used to answer questions; - clarity of presentation; - code style and documentation.\nPlease write up your results separately from your codes; codes should be included at the end of the notebook.\n\n\n\nMerge the city information with the air quality data and tidy the dataset (see notes below). Write a one- to two-paragraph description of the data.\nIn your description, answer the following questions:\n\nWhat is a CBSA (the geographic unit of measurement)?\nHow many CBSA’s are included in the data?\nIn how many states and territories do the CBSA’s reside? (Hint: str.split())\nIn which years were data values recorded?\nHow many observations are recorded?\nHow many variables are measured?\nWhich variables are non-missing most of the time (i.e., in at least 50% of instances)?\nWhat is PM 2.5 and why is it important?\nWhat are the basic statistical properties of the variable(s) of interest?\n\nPlease write your description in narrative fashion; please do not list answers to the questions above one by one.\n\n\nWrite your description here.\n\n\n\n\nFocus on the PM2.5 measurements that are non-missing most of the time. Answer each of the following questions in a brief paragraph or two. Do not describe your analyses step-by-step for your answers; instead, report your findings. Your paragraph(s) should indicate both your answer to the question and a justification for your answer; please do not include codes with your answers.\n\n\nWrite your answer here.\n\n\n\nWrite your answer here.\n\n\n\nWrite your answer here. Be sure to explain how you defined ‘best improvement’ in each case.\n\n\n\nWrite your answer here.\n\n\n\n\nOne strategy for filling in missing values (‘imputation’) is to use non-missing values to predict the missing ones; the success of this strategy depends in part on the strength of relationship between the variable(s) used as predictors of missing values.\nIdentify one other pollutant that might be a good candidate for imputation based on the PM 2.5 measurements and explain why you selected the variable you did. Can you envision any potential pitfalls to this technique?"
  },
  {
    "objectID": "projects/mp1/mp1-airquality.html#notes-on-merging-keep-at-bottom-of-notebook",
    "href": "projects/mp1/mp1-airquality.html#notes-on-merging-keep-at-bottom-of-notebook",
    "title": "PSTAT100",
    "section": "Notes on merging (keep at bottom of notebook)",
    "text": "Notes on merging (keep at bottom of notebook)\nTo combine datasets based on shared information, you can use the pd.merge(A, B, how = ..., on = SHARED_COLS) function, which will match the rows of A and B based on the shared columns SHARED_COLS. If how = 'left', then only rows in A will be retained in the output (so B will be merged to A); conversely, if how = 'right', then only rows in B will be retained in the output (so A will be merged to B).\nA simple example of the use of pd.merge is illustrated below:\n\n# toy data frames\nA = pd.DataFrame(\n    {'shared_col': ['a', 'b', 'c'], \n    'x1': [1, 2, 3], \n    'x2': [4, 5, 6]}\n)\n\nB = pd.DataFrame(\n    {'shared_col': ['a', 'b'], \n    'y1': [7, 8]}\n)\n\n\nA\n\n\nB\n\nBelow, if A and B are merged retaining the rows in A, notice that a missing value is input because B has no row where the shared column (on which the merging is done) has value c. In other words, the third row of A has no match in B.\n\n# left join\npd.merge(A, B, how = 'left', on = 'shared_col')\n\nIf the direction of merging is reversed, and the row structure of B is dominant, then the third row of A is dropped altogether because it has no match in B.\n\n# right join\npd.merge(A, B, how = 'right', on = 'shared_col')"
  },
  {
    "objectID": "slides/week1-intro.html#attendance-form",
    "href": "slides/week1-intro.html#attendance-form",
    "title": "Course introduction",
    "section": "Attendance form",
    "text": "Attendance form"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-1-ace-and-health",
    "href": "slides/week1-intro.html#case-study-1-ace-and-health",
    "title": "Course introduction",
    "section": "Case study 1: ACE and health",
    "text": "Case study 1: ACE and health\n\nAssociation between adverse childhood experiences and general health, by sex."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-1-ace-and-health-1",
    "href": "slides/week1-intro.html#case-study-1-ace-and-health-1",
    "title": "Course introduction",
    "section": "Case study 1: ACE and health",
    "text": "Case study 1: ACE and health\nYou will:\n\nprocess and recode 10K survey responses from CDC’s 2019 behavior risk factor surveillance survey (BRFSS)\ncross-tabulate health-related measurements with frequency of adverse childhood experiences"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-2-seda",
    "href": "slides/week1-intro.html#case-study-2-seda",
    "title": "Course introduction",
    "section": "Case study 2: SEDA",
    "text": "Case study 2: SEDA\n\nEducation achievement gaps as functions of socioeconomic indicators, by gender."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-2-seda-1",
    "href": "slides/week1-intro.html#case-study-2-seda-1",
    "title": "Course introduction",
    "section": "Case study 2: SEDA",
    "text": "Case study 2: SEDA\nYou will:\n\nmerge test scores and socioeconomic indicators from the 2018 Standford Education Data Archive by school district\nvisually assess correlations between gender achievement gaps among grade schoolers and socioeconomic indicators across school districts in CA"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\n\nSea surface temperature reconstruction over the past 16,000 years."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology-1",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology-1",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\n\nClustering of diatom relative abundances in pleistocene (pre-11KyBP) vs. holocene (post-11KyBP) epochs."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-3-paleoclimatology-2",
    "href": "slides/week1-intro.html#case-study-3-paleoclimatology-2",
    "title": "Course introduction",
    "section": "Case study 3: Paleoclimatology",
    "text": "Case study 3: Paleoclimatology\nYou will:\n\nexplore ecological community structure from relative abundances of diatoms measured in ocean sediment core samples spanning ~15,000 years\nuse dimension reduction techniques to obtain measures of community structure\nidentify shifts associated with the transition from pleistocene to holocene epochs"
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nApparent disparity in allocation of DDS benefits across racial groups."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-1",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-1",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nExpenditure is strongly associated with age."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-2",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-2",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\n\nCorrecting for age shows comparable expenditure across racial groups."
  },
  {
    "objectID": "slides/week1-intro.html#case-study-4-discrimination-at-dds-3",
    "href": "slides/week1-intro.html#case-study-4-discrimination-at-dds-3",
    "title": "Course introduction",
    "section": "Case study 4: Discrimination at DDS?",
    "text": "Case study 4: Discrimination at DDS?\nYou will:\n\nassess the case for discrimination in allocation of DDS benefits\nidentify confounding factors present in the sample\nmodel median expenditure by racial group after correcting for age"
  },
  {
    "objectID": "slides/week1-intro.html#scope",
    "href": "slides/week1-intro.html#scope",
    "title": "Course introduction",
    "section": "Scope",
    "text": "Scope\nThis course is about developing your data science toolkit with foundational skills:\n\nCore competency with Python data science libraries\nCritical thinking about data\nVisualization and exploratory analysis\nApplication of basic statistical concepts and methods in practice\nCommunication and interpretation of results"
  },
  {
    "objectID": "slides/week1-intro.html#whats-unique-about-pstat100",
    "href": "slides/week1-intro.html#whats-unique-about-pstat100",
    "title": "Course introduction",
    "section": "What’s unique about PSTAT100?",
    "text": "What’s unique about PSTAT100?\nThere are a few distinctive aspects:\n\nmultiple end-to-end case studies\nquestion-driven rather than method-driven\nemphasis on project workflow\ndata storytelling and communication"
  },
  {
    "objectID": "slides/week1-intro.html#limitations",
    "href": "slides/week1-intro.html#limitations",
    "title": "Course introduction",
    "section": "Limitations",
    "text": "Limitations\nThere are also some things we won’t cover:\n\nPredictive modeling or machine learning\nAlgorithm design and implementation\nTechniques and methods for big data\nTheoretical basis for methods"
  },
  {
    "objectID": "slides/week1-intro.html#weekly-pattern",
    "href": "slides/week1-intro.html#weekly-pattern",
    "title": "Course introduction",
    "section": "Weekly Pattern",
    "text": "Weekly Pattern\nWe’ll follow a simple weekly pattern:\n\nMondays\n\nLecture\nSections\nAssignments due 11:59pm PST\n\nWednesdays\n\nLecture\nLate work due 11:59pm PST"
  },
  {
    "objectID": "slides/week1-intro.html#course-pages-materials",
    "href": "slides/week1-intro.html#course-pages-materials",
    "title": "Course introduction",
    "section": "Course pages & materials",
    "text": "Course pages & materials\n\nMaterials via course website ruizt.github.io/pstat100\nComputing at pstat100.lsit.ucsb.edu\nAssignments/gradebook at Gradescope\nDiscussion board (TBA)"
  },
  {
    "objectID": "slides/week1-intro.html#tentative-schedule",
    "href": "slides/week1-intro.html#tentative-schedule",
    "title": "Course introduction",
    "section": "Tentative schedule",
    "text": "Tentative schedule\n\n\n\nWeek\nTopic\nLab\nHomework\nProject\n\n\n\n\n1\nData science life cycle\n\n\n\n\n\n2\nTidy data\nL0\n\n\n\n\n3\nSampling and bias\nL1\n\n\n\n\n4\nStatistical graphics\nL2\nH1\n\n\n\n5\nKernel density estimation\nL3\n\nMP1\n\n\n6\nPrincipal components\nL4\nH2\n\n\n\n7\nSimple regression\n\n\nMP2\n\n\n8\nMultiple regression\nL5\nH3\n\n\n\n9\nClassification and clustering\n\n\nCP1\n\n\n10\nCase study\n\nH4\n\n\n\n11\nFinals week\n\n\nCP2"
  },
  {
    "objectID": "slides/week1-intro.html#assessments",
    "href": "slides/week1-intro.html#assessments",
    "title": "Course introduction",
    "section": "Assessments",
    "text": "Assessments\n\nLabs introduce and develop core skills\nHomeworks apply core skills to case studies\nProjects practice creative problem-solving"
  },
  {
    "objectID": "slides/week1-intro.html#policies",
    "href": "slides/week1-intro.html#policies",
    "title": "Course introduction",
    "section": "Policies",
    "text": "Policies\n\nCommunication\n\nIf you have questions, please come to office hours\nAvoid email except for personal matters\n\nDeadlines and late work\n\nOne-hour grace period on all deadlines\n48-hour late submissions\nTwo free lates on any assignment (except last assignment)\n75% partial credit thereafter for late work"
  },
  {
    "objectID": "slides/week1-intro.html#policies-1",
    "href": "slides/week1-intro.html#policies-1",
    "title": "Course introduction",
    "section": "Policies",
    "text": "Policies\n\nGrades\n\nRoughly 10-20-30-40 attendance-labs-homeworks-projects\nFinal weighting and grade assignment at instructor’s discretion\nDo not expect 92+% = A, 90-92% = A-, 87-89.9 = B+, etc.\nA’s are awarded sparingly and indicate exceptional work"
  },
  {
    "objectID": "slides/week1-intro.html#other-info",
    "href": "slides/week1-intro.html#other-info",
    "title": "Course introduction",
    "section": "Other info",
    "text": "Other info\n\nInformal section swaps are allowed with TA permission\nAttendance required at all class meetings, but a few absences without notice are okay\nHonors contracts not available this quarter\nOffice hours start week 2, check website for schedule"
  },
  {
    "objectID": "slides/week1-intro.html#getting-started",
    "href": "slides/week1-intro.html#getting-started",
    "title": "Course introduction",
    "section": "Getting started",
    "text": "Getting started\n\nLab this week will introduce you to computing and course infrastructure\nPlease fill out intake survey ASAP\nCheck access to Gradescope, LSIT, course page\nReview syllabus"
  },
  {
    "objectID": "slides/week1-lifecycle.html#whats-data-science",
    "href": "slides/week1-lifecycle.html#whats-data-science",
    "title": "Data science lifecycle",
    "section": "What’s data science?",
    "text": "What’s data science?\nData science is a term of art encompassing a wide range of activities that involve uncovering insights from quantitative information.\n\nPeople that refer to themselves as data scientists typically combine specific interests (“domain knowledge”, e.g., biology) with computation, mathematics, and statistics and probability to contribute to knowledge in their communities.\n\nIntersectional in nature\nNo singular disciplinary background among practitioners"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-science-lifecycle",
    "href": "slides/week1-lifecycle.html#data-science-lifecycle",
    "title": "Data science lifecycle",
    "section": "Data science lifecycle",
    "text": "Data science lifecycle\n\nData science lifecycle: an end-to-end process resulting in a data analysis product\n\n\nQuestion formulation\nData collection and cleaning\nExploration\nAnalysis\n\n\nThese form a cycle in the sense that the steps are iterated for question refinement and futher discovery."
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-science-lifecylce",
    "href": "slides/week1-lifecycle.html#data-science-lifecylce",
    "title": "Data science lifecycle",
    "section": "Data science lifecylce",
    "text": "Data science lifecylce\n\n\nThe point isn’t really the exact steps, but rather the notion of an iterative process."
  },
  {
    "objectID": "slides/week1-lifecycle.html#starting-with-a-question",
    "href": "slides/week1-lifecycle.html#starting-with-a-question",
    "title": "Data science lifecycle",
    "section": "Starting with a question",
    "text": "Starting with a question\nThe scaling of brains with bodies is thought to contain clues about evolutionary patterns pertaining to intelligence.\n\nThere are lots of datasets out there with brain and body weight measurements, so let’s consider the question:\n\nWhat is the relationship between an animal’s brain and body weight?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-acquisition",
    "href": "slides/week1-lifecycle.html#data-acquisition",
    "title": "Data science lifecycle",
    "section": "Data acquisition",
    "text": "Data acquisition\nFrom Allison et al. 1976, average body and brain weights for 62 mammals.\n\n\n\n\n\n\n  \n    \n      \n      species\n      body_wt\n      brain_wt\n    \n  \n  \n    \n      0\n      Africanelephant\n      6654.000\n      5712.0\n    \n    \n      1\n      Africangiantpouchedrat\n      1.000\n      6.6\n    \n    \n      2\n      ArcticFox\n      3.385\n      44.5\n    \n  \n\n\n\n\nUnits of measurement\n\nbody weight in kilograms\nbrain weight in grams"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment",
    "href": "slides/week1-lifecycle.html#data-assessment",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nHow well-matched is the data to our question?\n\nMammals only (no birds, fish, reptiles, etc.)\nSpecies are those for which convenient specimens were available\nAverages across specimens are reported (‘aggregated’ data)\n\n\nWhat do you think? Take a moment to discuss with your neighbor."
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment-1",
    "href": "slides/week1-lifecycle.html#data-assessment-1",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nBased on the great points you just made, we really only stand to learn something about this particular sample of animals.\n\nIn other words, no inference is possible.\n\n\n\nDo you think the data are still useful?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#inpection",
    "href": "slides/week1-lifecycle.html#inpection",
    "title": "Data science lifecycle",
    "section": "Inpection",
    "text": "Inpection\nThis dataset is already impeccably neat: each row is an observation for some species of mammal, and the columns are the two variables (average weight).\nSo no tidying needed – we’ll just check the dimensions and see if any values are missing.\n\n# dimensions?\nbb_weights.shape\n\n(62, 3)\n\n\n\n# missing values?\nbb_weights.isna().sum(axis = 0)\n\nspecies     0\nbody_wt     0\nbrain_wt    0\ndtype: int64"
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration",
    "href": "slides/week1-lifecycle.html#exploration",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nVisualization is usually a good starting point for exploring data.\n\n\n\n\n\n\n\nNotice the apparent density of points near \\((0, 0)\\) – that suggests we shouldn’t look for a relationship on the scale of kg/g."
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration-1",
    "href": "slides/week1-lifecycle.html#exploration-1",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nA simple transformation of the axes reveals a clearer pattern."
  },
  {
    "objectID": "slides/week1-lifecycle.html#analysis",
    "href": "slides/week1-lifecycle.html#analysis",
    "title": "Data science lifecycle",
    "section": "Analysis",
    "text": "Analysis\nThe plot shows us that there’s a roughly linear relationship on the log scale:\n\\[\\log(\\text{brain}) = \\alpha \\log(\\text{body}) + c\\]\n\nSo what does that mean in terms of brain and body weights? A little algebra and we have a “power law”:\n\\[(\\text{brain}) \\propto (\\text{body})^\\alpha\\]\n\n\nCheck your understanding: what’s the proportionality constant?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#interpretation",
    "href": "slides/week1-lifecycle.html#interpretation",
    "title": "Data science lifecycle",
    "section": "Interpretation",
    "text": "Interpretation\nSo it appears that the brain-body scaling is well-described by a power law:\n\namong selected specimens of these 62 species of mammal, species average brain weight is approximately proportional to a power of species average body weight\n\n\nNotice that I did not say:\n\nanimals’ brains are proportional to a power of their bodies\namong these 62 mammals, average brain weight is approximately proportional to a power of average body weight"
  },
  {
    "objectID": "slides/week1-lifecycle.html#question-refinement",
    "href": "slides/week1-lifecycle.html#question-refinement",
    "title": "Data science lifecycle",
    "section": "Question refinement",
    "text": "Question refinement\nWe can now ask further, more specific questions:\n\nDo other types of animals exhibit the same power law relationship?\n\n\nTo investigate, we need richer data."
  },
  {
    "objectID": "slides/week1-lifecycle.html#more-data-acquisition",
    "href": "slides/week1-lifecycle.html#more-data-acquisition",
    "title": "Data science lifecycle",
    "section": "(More) data acquisition",
    "text": "(More) data acquisition\nA number of authors have compiled and published ‘meta-analysis’ datasets by combining the results of multiple studies.\nBelow we’ll import a few of these for three different animal classes.\n\n# import metaanalysis datasets\nreptiles = pd.read_csv('data/reptile_meta.csv')\nbirds = pd.read_csv('data/bird_meta.csv', encoding = 'latin1')\nmammals = pd.read_csv('data/mammal_meta.csv', encoding = 'latin1')"
  },
  {
    "objectID": "slides/week1-lifecycle.html#data-assessment-2",
    "href": "slides/week1-lifecycle.html#data-assessment-2",
    "title": "Data science lifecycle",
    "section": "Data assessment",
    "text": "Data assessment\nWhere does this data come from? It’s kind of a convenience sample of scientific data:\n\nMultiple studies \\(\\rightarrow\\) possibly different sampling and measurement protocols\nCriteria for inclusion unknown \\(\\rightarrow\\) probably neither comprehensive nor representative of all such measurements taken\n\n\nSo these data, while richer, are still relatively narrow in terms of generalizability."
  },
  {
    "objectID": "slides/week1-lifecycle.html#a-comment-on-scope-of-inference",
    "href": "slides/week1-lifecycle.html#a-comment-on-scope-of-inference",
    "title": "Data science lifecycle",
    "section": "A comment on scope of inference",
    "text": "A comment on scope of inference\nThese data don’t support general inferences (e.g., to all animals, all mammals, etc.) because they weren’t collected for the purpose to which we’re putting them.\n\nUsually, if data are not collected for the explicit purpose of the question you’re trying to answer, they won’t constitute a representative sample."
  },
  {
    "objectID": "slides/week1-lifecycle.html#tidying",
    "href": "slides/week1-lifecycle.html#tidying",
    "title": "Data science lifecycle",
    "section": "Tidying",
    "text": "Tidying\nBack to the task at hand, in order to comine the datasets one must:\n\nSelect columns of interest;\nPut in consistent order;\nGive consistent names;\nConcatenate row-wise.\n\n\nWe’ll skip the details for now."
  },
  {
    "objectID": "slides/week1-lifecycle.html#inspection",
    "href": "slides/week1-lifecycle.html#inspection",
    "title": "Data science lifecycle",
    "section": "Inspection",
    "text": "Inspection\nThis dataset has quite a lot of missing brain weight measurements: many of the studies combined to form these datasets did not include that particular measurement.\n\n# missing values?\ndata.isna().mean(axis = 0)\n\nOrder      0.00000\nFamily     0.00000\nGenus      0.00000\nSpecies    0.00000\nSex        0.00000\nbody       0.00000\nbrain      0.57404\nclass      0.00000\ndtype: float64"
  },
  {
    "objectID": "slides/week1-lifecycle.html#exploration-2",
    "href": "slides/week1-lifecycle.html#exploration-2",
    "title": "Data science lifecycle",
    "section": "Exploration",
    "text": "Exploration\nFocusing on the nonmissing values, we see the same power law relationship but with different proportionality constants and exponents for the three classes of animals."
  },
  {
    "objectID": "slides/week1-lifecycle.html#analysis-1",
    "href": "slides/week1-lifecycle.html#analysis-1",
    "title": "Data science lifecycle",
    "section": "Analysis",
    "text": "Analysis\nSo we might hypothesize that:\n\\[\n(\\text{brain}) = \\beta_1(\\text{body})^{\\alpha_1} \\qquad \\text{(mammal)} \\\\\n(\\text{brain}) = \\beta_2(\\text{body})^{\\alpha_2} \\qquad \\text{(reptile)} \\\\\n(\\text{brain}) = \\beta_3(\\text{body})^{\\alpha_3} \\qquad \\text{(bird)} \\\\\n\\beta_i \\neq \\beta_j, \\alpha_i \\neq \\alpha_j \\quad \\text{for } i \\neq j\n\\]"
  },
  {
    "objectID": "slides/week1-lifecycle.html#interpretation-1",
    "href": "slides/week1-lifecycle.html#interpretation-1",
    "title": "Data science lifecycle",
    "section": "Interpretation",
    "text": "Interpretation\nIt seems that the average brain and body weights of the birds, mammals, and reptiles measured in these studies exhibit distinct power law relationships.\n\nWhat would you investigate next?\n\nCorrelates of body weight?\nAdjust for lifespan, habitat, predation, etc.?\nEstimate the \\(\\alpha_i\\)’s and \\(\\beta_i\\)’s?\nPredict brain weights for unobserved species?\nSomething else?"
  },
  {
    "objectID": "slides/week1-lifecycle.html#a-comment",
    "href": "slides/week1-lifecycle.html#a-comment",
    "title": "Data science lifecycle",
    "section": "A comment",
    "text": "A comment\nNotice that I did not mention the word ‘model’ anywhere!\n\nThis was intentional – it is a common misconception that analyzing data always involves fitting models.\n\nModels are not not always necessary or appropriate\nYou can learn a lot from exploratory techniques\nModels approximate specific kinds of relationships in data\nExploratory analysis can reveal unexpected structure"
  },
  {
    "objectID": "slides/week1-lifecycle.html#but-if-we-did-want-to-fit-a-model",
    "href": "slides/week1-lifecycle.html#but-if-we-did-want-to-fit-a-model",
    "title": "Data science lifecycle",
    "section": "But if we did want to fit a model…",
    "text": "But if we did want to fit a model…\n\\((\\text{brain}) = \\beta_j(\\text{body})^{\\alpha_j} \\quad \\text{animal class } j = 1, 2, 3\\)\n\nFigureEstimates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                      coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Bird                -1.9574     0.040   -49.118  0.000    -2.036    -1.879\n\n\n  Mammal              -2.9391     0.029  -100.061  0.000    -2.997    -2.882\n\n\n  Reptile             -4.0335     0.083   -48.577  0.000    -4.196    -3.871\n\n\n  log.body.bird        0.5653     0.008    66.566  0.000     0.549     0.582\n\n\n  log.body.mammal      0.7651     0.004   191.544  0.000     0.757     0.773\n\n\n  log.body.reptile     0.5293     0.017    31.375  0.000     0.496     0.562"
  },
  {
    "objectID": "slides/week1-lifecycle.html#model-limitations",
    "href": "slides/week1-lifecycle.html#model-limitations",
    "title": "Data science lifecycle",
    "section": "Model limitations",
    "text": "Model limitations\nBack to the issue of representativeness:\n\nshouldn’t use this model for inferences\nmight not be reliable for prediction either\nbut does capture/convey some suggestive comparisons\n\n\nSo, just be careful with interpretation of results:\n\n“For this particular collection of specimens, we estimated…”"
  },
  {
    "objectID": "slides/week1-lifecycle.html#zooming-out",
    "href": "slides/week1-lifecycle.html#zooming-out",
    "title": "Data science lifecycle",
    "section": "Zooming out",
    "text": "Zooming out\nThis example illustrates the aspects of the lifecylce we’ll cover in this class:\n\ndata retrieval and import\ntidying and transformation\nvisualization\nexploratory analysis\nmodeling\n\n\nWe’ll address these topics in sequence."
  },
  {
    "objectID": "slides/week1-lifecycle.html#next-week",
    "href": "slides/week1-lifecycle.html#next-week",
    "title": "Data science lifecycle",
    "section": "Next week",
    "text": "Next week\n\nTabular data structure\nData semantics\nTidy data\nTransformations of tabular data\nAggregation and grouping"
  },
  {
    "objectID": "slides/week2-tidy.html#announcements",
    "href": "slides/week2-tidy.html#announcements",
    "title": "Tidy data",
    "section": "Announcements",
    "text": "Announcements\n\nPDF export fixed on JupyterHub\nLab 0 due today 11:59PM; late submissions allowed until Wednesday 11:59PM\nComplete Q1-Q4 (fruit_info section) of Lab 1 before section"
  },
  {
    "objectID": "slides/week2-tidy.html#this-week",
    "href": "slides/week2-tidy.html#this-week",
    "title": "Tidy data",
    "section": "This week",
    "text": "This week\n\nTabular data\n\nMany ways to structure a dataset\nFew organizational constraints ‘in the wild’\n\nPrinciples of tidy data: matching semantics with structure\n\nData semantics: observations and variables\nTabular structure: rows and columns\nThe tidy standard\nCommon messes\nTidying operations\n\nTransforming data frames\n\nSubsetting (slicing and filtering)\nDerived variables\nAggregation and summary statistics"
  },
  {
    "objectID": "slides/week2-tidy.html#tabular-data",
    "href": "slides/week2-tidy.html#tabular-data",
    "title": "Tidy data",
    "section": "Tabular data",
    "text": "Tabular data\n\nMany possible layouts for tabular data\n‘Real’ datasets have few organizational constraints\n\n\nMost data are stored in tables, but there are always multiple possible tabular layouts for the same underlying data.\n\n\nLet’s look at some examples."
  },
  {
    "objectID": "slides/week2-tidy.html#mammal-data-long-layouts",
    "href": "slides/week2-tidy.html#mammal-data-long-layouts",
    "title": "Tidy data",
    "section": "Mammal data: long layouts",
    "text": "Mammal data: long layouts\nBelow is the Allison 1976 mammal brain-body weight dataset from last time shown in two ‘long’ layouts:\n\n\n\n\n\n\n\n  \n    \n      \n      body_wt\n      brain_wt\n    \n    \n      species\n      \n      \n    \n  \n  \n    \n      Africanelephant\n      6654.0\n      5712.0\n    \n    \n      Africangiantpouchedrat\n      1.0\n      6.6\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      measurement\n      weight\n    \n    \n      species\n      \n      \n    \n  \n  \n    \n      Africanelephant\n      brain_wt\n      5712.0\n    \n    \n      Africanelephant\n      body_wt\n      6654.0\n    \n    \n      Africangiantpouchedrat\n      brain_wt\n      6.6\n    \n    \n      Africangiantpouchedrat\n      body_wt\n      1.0"
  },
  {
    "objectID": "slides/week2-tidy.html#mammal-data-wide-layout",
    "href": "slides/week2-tidy.html#mammal-data-wide-layout",
    "title": "Tidy data",
    "section": "Mammal data: wide layout",
    "text": "Mammal data: wide layout\nHere’s a third possible layout for the mammal brain-body weight data:\n\n\n\n\n\n\n\n  \n    \n      species\n      Africanelephant\n      Africangiantpouchedrat\n      ArcticFox\n      Arcticgroundsquirrel\n    \n    \n      measurement\n      \n      \n      \n      \n    \n  \n  \n    \n      body_wt\n      6654.0\n      1.0\n      3.385\n      0.92\n    \n    \n      brain_wt\n      5712.0\n      6.6\n      44.500\n      5.70"
  },
  {
    "objectID": "slides/week2-tidy.html#gdp-growth-data-wide-layout",
    "href": "slides/week2-tidy.html#gdp-growth-data-wide-layout",
    "title": "Tidy data",
    "section": "GDP growth data: wide layout",
    "text": "GDP growth data: wide layout\nHere’s another example: World Bank data on annual GDP growth for 264 countries from 1961 – 2019.\n\n\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Country Code\n      2009\n      2010\n      2011\n    \n  \n  \n    \n      0\n      Aruba\n      ABW\n      -10.519749\n      -3.685029\n      3.446055\n    \n    \n      1\n      Afghanistan\n      AFG\n      21.390528\n      14.362441\n      0.426355\n    \n    \n      2\n      Angola\n      AGO\n      0.858713\n      4.403933\n      3.471976\n    \n    \n      3\n      Albania\n      ALB\n      3.350067\n      3.706892\n      2.545322\n    \n    \n      4\n      Andorra\n      AND\n      -5.302847\n      -1.974958\n      -0.008070"
  },
  {
    "objectID": "slides/week2-tidy.html#gdp-growth-data-long-layout",
    "href": "slides/week2-tidy.html#gdp-growth-data-long-layout",
    "title": "Tidy data",
    "section": "GDP growth data: long layout",
    "text": "GDP growth data: long layout\nHere’s an alternative layout for the annual GDP growth data:\n\n\n\n\n\n\n  \n    \n      \n      year\n      growth_pct\n    \n    \n      Country Name\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      2009\n      21.390528\n    \n    \n      Aruba\n      2009\n      -10.519749\n    \n    \n      Afghanistan\n      2010\n      14.362441\n    \n    \n      Aruba\n      2010\n      -3.685029\n    \n    \n      Afghanistan\n      2011\n      0.426355\n    \n    \n      Aruba\n      2011\n      3.446055"
  },
  {
    "objectID": "slides/week2-tidy.html#sb-weather-data-long-layouts",
    "href": "slides/week2-tidy.html#sb-weather-data-long-layouts",
    "title": "Tidy data",
    "section": "SB weather data: long layouts",
    "text": "SB weather data: long layouts\nA third example: daily minimum and maximum temperatures recorded at Santa Barbara Municipal Airport from January 2021 through March 2021.\n\n\n\n\n\n\n\n  \n    \n      \n      STATION\n      TMAX\n      TMIN\n      MONTH\n      DAY\n      YEAR\n    \n  \n  \n    \n      0\n      USW00023190\n      65\n      37\n      1\n      1\n      2021\n    \n    \n      1\n      USW00023190\n      62\n      38\n      1\n      2\n      2021\n    \n    \n      2\n      USW00023190\n      60\n      42\n      1\n      3\n      2021"
  },
  {
    "objectID": "slides/week2-tidy.html#sb-weather-data-wide-layout",
    "href": "slides/week2-tidy.html#sb-weather-data-wide-layout",
    "title": "Tidy data",
    "section": "SB weather data: wide layout",
    "text": "SB weather data: wide layout\nHere’s a wide layout for the SB weather data:\n\n\n\n\n\n\n\n  \n    \n      \n      DAY\n      1\n      2\n      3\n      4\n    \n    \n      MONTH\n      type\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      TMAX\n      65.0\n      62.0\n      60.0\n      72.0\n    \n    \n      TMIN\n      37.0\n      38.0\n      42.0\n      43.0\n    \n    \n      2\n      TMAX\n      66.0\n      67.0\n      69.0\n      63.0\n    \n    \n      TMIN\n      45.0\n      40.0\n      44.0\n      37.0\n    \n    \n      3\n      TMAX\n      68.0\n      66.0\n      59.0\n      62.0"
  },
  {
    "objectID": "slides/week2-tidy.html#un-development-data-multiple-tables",
    "href": "slides/week2-tidy.html#un-development-data-multiple-tables",
    "title": "Tidy data",
    "section": "UN development data: multiple tables",
    "text": "UN development data: multiple tables\nA final example: United Nations country development data organized into different tables according to variable type.\nHere is a table of population measurements:\n\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n    \n    \n      country\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n    \n  \n\n\n\n\nAnd here is a table of a few gender-related variables:\n\n\n\n\n\n\n\n\n  \n    \n      \n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n    \n  \n  \n    \n      Norway\n      0.045\n      40.8\n      60.4\n      67.2\n    \n    \n      Ireland\n      0.093\n      24.3\n      56.0\n      68.4"
  },
  {
    "objectID": "slides/week2-tidy.html#un-development-data-one-table",
    "href": "slides/week2-tidy.html#un-development-data-one-table",
    "title": "Tidy data",
    "section": "UN development data: one table",
    "text": "UN development data: one table\nHere are both tables merged by country:\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n      0.655\n      27.2\n      21.6\n      74.7\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n      0.181\n      29.5\n      46.7\n      64.6\n    \n    \n      Algeria\n      43.1\n      73.2\n      5.0\n      27.1\n      2.8\n      0.429\n      21.5\n      14.6\n      67.4"
  },
  {
    "objectID": "slides/week2-tidy.html#un-development-data-one-longer-table",
    "href": "slides/week2-tidy.html#un-development-data-one-longer-table",
    "title": "Tidy data",
    "section": "UN development data: one (longer) table",
    "text": "UN development data: one (longer) table\nAnd here is another arrangement of the merged table:\n\n\n\n\n\n\n  \n    \n      \n      gender_variable\n      gender_value\n      population_variable\n      population_value\n    \n    \n      country\n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      gender_inequality\n      0.655\n      total_pop\n      38.0\n    \n    \n      Albania\n      gender_inequality\n      0.181\n      total_pop\n      2.9\n    \n    \n      Algeria\n      gender_inequality\n      0.429\n      total_pop\n      43.1\n    \n    \n      Andorra\n      gender_inequality\n      NaN\n      total_pop\n      0.1\n    \n    \n      Angola\n      gender_inequality\n      0.536\n      total_pop\n      31.8"
  },
  {
    "objectID": "slides/week2-tidy.html#what-are-the-differences",
    "href": "slides/week2-tidy.html#what-are-the-differences",
    "title": "Tidy data",
    "section": "What are the differences?",
    "text": "What are the differences?\nIn short, the alternate layouts differ in three respects:\n\nRows\nColumns\nNumber of tables"
  },
  {
    "objectID": "slides/week2-tidy.html#how-to-choose",
    "href": "slides/week2-tidy.html#how-to-choose",
    "title": "Tidy data",
    "section": "How to choose?",
    "text": "How to choose?\nReturn to one of the examples and review the different layouts with your neighbor.\n\nList a few advantages and disadvantages for each layout.\nWhich do you prefer and why?"
  },
  {
    "objectID": "slides/week2-tidy.html#few-organizational-constraints",
    "href": "slides/week2-tidy.html#few-organizational-constraints",
    "title": "Tidy data",
    "section": "Few organizational constraints",
    "text": "Few organizational constraints\nIt’s surprisingly difficult to articulate reasons why one layout might be preferable to another.\n\nUsually the choice of layout isn’t principled\nIdiosyncratic: two people are likely to make different choices\n\n\nAs a result:\n\nFew widely used conventions\nLots of variability ‘in the wild’\nDatasets are often organized in bizarre ways"
  },
  {
    "objectID": "slides/week2-tidy.html#form-and-representation",
    "href": "slides/week2-tidy.html#form-and-representation",
    "title": "Tidy data",
    "section": "Form and representation",
    "text": "Form and representation\nBecause of the wide range of possible layouts for a dataset, and the variety of choices that are made about how to store data, data scientists are constantly faced with determining how best to reorganize datasets in a way that facilitates exploration and analysis.\n\nBroadly, this involves two interdependent choices:\n\nChoice of representation: how to encode information.\n\nExample: parse dates as ‘MM/DD/YYYY’ (one variable) or ‘MM’, ‘DD’, ‘YYYY’ (three variables)?\nExample: use values 1, 2, 3 or ‘low’, ‘med’, ‘high’?\nExample: name variables ‘question1’, ‘question2’, …, or ‘age’, ‘income’, …?\n\nChoice of form: how to display information\n\nExample: wide table or long table?\nExample: one table or many?"
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-data",
    "href": "slides/week2-tidy.html#tidy-data",
    "title": "Tidy data",
    "section": "Tidy data",
    "text": "Tidy data\nThe tidy data standard is a principled way of organizing tabular data. It has two main advantages:\n\nFacilitates workflow by establishing a consistent dataset structure.\nPrinciples are designed to make transformation, exploration, visualization, and modeling easy."
  },
  {
    "objectID": "slides/week2-tidy.html#semantics-and-structure",
    "href": "slides/week2-tidy.html#semantics-and-structure",
    "title": "Tidy data",
    "section": "Semantics and structure",
    "text": "Semantics and structure\n\n“Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored.” Wickham and Grolemund, R for Data Science, 2017.\n\n\nA dataset is a collection of values.\n\nthe semantics of a dataset are the meanings of the values\nthe structure of a dataset is the arrangement of the values"
  },
  {
    "objectID": "slides/week2-tidy.html#data-semantics",
    "href": "slides/week2-tidy.html#data-semantics",
    "title": "Tidy data",
    "section": "Data semantics",
    "text": "Data semantics\nTo introduce some general vocabulary, each value in a dataset is\n\nan observation\nof a variable\ntaken on an observational unit."
  },
  {
    "objectID": "slides/week2-tidy.html#units-variables-and-observations",
    "href": "slides/week2-tidy.html#units-variables-and-observations",
    "title": "Tidy data",
    "section": "Units, variables, and observations",
    "text": "Units, variables, and observations\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n    \n    \n      country\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n    \n    \n      Albania\n      2.9\n      61.2\n    \n  \n\n\n\n\n\nAn observational unit is the entity measured.\n\nAbove, country\n\nA variable is an attribute measured on each unit.\n\nAbove, total population and urban percentage\n\nAn observation is a collection of measurements taken on one unit.\n\nAbove, 38.0 and 25.8"
  },
  {
    "objectID": "slides/week2-tidy.html#identifying-units-variables-and-observations",
    "href": "slides/week2-tidy.html#identifying-units-variables-and-observations",
    "title": "Tidy data",
    "section": "Identifying units, variables, and observations",
    "text": "Identifying units, variables, and observations\nLet’s do an example. Here’s one record from the GDP growth data:\n\n\n\n\n\n\n\n  \n    \n      \n      year\n      growth_pct\n    \n    \n      Country Name\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      2010\n      14.362441\n    \n  \n\n\n\n\n\n\nAbove, the values -13.605441 and 1961 are observations of the variables GDP growth and year recorded for the observational unit Algeria."
  },
  {
    "objectID": "slides/week2-tidy.html#your-turn",
    "href": "slides/week2-tidy.html#your-turn",
    "title": "Tidy data",
    "section": "Your turn",
    "text": "Your turn\nWhat are the units, variables and observations?\n\n\n\n\n\n\n\n  \n    \n      \n      DAY\n      1\n      2\n      3\n      4\n    \n    \n      MONTH\n      type\n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      TMAX\n      65.0\n      62.0\n      60.0\n      72.0\n    \n    \n      TMIN\n      37.0\n      38.0\n      42.0\n      43.0\n    \n    \n      2\n      TMAX\n      66.0\n      67.0\n      69.0\n      63.0\n    \n    \n      TMIN\n      45.0\n      40.0\n      44.0\n      37.0\n    \n  \n\n\n\n\n\n\nThink about it, then confer with your neighbor."
  },
  {
    "objectID": "slides/week2-tidy.html#data-structure",
    "href": "slides/week2-tidy.html#data-structure",
    "title": "Tidy data",
    "section": "Data structure",
    "text": "Data structure\nData structure refers to the form in which it is stored.\n\nTabular data is arranged in rows and columns.\n\n\nAs we saw, there are multiple structures – arrangements of rows and columns – available to represent any dataset."
  },
  {
    "objectID": "slides/week2-tidy.html#the-tidy-standard",
    "href": "slides/week2-tidy.html#the-tidy-standard",
    "title": "Tidy data",
    "section": "The tidy standard",
    "text": "The tidy standard\nThe tidy standard consists in matching semantics and structure. A dataset is tidy if:\n\nEach variable is a column.\nEach observation is a row.\nEach table contains measurements on only one type of observational unit.\n\n\nTidy data."
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy",
    "href": "slides/week2-tidy.html#tidy-or-messy",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\nLet’s revisit some of our examples of multiple layouts.\n\n\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Country Code\n      2009\n      2010\n      2011\n    \n  \n  \n    \n      0\n      Aruba\n      ABW\n      -10.519749\n      -3.685029\n      3.446055\n    \n    \n      1\n      Afghanistan\n      AFG\n      21.390528\n      14.362441\n      0.426355\n    \n    \n      2\n      Angola\n      AGO\n      0.858713\n      4.403933\n      3.471976\n    \n  \n\n\n\n\n\n\nWe can compare the semantics and structure for alignment:\n\n\n\nSemantics\n\nStructure\n\n\n\n\n\nObservations\nAnnual records\nRows\nCountries\n\n\nVariables\nGDP growth and year\nColumns\nValue of year\n\n\nObservational units\nCountries\nTables\nJust one\n\n\n\n\n\nRules 1 and 2 are violated, since column names are values (of year), not variables. Not tidy."
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy-1",
    "href": "slides/week2-tidy.html#tidy-or-messy-1",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\n\n\n\n\n\n\n\n  \n    \n      \n      year\n      growth_pct\n    \n    \n      Country Name\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      2009\n      21.390528\n    \n    \n      Aruba\n      2009\n      -10.519749\n    \n    \n      Afghanistan\n      2010\n      14.362441\n    \n    \n      Aruba\n      2010\n      -3.685029\n    \n  \n\n\n\n\n\n\nComparison of semantics and structure:\n\n\n\nSemantics\n\nStructure\n\n\n\n\n\nObservations\nAnnual records\nRows\nAnnual records\n\n\nVariables\nGDP growth and year\nColumns\nGDP growth and year\n\n\nObservational units\nCountries\nTables\nJust one\n\n\n\n\n\nAll three rules are met: rows are observations, columns are variables, and there’s one unit type and one table. Tidy."
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy-2",
    "href": "slides/week2-tidy.html#tidy-or-messy-2",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\n\n\n\n\n\n\n\n  \n    \n      \n      STATION\n      TMAX\n      TMIN\n      MONTH\n      DAY\n      YEAR\n    \n  \n  \n    \n      0\n      USW00023190\n      65\n      37\n      1\n      1\n      2021\n    \n    \n      1\n      USW00023190\n      62\n      38\n      1\n      2\n      2021\n    \n    \n      2\n      USW00023190\n      60\n      42\n      1\n      3\n      2021\n    \n  \n\n\n\n\nTry this one on your own. Then compare with your neighbor.\n\nIdentify the observations and variables\nWhat are the observational units?"
  },
  {
    "objectID": "slides/week2-tidy.html#tidy-or-messy-3",
    "href": "slides/week2-tidy.html#tidy-or-messy-3",
    "title": "Tidy data",
    "section": "Tidy or messy?",
    "text": "Tidy or messy?\nIn undev1 and undev2:\n\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n    \n    \n      country\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n    \n  \n  \n    \n      Norway\n      0.045\n      40.8\n      60.4\n      67.2\n    \n    \n      Ireland\n      0.093\n      24.3\n      56.0\n      68.4\n    \n  \n\n\n\n\n\n\nHere there are multiple tables. To discuss:\n\nAre the observational units the same or different?\nBased on your answer above, is the data tidy or not?"
  },
  {
    "objectID": "slides/week2-tidy.html#common-messes",
    "href": "slides/week2-tidy.html#common-messes",
    "title": "Tidy data",
    "section": "Common messes",
    "text": "Common messes\n\n“Well, here’s another nice mess you’ve gotten me into” – Oliver Hardy\n\nThese examples illustrate some common messes:\n\nColumns are values, not variables\n\nGDP data: columns are 1961, 1962, …\n\nMultiple variables are stored in one column\n\nMammal data: weight column contains both body and brain weights\n\nVariables or values are stored in rows and columns\n\nWeather data: date values are stored in rows and columns, each column contains both min and max temperatures\n\nMeasurements on one type of observational unit are divided into multiple tables.\n\nUN development data: one table for population statistics and a separate table for gender statistics."
  },
  {
    "objectID": "slides/week2-tidy.html#tidying-operations",
    "href": "slides/week2-tidy.html#tidying-operations",
    "title": "Tidy data",
    "section": "Tidying operations",
    "text": "Tidying operations\nThese common messes can be cleaned up by some simple operations:\n\nmelt\n\nreshape a dataframe from wide to long format\n\npivot\n\nreshape a dataframe from long to wide format\n\nmerge\n\ncombine two dataframes row-wise by matching the values of certain columns"
  },
  {
    "objectID": "slides/week2-tidy.html#melt",
    "href": "slides/week2-tidy.html#melt",
    "title": "Tidy data",
    "section": "Melt",
    "text": "Melt\nMelting resolves the problem of having values stored as columns (common mess 1)."
  },
  {
    "objectID": "slides/week2-tidy.html#melt-1",
    "href": "slides/week2-tidy.html#melt-1",
    "title": "Tidy data",
    "section": "Melt",
    "text": "Melt\n\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Country Code\n      1961\n      1962\n      1963\n      1964\n      1965\n      1966\n      1967\n      1968\n      ...\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n  \n  \n    \n      0\n      Aruba\n      ABW\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      -3.685029\n      3.446055\n      -1.369863\n      4.198232\n      0.300000\n      5.700001\n      2.100000\n      1.999999\n      NaN\n      NaN\n    \n    \n      1\n      Afghanistan\n      AFG\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      14.362441\n      0.426355\n      12.752287\n      5.600745\n      2.724543\n      1.451315\n      2.260314\n      2.647003\n      1.189228\n      3.911603\n    \n  \n\n2 rows × 61 columns\n\n\n\n\n\n# in pandas\ngdp1.melt(\n    id_vars = ['Country Name', 'Country Code'], # which variables do you want to retain for each row? .\n    var_name = 'Year', # what do you want to name the variable that will contain the column names?\n    value_name = 'GDP Growth', # what do you want to name the variable that will contain the values?\n).head(2)\n\n\n\n\n\n  \n    \n      \n      Country Name\n      Country Code\n      Year\n      GDP Growth\n    \n  \n  \n    \n      0\n      Aruba\n      ABW\n      1961\n      NaN\n    \n    \n      1\n      Afghanistan\n      AFG\n      1961\n      NaN"
  },
  {
    "objectID": "slides/week2-tidy.html#pivot",
    "href": "slides/week2-tidy.html#pivot",
    "title": "Tidy data",
    "section": "Pivot",
    "text": "Pivot\nPivoting resolves the issue of having multiple variables stored in one column (common mess 2). It’s the inverse operation of melting."
  },
  {
    "objectID": "slides/week2-tidy.html#pivot-1",
    "href": "slides/week2-tidy.html#pivot-1",
    "title": "Tidy data",
    "section": "Pivot",
    "text": "Pivot\n\n\n\n\n\n\n  \n    \n      \n      measurement\n      weight\n    \n    \n      species\n      \n      \n    \n  \n  \n    \n      Africanelephant\n      brain_wt\n      5712.0\n    \n    \n      Africanelephant\n      body_wt\n      6654.0\n    \n    \n      Africangiantpouchedrat\n      brain_wt\n      6.6\n    \n    \n      Africangiantpouchedrat\n      body_wt\n      1.0\n    \n  \n\n\n\n\n\n# in pandas\nmammal2.pivot(\n    columns = 'measurement', # which variable(s) do you want to send to new column names?\n    values = 'weight' # which variable(s) do you want to use to populate the new columns?\n).head(2)\n\n\n\n\n\n  \n    \n      measurement\n      body_wt\n      brain_wt\n    \n    \n      species\n      \n      \n    \n  \n  \n    \n      Africanelephant\n      6654.0\n      5712.0\n    \n    \n      Africangiantpouchedrat\n      1.0\n      6.6"
  },
  {
    "objectID": "slides/week2-tidy.html#pivot-and-melt",
    "href": "slides/week2-tidy.html#pivot-and-melt",
    "title": "Tidy data",
    "section": "Pivot and melt",
    "text": "Pivot and melt\nCommon mess 3 is a combination of messes 1 and 2: values or variables are stored in both rows and columns. Pivoting and melting in sequence can usually fix this.\n\n\n\n\n\n\n\n  \n    \n      \n      DAY\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      22\n      23\n      24\n      25\n      26\n      27\n      28\n      29\n      30\n      31\n    \n    \n      MONTH\n      type\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      TMAX\n      65.0\n      62.0\n      60.0\n      72.0\n      61.0\n      71.0\n      73.0\n      79.0\n      71.0\n      67.0\n      ...\n      61.0\n      59.0\n      65.0\n      55.0\n      57.0\n      54.0\n      55.0\n      55.0\n      58.0\n      63.0\n    \n    \n      TMIN\n      37.0\n      38.0\n      42.0\n      43.0\n      40.0\n      39.0\n      38.0\n      36.0\n      39.0\n      37.0\n      ...\n      41.0\n      40.0\n      38.0\n      44.0\n      40.0\n      48.0\n      49.0\n      42.0\n      37.0\n      37.0\n    \n    \n      2\n      TMAX\n      66.0\n      67.0\n      69.0\n      63.0\n      66.0\n      68.0\n      60.0\n      57.0\n      59.0\n      61.0\n      ...\n      75.0\n      75.0\n      70.0\n      66.0\n      69.0\n      76.0\n      68.0\n      NaN\n      NaN\n      NaN\n    \n    \n      TMIN\n      45.0\n      40.0\n      44.0\n      37.0\n      38.0\n      38.0\n      38.0\n      49.0\n      49.0\n      41.0\n      ...\n      37.0\n      39.0\n      41.0\n      39.0\n      36.0\n      43.0\n      38.0\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      TMAX\n      68.0\n      66.0\n      59.0\n      62.0\n      67.0\n      69.0\n      60.0\n      69.0\n      65.0\n      58.0\n      ...\n      71.0\n      72.0\n      67.0\n      65.0\n      63.0\n      72.0\n      73.0\n      77.0\n      NaN\n      NaN\n    \n    \n      TMIN\n      37.0\n      36.0\n      36.0\n      37.0\n      39.0\n      43.0\n      47.0\n      47.0\n      47.0\n      43.0\n      ...\n      50.0\n      49.0\n      41.0\n      44.0\n      40.0\n      41.0\n      41.0\n      42.0\n      NaN\n      NaN\n    \n  \n\n6 rows × 31 columns"
  },
  {
    "objectID": "slides/week2-tidy.html#pivot-and-melt-1",
    "href": "slides/week2-tidy.html#pivot-and-melt-1",
    "title": "Tidy data",
    "section": "Pivot and melt",
    "text": "Pivot and melt\n\nFirst, meltThen, pivot\n\n\n\nweather3.melt(\n    ignore_index = False,\n    var_name = 'day',\n    value_name = 'temp'\n).head()\n\n\n\n\n\n  \n    \n      \n      \n      day\n      temp\n    \n    \n      MONTH\n      type\n      \n      \n    \n  \n  \n    \n      1\n      TMAX\n      1\n      65.0\n    \n    \n      TMIN\n      1\n      37.0\n    \n    \n      2\n      TMAX\n      1\n      66.0\n    \n    \n      TMIN\n      1\n      45.0\n    \n    \n      3\n      TMAX\n      1\n      68.0\n    \n  \n\n\n\n\n\n\n\nweather3.melt(\n    ignore_index = False,\n    var_name = 'day',\n    value_name = 'temp'\n).reset_index().pivot(\n    index = ['MONTH', 'day'],\n    columns = 'type',\n    values = 'temp'\n).reset_index().rename_axis(columns = {'type': ''}).head()\n\n\n\n\n\n  \n    \n      \n      MONTH\n      day\n      TMAX\n      TMIN\n    \n  \n  \n    \n      0\n      1\n      1\n      65.0\n      37.0\n    \n    \n      1\n      1\n      2\n      62.0\n      38.0\n    \n    \n      2\n      1\n      3\n      60.0\n      42.0\n    \n    \n      3\n      1\n      4\n      72.0\n      43.0\n    \n    \n      4\n      1\n      5\n      61.0\n      40.0"
  },
  {
    "objectID": "slides/week2-tidy.html#merge",
    "href": "slides/week2-tidy.html#merge",
    "title": "Tidy data",
    "section": "Merge",
    "text": "Merge\nMerging resolves the issue of storing observations or variables on one unit type in multiple tables (mess 4). The basic idea is to combine by matching rows."
  },
  {
    "objectID": "slides/week2-tidy.html#merge-1",
    "href": "slides/week2-tidy.html#merge-1",
    "title": "Tidy data",
    "section": "Merge",
    "text": "Merge\nThe code below combines columns in each table by matching rows based on country.\n\npd.merge(undev1, undev2, on = 'country').head(4)\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n      0.655\n      27.2\n      21.6\n      74.7\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n      0.181\n      29.5\n      46.7\n      64.6\n    \n    \n      Algeria\n      43.1\n      73.2\n      5.0\n      27.1\n      2.8\n      0.429\n      21.5\n      14.6\n      67.4\n    \n    \n      Andorra\n      0.1\n      88.0\n      NaN\n      NaN\n      NaN\n      NaN\n      46.4\n      NaN\n      NaN"
  },
  {
    "objectID": "slides/week2-tidy.html#merge-2",
    "href": "slides/week2-tidy.html#merge-2",
    "title": "Tidy data",
    "section": "Merge",
    "text": "Merge\nThere are various rules for exactly how to merge, but the general syntactical procedure to merge dataframes df1 and df2 is this.\n\nSpecify an order: merge(df1, df2) or merge(df2, df1).\nSpecify keys: the shared columns to use for matching rows of df1 with rows of df2.\n\nfor example, merging on date will align rows in df2 with rows of df1 that have the same value for date\n\nSpecify a rule for which rows to return after merging\n\nkeep all rows with key entries in df1, drop non-matching rows in df2 (‘left’ join)\nkeep all rows with key entries in df2 drop non-matching rows in df1 (‘right’ join)\nkeep all rows with key entries in either df1 or df2, inducing missing values (‘outer’ join)\nkeep all rows with key entries in both df1 and df2 (‘inner’ join)"
  },
  {
    "objectID": "slides/week2-tidy.html#next-time",
    "href": "slides/week2-tidy.html#next-time",
    "title": "Tidy data",
    "section": "Next time",
    "text": "Next time\nTransformations of tabular data\n\nSlicing and filtering\nDefining new variables\nVectorized operatioons\nAggregation and grouping"
  },
  {
    "objectID": "slides/week2-transform.html#recap-tidy-data",
    "href": "slides/week2-transform.html#recap-tidy-data",
    "title": "Dataframe Transformations",
    "section": "Recap: tidy data",
    "text": "Recap: tidy data\nThe tidy standard consists in matching semantics and structure.\n\nA dataset is tidy if:\n\nEach variable is a column.\nEach observation is a row.\nEach table contains measurements on only one type of observational unit."
  },
  {
    "objectID": "slides/week2-transform.html#why-tidy",
    "href": "slides/week2-transform.html#why-tidy",
    "title": "Dataframe Transformations",
    "section": "Why tidy?",
    "text": "Why tidy?\n\nWhy use the tidy standard? Wouldn’t any system of organization do just as well?\n\n\nThe tidy standard has three main advantages:\n\nHaving a consistent system of organization makes it easier to focus on analysis and exploration. (True of any system)\nMany software tools are designed to work with tidy data inputs. (Tidy only)\nTransformation of tidy data is especially natural in most computing environments due to vectorized operations. (Tidy only)"
  },
  {
    "objectID": "slides/week2-transform.html#transformations",
    "href": "slides/week2-transform.html#transformations",
    "title": "Dataframe Transformations",
    "section": "Transformations",
    "text": "Transformations\nTransformations of data frames are operations that modify the shape or values of a data frame. These include:\n\nSlicing rows and columns by index\nFiltering rows by logical conditions\nDefining new variables from scratch or by operations on existing variables\nAggregations (min, mean, max, etc.)"
  },
  {
    "objectID": "slides/week2-transform.html#slicing",
    "href": "slides/week2-transform.html#slicing",
    "title": "Dataframe Transformations",
    "section": "Slicing",
    "text": "Slicing\nSlicing refers to retrieving a (usually contiguous) subset (a ‘slice’) of rows/columns from a data frame.\n\nUses:\n\ndata inspection/retrieval\nsubsetting for further analysis/manipulation\ndata display"
  },
  {
    "objectID": "slides/week2-transform.html#data-display",
    "href": "slides/week2-transform.html#data-display",
    "title": "Dataframe Transformations",
    "section": "Data display",
    "text": "Data display\nRecall the UN Development data:\n\n# preview UN data -- note indexed by country\nundev.head(3)\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n      5.6\n      20.9\n      1.0\n      0.655\n      27.2\n      21.6\n      74.7\n    \n    \n      Albania\n      2.9\n      61.2\n      0.2\n      2.0\n      0.4\n      0.181\n      29.5\n      46.7\n      64.6\n    \n    \n      Algeria\n      43.1\n      73.2\n      5.0\n      27.1\n      2.8\n      0.429\n      21.5\n      14.6\n      67.4\n    \n  \n\n\n\n\n\nAside: .head() is a slicing operation – it returns the ‘top’ slice of rows."
  },
  {
    "objectID": "slides/week2-transform.html#data-inspectionretrieval",
    "href": "slides/week2-transform.html#data-inspectionretrieval",
    "title": "Dataframe Transformations",
    "section": "Data inspection/retrieval",
    "text": "Data inspection/retrieval\nTo inspect the percentage of women in parliament in Mexico, slice accordingly:\n\n\nundev.loc[['Mexico'], ['parliament_pct_women']]\n\n\n\n\n\n  \n    \n      \n      parliament_pct_women\n    \n    \n      country\n      \n    \n  \n  \n    \n      Mexico\n      48.4"
  },
  {
    "objectID": "slides/week2-transform.html#review-.loc-and-.iloc",
    "href": "slides/week2-transform.html#review-.loc-and-.iloc",
    "title": "Dataframe Transformations",
    "section": "Review: .loc and .iloc",
    "text": "Review: .loc and .iloc\nThe primary slicing functions in pandas are\n\n.loc (location) to slice by index\n.iloc (integer location) to slice by position\n\n\n\n# .iloc equivalent of previous slice\nundev.iloc[[111], [6]]\n\n\n\n\n\n  \n    \n      \n      parliament_pct_women\n    \n    \n      country\n      \n    \n  \n  \n    \n      Mexico\n      48.4\n    \n  \n\n\n\n\n\n\nCheck your understanding: which row in the dataframe is the observation for Mexico?\n\n\nIf a single index rather than a list is provided – e.g., Mexico rather than [Mexico], – these functions will return the raw value as a float rather than a dataframe.\n\nundev.loc['Mexico', 'parliament_pct_women']\n\n48.4"
  },
  {
    "objectID": "slides/week2-transform.html#larger-slices",
    "href": "slides/week2-transform.html#larger-slices",
    "title": "Dataframe Transformations",
    "section": "Larger slices",
    "text": "Larger slices\nMore typically, a slice will be a contiguous chunk of rows and columns.\n\nSlicing operations can interpret start:end as shorthand for a range of indices.\n\nundev.loc['Mexico':'Mongolia', ['parliament_pct_women']]\n\n\n\n\n\n  \n    \n      \n      parliament_pct_women\n    \n    \n      country\n      \n    \n  \n  \n    \n      Mexico\n      48.4\n    \n    \n      Micronesia (Federated States of)\n      0.0\n    \n    \n      Moldova (Republic of)\n      25.7\n    \n    \n      Mongolia\n      17.3\n    \n  \n\n\n\n\n\n\nNote: start:end is inclusive of both endpoints with .loc, but not inclusive of the right endpoint with .iloc. Get in the habit of double-checking results."
  },
  {
    "objectID": "slides/week2-transform.html#defining-new-variables",
    "href": "slides/week2-transform.html#defining-new-variables",
    "title": "Dataframe Transformations",
    "section": "Defining new variables",
    "text": "Defining new variables\nVectorization of operations in pandas and numpy make tidy data especially nice to manipulate mathematically. For example:\n\n\nweather2['TRANGE'] = weather2.TMAX - weather2.TMIN\nweather2.loc[0:3, ['TMAX', 'TMIN', 'TRANGE']]\n\n\n\n\n\n  \n    \n      \n      TMAX\n      TMIN\n      TRANGE\n    \n  \n  \n    \n      0\n      65\n      37\n      28\n    \n    \n      1\n      62\n      38\n      24\n    \n    \n      2\n      60\n      42\n      18\n    \n    \n      3\n      72\n      43\n      29\n    \n  \n\n\n\n\n\n\nThis computes \\(t_{min, i} - t_{max, i}\\) for all observations \\(i = 1, \\dots, n\\).\n\n\nCheck your understanding: express this calculation as a linear algebra arithmetic operation."
  },
  {
    "objectID": "slides/week2-transform.html#your-turn",
    "href": "slides/week2-transform.html#your-turn",
    "title": "Dataframe Transformations",
    "section": "Your turn",
    "text": "Your turn\nLet’s take another example – consider this slice of the undev data:\n\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n    \n    \n      country\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      38.0\n      25.8\n    \n    \n      Albania\n      2.9\n      61.2\n    \n    \n      Algeria\n      43.1\n      73.2\n    \n  \n\n\n\n\n\nWith your neighbor, write a line of code that calculates the percentage of the population living in rural areas."
  },
  {
    "objectID": "slides/week2-transform.html#filtering",
    "href": "slides/week2-transform.html#filtering",
    "title": "Dataframe Transformations",
    "section": "Filtering",
    "text": "Filtering\nFiltering refers to removing a subset of rows based on one or more conditions. (Think of “filtering out” certain rows.)\n\nFor example, suppose we wanted to retrieve only the countries with populations exceeding 1Bn people:\n\nundev[undev.total_pop > 1000]\n\n\n\n\n\n  \n    \n      \n      total_pop\n      urban_pct_pop\n      pop_under5\n      pop_15to64\n      pop_over65\n      gender_inequality\n      parliament_pct_women\n      labor_participation_women\n      labor_participation_men\n    \n    \n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      China\n      1433.8\n      60.3\n      85.0\n      1014.0\n      164.5\n      0.168\n      24.9\n      60.5\n      75.3\n    \n    \n      India\n      1366.4\n      34.5\n      116.8\n      915.6\n      87.1\n      0.488\n      13.5\n      20.5\n      76.1"
  },
  {
    "objectID": "slides/week2-transform.html#filtering-1",
    "href": "slides/week2-transform.html#filtering-1",
    "title": "Dataframe Transformations",
    "section": "Filtering",
    "text": "Filtering\nTechnically, filtering works by slicing according to a long logical vector with one entry per row specifying whether to retain (True) or drop (False).\n\nundev.total_pop > 1000\n\ncountry\nAfghanistan                           False\nAlbania                               False\nAlgeria                               False\nAndorra                               False\nAngola                                False\n                                      ...  \nVenezuela (Bolivarian Republic of)    False\nViet Nam                              False\nYemen                                 False\nZambia                                False\nZimbabwe                              False\nName: total_pop, Length: 189, dtype: bool"
  },
  {
    "objectID": "slides/week2-transform.html#a-small-puzzle",
    "href": "slides/week2-transform.html#a-small-puzzle",
    "title": "Dataframe Transformations",
    "section": "A small puzzle",
    "text": "A small puzzle\nConsider a random filter:\n\nrandom_filter = np.random.binomial(n = 1, p = 0.03, size = undev.shape[0]).astype('bool')\n\nrandom_filter\n\narray([False,  True, False, False, False, False, False,  True, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n        True, False, False, False, False, False, False, False, False,\n       False, False, False,  True, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False,  True,\n       False,  True, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False])\n\n\n\n\nHow many rows will undev[random_filter] have?\nHow many rows should this random filtering produce on average?"
  },
  {
    "objectID": "slides/week2-transform.html#logical-comparisons",
    "href": "slides/week2-transform.html#logical-comparisons",
    "title": "Dataframe Transformations",
    "section": "Logical comparisons",
    "text": "Logical comparisons\nAny of the following relations can be used to define filtering conditions\n\n\n\nSymbol\nUsage\nMeaning\n\n\n\n\n==\na == b\nDoes a equal b?\n\n\n<=\na <= b\nIs a less than or equal to b?\n\n\n>=\na >= b\nIs a greater than or equal to b?\n\n\n<\na < b\nIs a less than b?\n\n\n>\na > b\nIs a greater than b?\n\n\n~\n~p\nReturns negation of p\n\n\n|\np | q\np OR q\n\n\n&\np & q\np AND q\n\n\n^\np ^ q\np XOR q (exclusive or)"
  },
  {
    "objectID": "slides/week2-transform.html#aggregation",
    "href": "slides/week2-transform.html#aggregation",
    "title": "Dataframe Transformations",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation refers to any operation that combines many values into fewer values.\n\nCommon aggregation operations include:\n\nsummation \\(\\sum_{i} x_i\\)\naveraging \\(n^{-1} \\sum_i x_i\\)\nextrema \\(\\text{min}_i x_i\\) and \\(\\text{max}_i x_i\\)\nstatistics: median, variance, standard deviation, mean absolute deviation, order statistics, quantiles"
  },
  {
    "objectID": "slides/week2-transform.html#aggregation-vs.-other-transformations",
    "href": "slides/week2-transform.html#aggregation-vs.-other-transformations",
    "title": "Dataframe Transformations",
    "section": "Aggregation vs. other transformations",
    "text": "Aggregation vs. other transformations\nAggregations reduce the number of values, whereas other transformations do not.\n\nA bit more formally:\n\naggregations map larger sets of values to smaller sets of values\ntransformations map sets of values to sets of the same size\n\n\n\nCheck your understanding:\n\nis \\((f*g)(x_i) = \\int f(h)g(x_i - h)dh\\) an aggregation?\nis \\(f(x_1, x_2, \\dots, x_n) = \\left(\\prod_i x_i\\right)^{\\frac{1}{n}}\\) an aggregation?"
  },
  {
    "objectID": "slides/week2-transform.html#aggregation-1",
    "href": "slides/week2-transform.html#aggregation-1",
    "title": "Dataframe Transformations",
    "section": "Aggregation?",
    "text": "Aggregation?\n\nGaussian blur."
  },
  {
    "objectID": "slides/week2-transform.html#example-aggregations",
    "href": "slides/week2-transform.html#example-aggregations",
    "title": "Dataframe Transformations",
    "section": "Example aggregations",
    "text": "Example aggregations\nIn numpy, the most common aggregations are implemented as functions:\n\n\n\nnumpy\nfunction\n\n\n\n\nnp.sum()\n\\(\\sum_i x_i\\)\n\n\nnp.max()\n\\(\\text{max}(x_1, \\dots, x_n)\\)\n\n\nnp.min()\n\\(\\text{min}(x_1, \\dots, x_n)\\)\n\n\nnp.median()\n\\(\\text{median}(x_1, \\dots, x_n)\\)\n\n\nnp.mean()\n\\(n^{-1}\\sum_{i = 1}^n x_i\\)\n\n\nnp.var()\n\\((n - 1)^{-1}\\sum_{i = 1}^n (x_i - \\bar{x})^2\\)\n\n\nnp.std()\n\\(\\sqrt{(n - 1)^{-1}\\sum_{i = 1}^n (x_i - \\bar{x})^2}\\)\n\n\nnp.prod()\n\\(\\prod_i x_i\\)\n\n\nnp.percentile()\n\\(\\hat{F}^{-1}(q)\\)"
  },
  {
    "objectID": "slides/week2-transform.html#argmin-and-argmax",
    "href": "slides/week2-transform.html#argmin-and-argmax",
    "title": "Dataframe Transformations",
    "section": "Argmin and argmax",
    "text": "Argmin and argmax\n\\(\\text{argmax}_D f(x)\\) refers to the value or values in the domain \\(D\\) of \\(f\\) at which the function attains its maximum – the argument in \\(D\\) maximizing \\(f\\).\n\nSimilarly, \\(\\text{argmax}_i x_i\\) refers to the index (or indices, if ties) of the largest value in the set \\(\\{x_i\\}\\).\n\n\nCheck your understanding: what does the following return?\n\nnp.array([1, 5, 10, 2]).argmin()"
  },
  {
    "objectID": "slides/week2-transform.html#argmin-and-argmax-1",
    "href": "slides/week2-transform.html#argmin-and-argmax-1",
    "title": "Dataframe Transformations",
    "section": "Argmin and argmax",
    "text": "Argmin and argmax\nThese index retrieval functions can be handy for slicing rows of interest.\n\nFor example, which country had the largest percentage of women in parliament in the year the UN development data was collected?\n\n\n\nundev.index[undev.parliament_pct_women.argmax()]\n\n'Rwanda'\n\n\n\n\nAnd what were the observations?\n\n\n\nundev.iloc[undev.parliament_pct_women.argmax(), :]\n\ntotal_pop                    12.600\nurban_pct_pop                17.300\npop_under5                    1.800\npop_15to64                    7.200\npop_over65                    0.400\ngender_inequality             0.402\nparliament_pct_women         55.700\nlabor_participation_women    83.900\nlabor_participation_men      83.400\nName: Rwanda, dtype: float64"
  },
  {
    "objectID": "slides/week2-transform.html#dataframe-aggregations",
    "href": "slides/week2-transform.html#dataframe-aggregations",
    "title": "Dataframe Transformations",
    "section": "Dataframe aggregations",
    "text": "Dataframe aggregations\nIn pandas, the numpy aggregation operations are available as dataframe methods that apply the corresponding operation over each column:\n\n# mean of every column\nundev.mean()\n\ntotal_pop                    40.423810\nurban_pct_pop                58.660847\npop_under5                    3.666120\npop_15to64                   27.250820\npop_over65                    3.797814\ngender_inequality             0.344154\nparliament_pct_women         23.093048\nlabor_participation_women    52.139888\nlabor_participation_men      72.470787\ndtype: float64"
  },
  {
    "objectID": "slides/week2-transform.html#row-wise-aggregation",
    "href": "slides/week2-transform.html#row-wise-aggregation",
    "title": "Dataframe Transformations",
    "section": "Row-wise aggregation",
    "text": "Row-wise aggregation\nIn general, supplying the argument axis = 1 will compute rowwise aggregations. For example:\n\n# sum `pop_under5`, `pop_15to64`, and `pop_over65`\nundev.iloc[:, 2:5].sum(axis = 1).head(3)\n\ncountry\nAfghanistan    27.5\nAlbania         2.6\nAlgeria        34.9\ndtype: float64\n\n\n\nThis facilitates, for example:\n\nundev['pop_5to14'] = undev.total_pop - undev.iloc[:, 2:5].sum(axis = 1)"
  },
  {
    "objectID": "slides/week2-transform.html#argminidxmin-and-argmaxidxmax",
    "href": "slides/week2-transform.html#argminidxmin-and-argmaxidxmax",
    "title": "Dataframe Transformations",
    "section": "Argmin/idxmin and argmax/idxmax",
    "text": "Argmin/idxmin and argmax/idxmax\nIn pandas, np.argmin() and np.argmax() are implemented as pd.df.idxmin() and pd.df.idxmax().\n\nundev.idxmax()\n\ntotal_pop                                     China\nurban_pct_pop                Hong Kong, China (SAR)\npop_under5                                    India\npop_15to64                                    China\npop_over65                                    China\ngender_inequality                             Yemen\nparliament_pct_women                         Rwanda\nlabor_participation_women                    Rwanda\nlabor_participation_men                       Qatar\npop_5to14                                     India\ndtype: object"
  },
  {
    "objectID": "slides/week2-transform.html#other-functions",
    "href": "slides/week2-transform.html#other-functions",
    "title": "Dataframe Transformations",
    "section": "Other functions",
    "text": "Other functions\nPandas has a wide array of other aggregation and transformation functions. To show just one example:\n\n## slice weather data\nweather4 = weather1.set_index('DATE').iloc[:, 2:4]\nweather4.head(2)\n\n\n\n\n\n  \n    \n      \n      TMAX\n      TMIN\n    \n    \n      DATE\n      \n      \n    \n  \n  \n    \n      1/1/2021\n      65\n      37\n    \n    \n      1/2/2021\n      62\n      38\n    \n  \n\n\n\n\n\n\n# rolling average\nweather4.rolling(window = 7).mean().head(10)\n\n\n\n\n\n  \n    \n      \n      TMAX\n      TMIN\n    \n    \n      DATE\n      \n      \n    \n  \n  \n    \n      1/1/2021\n      NaN\n      NaN\n    \n    \n      1/2/2021\n      NaN\n      NaN\n    \n    \n      1/3/2021\n      NaN\n      NaN\n    \n    \n      1/4/2021\n      NaN\n      NaN\n    \n    \n      1/5/2021\n      NaN\n      NaN\n    \n    \n      1/6/2021\n      NaN\n      NaN\n    \n    \n      1/7/2021\n      66.285714\n      39.571429\n    \n    \n      1/8/2021\n      68.285714\n      39.428571\n    \n    \n      1/9/2021\n      69.571429\n      39.571429\n    \n    \n      1/10/2021\n      70.571429\n      38.857143"
  },
  {
    "objectID": "slides/week2-transform.html#check-your-understanding",
    "href": "slides/week2-transform.html#check-your-understanding",
    "title": "Dataframe Transformations",
    "section": "Check your understanding",
    "text": "Check your understanding\nInterpret this result:\n\nweather4.rolling(window = 7).mean().idxmax()\n\nTMAX    1/20/2021\nTMIN    3/24/2021\ndtype: object\n\n\n(The weather data is January through March.)"
  },
  {
    "objectID": "slides/week2-transform.html#custom-functions",
    "href": "slides/week2-transform.html#custom-functions",
    "title": "Dataframe Transformations",
    "section": "Custom functions",
    "text": "Custom functions\nSee the documentation for a comprehensive list of transformations and aggregations.\n\nIf pandas doesn’t have a method for an operation you’re wanting to perform, you can implement custom transformations/aggregations with:\n\npd.df.apply() or pd.df.transform() apply a function row-wise or column-wise\npd.df.agg() or pd.df.aggregate()"
  },
  {
    "objectID": "slides/week2-transform.html#custom-functions-1",
    "href": "slides/week2-transform.html#custom-functions-1",
    "title": "Dataframe Transformations",
    "section": "Custom functions",
    "text": "Custom functions\nHere’s an example:\n\n\n\n\n\n\n  \n    \n      \n      1961\n      1962\n      1963\n      1964\n      1965\n      1966\n      1967\n      1968\n      1969\n      1970\n      ...\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Country Name\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Argentina\n      5.427843\n      -0.852022\n      -5.308197\n      10.130298\n      10.569433\n      -0.659726\n      3.191997\n      4.822501\n      9.679526\n      3.045643\n      ...\n      10.125398\n      6.003952\n      -1.026420\n      2.405324\n      -2.512615\n      2.731160\n      -2.080328\n      2.818503\n      -2.565352\n      -2.088015\n    \n    \n      Australia\n      2.485769\n      1.296087\n      6.214630\n      6.978522\n      5.983506\n      2.382458\n      6.302620\n      5.095814\n      7.044329\n      7.172187\n      ...\n      2.067417\n      2.462756\n      3.918163\n      2.584898\n      2.533115\n      2.192647\n      2.770652\n      2.300611\n      2.949286\n      2.160956\n    \n  \n\n2 rows × 59 columns\n\n\n\n\n\n# convert percentages to proportions\ngdp_prop = gdp.transform(lambda x: x/100 + 1)\n\n# compute geometric mean\ngdp_prop.aggregate(\n    lambda x: np.prod(x)**(1/len(x)), \n    axis = 1).head(4)\n\nCountry Name\nArgentina    1.022831\nAustralia    1.034228\nAustria      1.027254\nBurundi      1.023854\ndtype: float64"
  },
  {
    "objectID": "slides/week2-transform.html#your-turn-1",
    "href": "slides/week2-transform.html#your-turn-1",
    "title": "Dataframe Transformations",
    "section": "Your turn",
    "text": "Your turn\nHere’s the country with the highest annualized GDP growth for the period 1961-2019:\n\n\nCountry Name\nBotswana    1.079442\ndtype: float64\n\n\n\nHow did I find this? Suppose that the result on the previous slide were stored as gdp_annualized. Write a line of code that generates the result shown above."
  },
  {
    "objectID": "slides/week2-transform.html#grouped-aggregations",
    "href": "slides/week2-transform.html#grouped-aggregations",
    "title": "Dataframe Transformations",
    "section": "Grouped aggregations",
    "text": "Grouped aggregations\nSuppose we wanted to compute annualized growth by decade for each country.\n\nTo do so, we’d compute the same aggregation (geometric mean) repeatedly for subsets of data values. This is called a grouped aggregation.\n\n\nUsually, one defines a grouping of dataframe rows using columns in the dataset. For example:\n\ngdp_decades.head(4)\n\n\n\n\n\n  \n    \n      \n      Country Name\n      growth\n      decade\n    \n  \n  \n    \n      0\n      Argentina\n      1.054278\n      1960\n    \n    \n      1\n      Australia\n      1.024858\n      1960\n    \n    \n      2\n      Austria\n      1.055380\n      1960\n    \n    \n      3\n      Burundi\n      0.862539\n      1960\n    \n  \n\n\n\n\n\n\nHow should the rows be grouped?"
  },
  {
    "objectID": "slides/week2-transform.html#groupby",
    "href": "slides/week2-transform.html#groupby",
    "title": "Dataframe Transformations",
    "section": ".groupby",
    "text": ".groupby\nIn pandas, df.groupby('COLUMN') defines a grouping of dataframe rows in which each group is a set of rows with the same value of 'COLUMN'.\n\nThere will be exactly as many groups as the number of unique values in 'COLUMN'.\nMultiple columns may be specified to define a grouping, e.g., df.groupby(['COL1', 'COL2'])\nSubsequent operations will be performed group-wise"
  },
  {
    "objectID": "slides/week2-transform.html#annualized-gdp-growth-by-decade",
    "href": "slides/week2-transform.html#annualized-gdp-growth-by-decade",
    "title": "Dataframe Transformations",
    "section": "Annualized GDP growth by decade",
    "text": "Annualized GDP growth by decade\nReturning to our example:\n\ngdp_anngrowth = gdp_decades.groupby(\n    ['Country Name', 'decade']\n    ).aggregate(\n    lambda x: np.prod(x)**(1/len(x))\n    )\n\ngdp_anngrowth\n\n\n\n\n\n  \n    \n      \n      \n      growth\n    \n    \n      Country Name\n      decade\n      \n    \n  \n  \n    \n      Algeria\n      1960\n      1.030579\n    \n    \n      1970\n      1.068009\n    \n    \n      1980\n      1.027661\n    \n    \n      1990\n      1.015431\n    \n    \n      2000\n      1.038750\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      Zimbabwe\n      1970\n      1.038505\n    \n    \n      1980\n      1.051066\n    \n    \n      1990\n      1.027630\n    \n    \n      2000\n      0.944765\n    \n    \n      2010\n      1.055855\n    \n  \n\n714 rows × 1 columns"
  },
  {
    "objectID": "slides/week2-transform.html#your-turn-2",
    "href": "slides/week2-transform.html#your-turn-2",
    "title": "Dataframe Transformations",
    "section": "Your turn",
    "text": "Your turn\nHow do you find the country with the highest annualized GDP growth for each decade?\n\nWrite a line of code that would perform this calculation.\n\ngdp_anngrowth...\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      growth\n    \n    \n      decade\n      \n    \n  \n  \n    \n      1960\n      (Iran, Islamic Rep., 1960)\n    \n    \n      1970\n      (Botswana, 1970)\n    \n    \n      1980\n      (Botswana, 1980)\n    \n    \n      1990\n      (China, 1990)\n    \n    \n      2000\n      (Myanmar, 2000)\n    \n    \n      2010\n      (China, 2010)"
  },
  {
    "objectID": "slides/week2-transform.html#recap",
    "href": "slides/week2-transform.html#recap",
    "title": "Dataframe Transformations",
    "section": "Recap",
    "text": "Recap\n\nIn tidy data, rows and columns correspond to observations and variables.\n\nThis provides a standard dataset structure that facilitates exploration and analysis.\nMany datasets are not stored in this format.\nTransformation operations are a lot easier with tidy data, due in part to the way tools in pandas are designed.\n\nTransformations are operations that modify the shape or values of dataframes. We discussed\n\nslicing\nfiltering\ncreating new variables\naggregations (mean, min, max, argmin, etc.)\ngrouped aggregations\n\nDataframe manipulations will be used throughout the course to tidy up data and perform various inspections and summaries."
  },
  {
    "objectID": "slides/week2-transform.html#up-next",
    "href": "slides/week2-transform.html#up-next",
    "title": "Dataframe Transformations",
    "section": "Up next",
    "text": "Up next\nWe started en media res at this stage of the lifecyle (tidy) so that you could start developing skills that would enable you to jump right into playing with datasets.\n\nNext week, we’ll backtrack to the data collection and assessment stages of a project and discuss:\n\nsampling\nscope of inference\ndata assessment\nmissing data"
  },
  {
    "objectID": "slides/week3-casestudy.html#recap",
    "href": "slides/week3-casestudy.html#recap",
    "title": "Case study on sampling and missingness",
    "section": "Recap",
    "text": "Recap"
  },
  {
    "objectID": "slides/week3-casestudy.html#the-miller-case",
    "href": "slides/week3-casestudy.html#the-miller-case",
    "title": "Case study on sampling and missingness",
    "section": "The Miller case",
    "text": "The Miller case\nOn November 21, 2020, a professor at Williams College, Steven Miller, filed an affidavit alleging that an analysis of phone surveys showed that among registered republican voters in PA:\n\n~40K mail ballots were fraudlently requested;\n~48K mail ballots were not counted.\n\n\n\n“President Donald J. Trump amplified the statement in a tweet, the Chairman of the Federal Elections Commission (FEC) referenced the statement as indicative of fraud, and a conservative group prominently featured it in a legal brief seeking to overturn the Pennsylvania election results.” (Samuel Wolf, Williams Record, 11/25/20)\n\n\n\nThe Miller affidavit was criticized by statisticians as incorrect, irresponsible, and unethical."
  },
  {
    "objectID": "slides/week3-casestudy.html#the-flawed-assumption",
    "href": "slides/week3-casestudy.html#the-flawed-assumption",
    "title": "Case study on sampling and missingness",
    "section": "The flawed assumption",
    "text": "The flawed assumption\nOn a purely mathematical level, Miller’s calculations were standard. The key issue was a single flawed assumption:\n\n\n“The analysis is predicated on the assumption that the responders are a representative sample of the population of registered Republicans in Pennsylvania for whom a mail-in ballot was requested but not counted, and responded accurately to the questions during the phone calls.”” (Miller affidavit)\n\n\n\nEssentially, two critical mistakes were made in the analysis:\n\nFailure to critically assess the sampling design and scope of inference.\nIgnoring missing data.\n\n\n\nMiller is a number theorist, not a trained survey statistician, so on some level his mistakes were understandable, but they did a lot of damage. He issued an apology in short order."
  },
  {
    "objectID": "slides/week3-casestudy.html#sampling-design",
    "href": "slides/week3-casestudy.html#sampling-design",
    "title": "Case study on sampling and missingness",
    "section": "Sampling design",
    "text": "Sampling design\n\nThere were 165,412 unreturned mail ballots requested by registered republicans in PA.\n\n\nThose voters were surveyed by phone by Matt Braynard’s private firm External Affairs on behalf of the Voter Integrity Fund.\n\n\nWe don’t really know how they obtained and selected phone numbers or exactly what the survey procedure was, but here’s what we do know:\n\n~23K individuals were called on Nov. 9-10.\nThe ~2.5K who answered were asked if they were the registered voter or a family member.\nIf they said yes, they were asked if they requested a ballot.\nThose who requested a ballot were asked if they mailed it."
  },
  {
    "objectID": "slides/week3-casestudy.html#immediate-problems",
    "href": "slides/week3-casestudy.html#immediate-problems",
    "title": "Case study on sampling and missingness",
    "section": "Immediate problems",
    "text": "Immediate problems\nWe’ll look in greater detail at the survey, but there are some obvious problems to start.\n\n~23K individuals were called on Nov. 9-10\n\nUndisclosed selection mechanism\nNarrow snapshot in time.\n9th and 10th were a Monday and Tuesday.\nMail ballots were still being counted, so the interviewers don’t actually know whether returned ballots were ultimately counted or not by this time.\n\nThe ~2.5K who answered were asked if they were the registered voter or a family member of the registered voter.\n\nFamily members could answer on behalf of one another, and may give incorrect answers.\n\nIf they said yes, they were asked if the voter requested a ballot.\n\nMisleading question: there’s a registration checkbox; you don’t have to file an explicit request in Pennsylvania.\n\nThose who said they requested a ballot were asked if they mailed it.\n\nWhat about voters who claimed not to request a ballot? Did they receive one, and if so, did they mail it?"
  },
  {
    "objectID": "slides/week3-casestudy.html#survey-schematic",
    "href": "slides/week3-casestudy.html#survey-schematic",
    "title": "Case study on sampling and missingness",
    "section": "Survey schematic",
    "text": "Survey schematic"
  },
  {
    "objectID": "slides/week3-casestudy.html#sampling-design-1",
    "href": "slides/week3-casestudy.html#sampling-design-1",
    "title": "Case study on sampling and missingness",
    "section": "Sampling design",
    "text": "Sampling design\nPopulation: republicans registered to vote in PA who had mail ballots officially requested that hadn’t been returned or counted by November 9?\n\nSampling frame: unknown; source of phone numbers unspecified.\n\n\nSample: 2684 registered republicans or family members of registered repbulicans who had a mail ballot officially requested in PA and answered survey calls on Nov. 9 or 10.\n\n\nSampling mechanism: nonrandom; depends on availability during calling hours on Monday and Tuesday, language spoken, and willingness to talk.\n\n\nThis is not a representative sample of any meaningful population."
  },
  {
    "objectID": "slides/week3-casestudy.html#missingness",
    "href": "slides/week3-casestudy.html#missingness",
    "title": "Case study on sampling and missingness",
    "section": "Missingness",
    "text": "Missingness\nRespondents hung up at every stage of the survey. This is probably not at random – individuals who do not believe voter fraud occurred are more likely to hang up.\n\nSo data are MNAR, and likely over-represent people more likely to claim they never requested a ballot."
  },
  {
    "objectID": "slides/week3-casestudy.html#the-analysis",
    "href": "slides/week3-casestudy.html#the-analysis",
    "title": "Case study on sampling and missingness",
    "section": "The analysis",
    "text": "The analysis\nMiller first calculated the proportion of respondents who reported not requesting ballots among those who did not hang up.\n\\[\n\\left(\\frac{556}{1150 + 556 + 544}\\right) = 0.2471\n\\]\n\nThen he extrapolated that the estimated number of fraudulent requests was:\n\\[\n0.2471 \\times 165,412 = 40,875\n\\]\n\n\nThe two main problems with this are:\n\nnonrandom sampling \\(\\Longrightarrow\\) no scope of inference\nno adjustment for nonresponse (i.e., missing data)"
  },
  {
    "objectID": "slides/week3-casestudy.html#simulation",
    "href": "slides/week3-casestudy.html#simulation",
    "title": "Case study on sampling and missingness",
    "section": "Simulation",
    "text": "Simulation\nIt’s not too tricky to envision sources of bias that would affect the results.\n\nAssume that:\n\nrespondents all know whether they actually requested a ballot\nrespondents tell the truth\nrespondents who didn’t request a ballot are more likely to be reached\nrespondents who did request a ballot are more likely to hang up during the interview\n\n\n\nThen we can show through a simple simulation that an actual fraud rate of under 1% will be estimated at over 20% almost all the time."
  },
  {
    "objectID": "slides/week3-casestudy.html#simulated-population",
    "href": "slides/week3-casestudy.html#simulated-population",
    "title": "Case study on sampling and missingness",
    "section": "Simulated population",
    "text": "Simulated population\nFirst let’s generate a population of 150K voters.\n\n\nnp.random.seed(41021)\n\n# proportion of fraudlent requests\ntrue_prop = 0.009\n\n# generate population of 100K; 100 of 100K did not request a ballot\nN = 150000\npopulation = pd.DataFrame(data = {'requested': np.ones(N)})\nnum_nrequest = round(N*true_prop) - 1\npopulation.iloc[0:num_nrequest, 0] = 0"
  },
  {
    "objectID": "slides/week3-casestudy.html#simulated-sample",
    "href": "slides/week3-casestudy.html#simulated-sample",
    "title": "Case study on sampling and missingness",
    "section": "Simulated sample",
    "text": "Simulated sample\nThen let’s introduce sampling weights based on the conditional probability that an individual will talk with the interviewer given whether they requested a ballot or not.\n\n\n# assume respondents tell the truth\np_request = 1 - true_prop\np_nrequest = true_prop\n\n# assume respondents who claim no request are 15x more likely to talk\ntalk_factor = 15\n\n# observed response rate\np_talk = 0.09\n\n# conditional probability of talking given claimed request or not \np_talk_request = p_talk/(p_request + talk_factor*p_nrequest) \np_talk_nrequest = talk_factor*p_talk_request\n\n# draw sample weighted by conditional probabilities\nnp.random.seed(41021)\npopulation.loc[population.requested == 1, 'sample_weight'] = p_talk_request\npopulation.loc[population.requested == 0, 'sample_weight'] = p_talk_nrequest\nsamp = population.sample(n = 2500, replace = False, weights = 'sample_weight')"
  },
  {
    "objectID": "slides/week3-casestudy.html#simulated-missing-mechanism",
    "href": "slides/week3-casestudy.html#simulated-missing-mechanism",
    "title": "Case study on sampling and missingness",
    "section": "Simulated missing mechanism",
    "text": "Simulated missing mechanism\nThen let’s introduce missing values at different rates for respondents who requested a ballot and respondents who didn’t.\n\n\n# assume respondents who affirm requesting are 4x more likely to hang up\nmissing_factor = 4\n\n# observed missing/unsure rate\np_missing = 0.25\n\n# conditional probabilities of missing given request status\np_missing_nrequest = p_missing/(0.8 + missing_factor*0.2) \np_missing_request = missing_factor*p_missing_nrequest\n\n# input missing values\nnp.random.seed(41021)\nsamp.loc[samp.requested == 1, 'missing_weight'] = p_missing_request\nsamp.loc[samp.requested == 0, 'missing_weight'] = p_missing_nrequest\nsamp['missing'] = np.random.binomial(n = 1, p = samp.missing_weight.values)\nsamp.loc[samp.missing == 1, 'requested'] = float('nan')"
  },
  {
    "objectID": "slides/week3-casestudy.html#simulated-result",
    "href": "slides/week3-casestudy.html#simulated-result",
    "title": "Case study on sampling and missingness",
    "section": "Simulated result",
    "text": "Simulated result\nIf we then drop all the missing values and calculate the proportion of respondents who didn’t request a ballot, we get:\n\n\n# compute mean after dropping missing values\n1 - samp.requested.mean()\n\n0.21206743566992015\n\n\n\n\nSo Miller’s result is expected if the sampling and missing mechanisms introduce bias, even if the true rate of fraudulent requests is under 1% – on the order of 1,000 ballots."
  },
  {
    "objectID": "slides/week3-casestudy.html#your-turn",
    "href": "slides/week3-casestudy.html#your-turn",
    "title": "Case study on sampling and missingness",
    "section": "Your turn",
    "text": "Your turn\nOpen the notebook and try it for yourself."
  },
  {
    "objectID": "slides/week3-casestudy.html#takeaways",
    "href": "slides/week3-casestudy.html#takeaways",
    "title": "Case study on sampling and missingness",
    "section": "Takeaways",
    "text": "Takeaways\nThe main mistakes were ignoring the sampling design and missing data. We should assume these were honest mistakes.\n\nAfter the affidavit was filed, a colleague spoke with Miller; he recanted and acknowledged his mistakes, but this received far less attention than the conclusions in the affidavit."
  },
  {
    "objectID": "slides/week3-casestudy.html#professional-ethics-and-social-responsibility",
    "href": "slides/week3-casestudy.html#professional-ethics-and-social-responsibility",
    "title": "Case study on sampling and missingness",
    "section": "Professional ethics and social responsibility",
    "text": "Professional ethics and social responsibility\nThe American Statistical Association publishes ethical guidelines for statistical practice. The Miller case violated a large number of these, most prominently, that an ethical practitioner:\n\nReports the sources and assessed adequacy of the data, accounts for all data considered in a study, and explains the sample(s) actually used.\nIn publications and reports, conveys the findings in ways that are both honest and meaningful to the user/reader. This includes tables, models, and graphics.\nIn publications or testimony, identifies the ultimate financial sponsor of the study, the stated purpose, and the intended use of the study results.\nWhen reporting analyses of volunteer data or other data that may not be representative of a defined population, includes appropriate disclaimers and, if used, appropriate weighting."
  },
  {
    "objectID": "slides/week3-sampling.html#announcements",
    "href": "slides/week3-sampling.html#announcements",
    "title": "Sampling and missingness",
    "section": "Announcements",
    "text": "Announcements\n\nFirst mini project released: air quality in U.S. cities\nLab 1 (pandas) due Monday 4/17 11:59pm PST\nHW 1 due in one week on Monday 4/24"
  },
  {
    "objectID": "slides/week3-sampling.html#this-week",
    "href": "slides/week3-sampling.html#this-week",
    "title": "Sampling and missingness",
    "section": "This week",
    "text": "This week\nObjective: Enable you to critically assess data quality based on how it was collected.\n\nSampling and statistical bias\n\nSampling terminology\nCommon sampling scenarios\nSampling mechanisms\nStatistical bias\n\nThe missing data problem\n\nTypes of missingness: MCAR, MAR, and MNAR\nPitfalls and simple fixes\n\nCase study: voter fraud\n\nSteven Miller’s analysis of Voter Integrity Fund surveys\nSources of bias\nEthical considerations"
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-terminology",
    "href": "slides/week3-sampling.html#sampling-terminology",
    "title": "Sampling and missingness",
    "section": "Sampling terminology",
    "text": "Sampling terminology\nHere we’ll introduce standard statistical terminology to describe data collection.\n\nAll data are collected somehow. A sampling design is a way of selecting observational units for measurement. It can be construed as a particular relationship between:\n\na population (all entities of interest);\na sampling frame (all entities that are possible to measure); and\na sample (a specific collection of entities)."
  },
  {
    "objectID": "slides/week3-sampling.html#population",
    "href": "slides/week3-sampling.html#population",
    "title": "Sampling and missingness",
    "section": "Population",
    "text": "Population\nLast week, we introduced the terminology observational unit to mean the entity measured for a study – datasets consist of observations made on observational units.\n\nIn less technical terms, all data are data on some kind of thing, such as countries, species, locations, and the like.\n\n\n\n\nA statistical population is the collection of all units of interest. For example:\n\nall countries (GDP data)\nall mammal species (Allison 1976)\nall babies born in the US (babynames data)\nall locations in a region (SB weather data)\nall adult U.S. residents (BRFSS data)"
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-frame",
    "href": "slides/week3-sampling.html#sampling-frame",
    "title": "Sampling and missingness",
    "section": "Sampling frame",
    "text": "Sampling frame\nThere are usually some units in a population that can’t be measured due to practical constraints – for instance, many adult U.S. residents don’t have phones or addresses.\n\n\n\nFor this reason, it is useful to introduce the concept of a sampling frame, which refers to the collection of all units in a population that can be observed for a study. For example:\n\nall countries reporting economic output between 1961 and 2019\nall babies with birth certificates from U.S. hospitals born between 1990 and 2018\nall adult U.S. residents with phone numbers in 2019"
  },
  {
    "objectID": "slides/week3-sampling.html#sample",
    "href": "slides/week3-sampling.html#sample",
    "title": "Sampling and missingness",
    "section": "Sample",
    "text": "Sample\nFinally, it’s rarely feasible to measure every observable unit due to limited data collection resources – for instance, states don’t have the time or money to call every phone number every year.\n\n\n\nA sample is a subcollection of units in the sampling frame actually selected for study. For instance:\n\n234 countries;\n62 mammal species;\n13,684,689 babies born in CA;\n1 weather station location at SB airport;\n418,268 adult U.S. residents."
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-scenarios",
    "href": "slides/week3-sampling.html#sampling-scenarios",
    "title": "Sampling and missingness",
    "section": "Sampling scenarios",
    "text": "Sampling scenarios\nWe can now imagine a few common sampling scenarios by varying the relationship between population, frame, and sample.\n\nDenote an observational unit by \\(U_i\\), and let:\n\\[\\begin{alignat*}{2}\n\\mathcal{U} &= \\{U_i\\}_{i \\in I} &&\\quad(\\text{universe}) \\\\\nP &= \\{U_1, \\dots, U_N\\} \\subseteq \\mathcal{U} &&\\quad(\\text{population}) \\\\\n    F &= \\{U_j: j \\in J \\subset I\\} \\subseteq P &&\\quad(\\text{frame})\\\\\n    S &\\subseteq F &&\\quad(\\text{sample})\n\\end{alignat*}\\]"
  },
  {
    "objectID": "slides/week3-sampling.html#census",
    "href": "slides/week3-sampling.html#census",
    "title": "Sampling and missingness",
    "section": "Census",
    "text": "Census\nThe simplest scenario is a population census, where the entire population is observed.\n\n\n\nFor a census: \\(S = F = P\\)\nAll properties of the population are definitevely known in a census. So there is no need to model census data."
  },
  {
    "objectID": "slides/week3-sampling.html#simple-random-sample",
    "href": "slides/week3-sampling.html#simple-random-sample",
    "title": "Sampling and missingness",
    "section": "Simple random sample",
    "text": "Simple random sample\nThe statistical gold standard for inference, modeling, and prediction is the simple random sample in which units are selected at random from the population.\n\n\n\nFor a simple random sample: \\(S \\subset F = P\\)\nSample properties are reflective of population properties in simple random samples. Population inference is straightforward."
  },
  {
    "objectID": "slides/week3-sampling.html#typical-sample",
    "href": "slides/week3-sampling.html#typical-sample",
    "title": "Sampling and missingness",
    "section": "‘Typical’ sample",
    "text": "‘Typical’ sample\nMore common in practice is a random sample from a sampling frame that overlaps but does not cover the population.\n\n\n\nFor a ‘typical’ sample: \\(S \\subset F \\quad\\text{and}\\quad F \\cap P \\neq \\emptyset\\)\nSample properties are reflective of the frame but not necessarily the study population. Population inference gets more complicated and may not be possible."
  },
  {
    "objectID": "slides/week3-sampling.html#administrative-data",
    "href": "slides/week3-sampling.html#administrative-data",
    "title": "Sampling and missingness",
    "section": "‘Administrative’ data",
    "text": "‘Administrative’ data\nAlso common is administrative data in which all units are selected from a convenient frame that partly covers the population.\n\n\n\nFor administrative data: \\(S = F \\quad\\text{and}\\quad F\\cap P \\neq \\emptyset\\)\nAdministrative data are not really proper samples; they cannot be replicated and they do not represent any broader group. No inference is possible."
  },
  {
    "objectID": "slides/week3-sampling.html#scope-of-inference",
    "href": "slides/week3-sampling.html#scope-of-inference",
    "title": "Sampling and missingness",
    "section": "Scope of inference",
    "text": "Scope of inference\nThe relationships among the population, frame, and sample determine the scope of inference: the extent to which conclusions based on the sample are generalizable.\n\nA good sampling design can ensure that the statistical properties of the sample are expected to match those of the population. If so, it is sound to generalize:\n\nthe sample is said to be representative of the population\nthe scope of inference is broad\n\n\n\nA poor sampling design will produce samples that distort the statistical properties of the population. If so, it is not sound to generalize:\n\nsample statistics are subjet to bias\nthe scope of inference is narrow"
  },
  {
    "objectID": "slides/week3-sampling.html#characterizing-sampling-designs",
    "href": "slides/week3-sampling.html#characterizing-sampling-designs",
    "title": "Sampling and missingness",
    "section": "Characterizing sampling designs",
    "text": "Characterizing sampling designs\nThe sampling scenarios above can be differentiated along two key attributes:\n\nThe overlap between the sampling frame and the population.\n\nframe \\(=\\) population\nframe \\(\\subset\\) population\nframe \\(\\cap\\) population \\(\\neq \\emptyset\\)\n\nThe mechanism of obtaining a sample from the sampling frame.\n\nrandom sampling\nconvenience sampling\n\n\n\nIf you can articulate these two points, you have fully characterized the sampling design."
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-mechanisms",
    "href": "slides/week3-sampling.html#sampling-mechanisms",
    "title": "Sampling and missingness",
    "section": "Sampling mechanisms",
    "text": "Sampling mechanisms\nIn order to describe sampling mechanisms precisely, we need a little terminology.\n\nEach unit has some inclusion probability – the probability of being included in the sample.\n\n\nLet’s suppose that the frame \\(F\\) comprises \\(N\\) units, and denote the inclusion probabilities by:\n\\[\np_i = P(\\text{unit } i \\text{ is included in the sample})\n\\quad i = 1, \\dots, N\n\\]\nThe inclusion probability of each unit depends on the physical procedure of collecting data."
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-mechanisms-1",
    "href": "slides/week3-sampling.html#sampling-mechanisms-1",
    "title": "Sampling and missingness",
    "section": "Sampling mechanisms",
    "text": "Sampling mechanisms\nSampling mechanisms are methods of drawing samples and are categorized into four types based on inclusion probabilities.\n\nin a census every unit is included\n\n\\(p_i = 1\\) for every unit \\(i = 1, \\dots, N\\)\n\nin a random sample every unit is equally likely to be included\n\n\\(p_i = p_j\\) for every pair of units \\(i, j\\)\n\nin a probability sample units have different inclusion probabilities\n\n\\(p_i \\neq p_j\\) for at least one \\(i \\neq j\\)\n\nin a nonrandom sample there is no random mechanism\n\n\\(p_i = 1\\) for \\(i \\in S\\)"
  },
  {
    "objectID": "slides/week3-sampling.html#revisiting-example-datasets-gdp",
    "href": "slides/week3-sampling.html#revisiting-example-datasets-gdp",
    "title": "Sampling and missingness",
    "section": "Revisiting example datasets: GDP",
    "text": "Revisiting example datasets: GDP\nAnnual observations of GDP growth for 234 countries from 1961 - 2018.\n\nPopulation: all countries in existence between 1961-2019.\nFrame: all countries reporting economic output for at least one year between 1961 and 2019.\nSample: equal to frame.\n\n\nSo:\n\nOverlap: frame partly overlaps population.\nMechanism: sample is every country in the sampling frame.\n\n\n\nThis is administrative data with no scope of inference."
  },
  {
    "objectID": "slides/week3-sampling.html#revisiting-example-datasets-brfss-data",
    "href": "slides/week3-sampling.html#revisiting-example-datasets-brfss-data",
    "title": "Sampling and missingness",
    "section": "Revisiting example datasets: BRFSS data",
    "text": "Revisiting example datasets: BRFSS data\nPhone surveys of 418K U.S. residents in 2019.\n\nPopulation: all U.S. residents.\nFrame: all adult U.S. residents with phone numbers.\nSample: 418K adult U.S. residents with phone numbers.\n\n\nSo:\n\nOverlap: frame is a subset of the population.\nMechanism: probability sample.\n\nRandomly selected phone numbers were dialed in each state, so individuals in less populous states or with multiple numbers are more likely to be included\n\n\n\n\nThis is a typical sample with narrow inference to adult residents with phone numbers."
  },
  {
    "objectID": "slides/week3-sampling.html#statistical-bias",
    "href": "slides/week3-sampling.html#statistical-bias",
    "title": "Sampling and missingness",
    "section": "Statistical bias",
    "text": "Statistical bias\nStatistical bias is the average difference between a sample property and a population property across all possible samples under a particular sampling design.\n\nIn less technical terms: the expected error of estimates.\n\n\nTwo possible sources of statistical bias:\n\nAn estimator systematically over- or under-estimates its target population property\n\ne.g., \\(\\frac{1}{n}\\sum_i (x_i - \\bar{x})^2\\) is biased for (underestimates) the population variance\n\nSampling design systematically over- or under-represents certain observational units\n\ne.g., studies conducted on college campuses are biased towards (overrepresent) young adults\n\n\n\n\nThese are distinct from other kinds of bias that we are not discussing:\n\nMeasurement bias: attributes or outcomes are measured unevenly across populations\nExperimenter bias: study design and/or outcomes favor an investigator’s preconceptions"
  },
  {
    "objectID": "slides/week3-sampling.html#sampling-bias",
    "href": "slides/week3-sampling.html#sampling-bias",
    "title": "Sampling and missingness",
    "section": "Sampling bias",
    "text": "Sampling bias\nIn Lab 2 you’ll explore sampling bias arising from sampling mechanisms. Here’s a preview:\n\n\n\n\n\nDistributions of body length by sex (top) and in aggregate (bottom) for a hypothetical population of 5K hawks.\n\n\n\nConsider:\n\nAre males or females generally longer?\nHow will the sample mean shift if disproportionately more males are sampled?\nIf disproportionately more females are sampled?"
  },
  {
    "objectID": "slides/week3-sampling.html#bias-corrections",
    "href": "slides/week3-sampling.html#bias-corrections",
    "title": "Sampling and missingness",
    "section": "Bias corrections",
    "text": "Bias corrections\nIf inclusion probabilities are known or estimable it is possible to apply bias corrections to estimates using inverse probability weighting.\n\nIf\n\n\\(p_i\\) is the probability that individual \\(i\\) is included in the sample \\(S\\)\n\\(Y_i\\) are observations of a variable of interest\n\n\n\nThen a bias-corrected estimate of the population mean is given by the weighted average:\n\\[\n\\sum_{i\\in S} \\left(\\frac{p_i^{-1}}{\\sum_i p_i^{-1}}\\right) Y_i\n\\]"
  },
  {
    "objectID": "slides/week3-sampling.html#bias-correction-example",
    "href": "slides/week3-sampling.html#bias-correction-example",
    "title": "Sampling and missingness",
    "section": "Bias correction example",
    "text": "Bias correction example\nSuppose we obtain a biased sample in which female hawks were 6 times as likely to be selected as males. This yields an overestimate:\n\n\npopulation mean:  54.73771716352954\nsample mean:  56.567779534464016\n\n\n\nBut since we know the exact inclusion probabilities up to a proportionality constant, we can apply inverse probability weighting to adjust for bias:\n\n# specify weights s.t. 6:1 female:male\nweight_df = pd.DataFrame(\n    data = {'sex': np.array(['male', 'female']),\n            'weight': np.array([1, 6])})\n\n# append weights to sample\nsamp_w = pd.merge(samp, weight_df, how = 'left', on = 'sex')\n\n# calculate inverse probability weightings\nsamp_w['correction_factor'] = (1/samp_w.weight)/np.sum(1/samp_w.weight)\n\n# multiply observed values by weightings\nsamp_w['weighted_length'] = samp_w.length*samp_w.correction_factor\n\n# take weighted average\nsamp_w.weighted_length.sum()\n\n54.40928091743469"
  },
  {
    "objectID": "slides/week3-sampling.html#bias-correction-example-1",
    "href": "slides/week3-sampling.html#bias-correction-example-1",
    "title": "Sampling and missingness",
    "section": "Bias correction example",
    "text": "Bias correction example\nHowever, even if we didn’t know the exact inclusion probabilities, we could estimate them from the sample:\n\nsamp.sex.value_counts()\n\nfemale    88\nmale      12\nName: sex, dtype: int64\n\n\n\nAnd use the same approach:\n\n# estimate factor by which F more likely than M\nratio = samp.sex.value_counts().loc['female']/samp.sex.value_counts().loc['male']\n\n# input as weights\nweight_df = pd.DataFrame(data = {'sex': np.array(['male', 'female']), 'weight': np.array([1, ratio])})\n\n# append weights to sample\nsamp_w = pd.merge(samp, weight_df, how = 'left', on = 'sex')\n\n# calculate inverse probability weightings\nsamp_w['correction_factor'] = (1/samp_w.weight)/np.sum(1/samp_w.weight)\n\n# multiply observed values by weightings\nsamp_w['weighted_length'] = samp_w.length*samp_w.correction_factor\n\n# take weighted average\nsamp_w.weighted_length.sum()\n\n54.082235672430265"
  },
  {
    "objectID": "slides/week3-sampling.html#remarks-on-ipw-and-bias-correction",
    "href": "slides/week3-sampling.html#remarks-on-ipw-and-bias-correction",
    "title": "Sampling and missingness",
    "section": "Remarks on IPW and bias correction",
    "text": "Remarks on IPW and bias correction\nInverse probability weighting can be applied to correct a wide range of estimators besides averages.\n\nIt is also applicable to adjust for bias due to missing data.\n\n\nIn principle, the technique is simple, but in practice, there are some common hurdles:\n\nusually inclusion probabilities are not known\nestimating inclusion probabilities can be difficult and messy"
  },
  {
    "objectID": "slides/week3-sampling.html#missingness",
    "href": "slides/week3-sampling.html#missingness",
    "title": "Sampling and missingness",
    "section": "Missingness",
    "text": "Missingness\nMissing data arise when one or more variable measurements fail for a subset of observations.\n\nThis can happen for a variety of reasons, but is very common in pratice due to, for instance:\n\nequipment failure;\nsample contamination or loss;\nrespondents leaving questions blank;\nattrition (dropping out) of study participants.\n\n\n\nMany researchers and data scientists ignore missingness by simply deleting affected observations, but this is bad practice! Missingness needs to be treated carefully."
  },
  {
    "objectID": "slides/week3-sampling.html#missing-representations",
    "href": "slides/week3-sampling.html#missing-representations",
    "title": "Sampling and missingness",
    "section": "Missing representations",
    "text": "Missing representations\nIt is standard practice to record observations with missingness but enter a special symbol (.., -, NA, etcetera) for missing values.\n\nIn python, missing values are mapped to a special float:\n\n\n\nfloat('nan')\n\nnan"
  },
  {
    "objectID": "slides/week3-sampling.html#missing-representations-1",
    "href": "slides/week3-sampling.html#missing-representations-1",
    "title": "Sampling and missingness",
    "section": "Missing representations",
    "text": "Missing representations\nHere is some made-up data with two missing values:\n\n\n\n\n\n\n\n  \n    \n      \n      value\n    \n    \n      obs\n      \n    \n  \n  \n    \n      0\n      -0.9286936933427271\n    \n    \n      1\n      -0.3088381742999848\n    \n    \n      2\n      -\n    \n    \n      3\n      -1.4345064041945543\n    \n    \n      4\n      0.03958917896644836\n    \n    \n      5\n      -\n    \n    \n      6\n      -0.5316890502224456\n    \n    \n      7\n      1.4734842645335422"
  },
  {
    "objectID": "slides/week3-sampling.html#missing-representations-2",
    "href": "slides/week3-sampling.html#missing-representations-2",
    "title": "Sampling and missingness",
    "section": "Missing representations",
    "text": "Missing representations\nIf we read in the file with an na_values argument, pandas will parse the specified characters as NaN:\n\n\nsome_data = pd.read_csv('data/some_data.csv', index_col = 'obs', na_values = '-')\nsome_data\n\n\n\n\n\n  \n    \n      \n      value\n    \n    \n      obs\n      \n    \n  \n  \n    \n      0\n      -0.928694\n    \n    \n      1\n      -0.308838\n    \n    \n      2\n      NaN\n    \n    \n      3\n      -1.434506\n    \n    \n      4\n      0.039589\n    \n    \n      5\n      NaN\n    \n    \n      6\n      -0.531689\n    \n    \n      7\n      1.473484"
  },
  {
    "objectID": "slides/week3-sampling.html#calculations-with-nans",
    "href": "slides/week3-sampling.html#calculations-with-nans",
    "title": "Sampling and missingness",
    "section": "Calculations with NaNs",
    "text": "Calculations with NaNs\nNaNs halt calculations on numpy arrays.\n\n# mean in numpy -- halt\nsome_data.values.mean()\n\nnan\n\n\n\nHowever, the default behavior in pandas is to ignore the NaN’s, which allows the computation to proceed:\n\n\n\n# mean in pandas -- ignore\nsome_data.mean()\n\nvalue   -0.281776\ndtype: float64"
  },
  {
    "objectID": "slides/week3-sampling.html#omitting-missing-values-alters-results",
    "href": "slides/week3-sampling.html#omitting-missing-values-alters-results",
    "title": "Sampling and missingness",
    "section": "Omitting missing values alters results",
    "text": "Omitting missing values alters results\nBut those missing values could have been anything. For example:\n\n\n# one counterfactual scenario\ncomplete_data = some_data.copy()\ncomplete_data.loc[[2, 5], 'value'] = [5, 6] \n\n\n\nNow the mean is:\n\ncomplete_data.mean()\n\nvalue    1.163668\ndtype: float64\n\n\n\n\nSo missing values can dramatically alter results if they are simply omitted from calculations!"
  },
  {
    "objectID": "slides/week3-sampling.html#the-missing-data-problem",
    "href": "slides/week3-sampling.html#the-missing-data-problem",
    "title": "Sampling and missingness",
    "section": "The missing data problem",
    "text": "The missing data problem\nIn a nutshell, the missing data problem is: how should missing values be handled in a data analysis?\n\n\nGetting the software to run is one thing, but this alone does not address the challenges posed by the missing data. Unless the analyst, or the software vendor, provides some way to work around the missing values, the analysis cannot continue because calculations on missing values are not possible. There are many approaches to circumvent this problem. Each of these affects the end result in a different way. (Stef van Buuren, 2018)\n\n\n\nThere’s no universal approach to the missing data problem. The choice of method depends on:\n\nthe analysis objective;\nthe missing data mechanism."
  },
  {
    "objectID": "slides/week3-sampling.html#missing-data-in-pstat100",
    "href": "slides/week3-sampling.html#missing-data-in-pstat100",
    "title": "Sampling and missingness",
    "section": "Missing data in PSTAT100",
    "text": "Missing data in PSTAT100\nWe won’t go too far into this topic in PSTAT 100. Our goal will be awareness-raising, specifically:\n\ncharacterizing types of missingness (missing data mechanisms);\nunderstanding missingness as a potential source of bias;\nbasic do’s and don’t’s when it comes to missingness.\n\n\nIf you are interested in the topic, Stef van Buuren’s Flexible Imputation of Missing Data (the source of one of your readings this week) provides an excellent introduction."
  },
  {
    "objectID": "slides/week3-sampling.html#missing-data-mechanisms",
    "href": "slides/week3-sampling.html#missing-data-mechanisms",
    "title": "Sampling and missingness",
    "section": "Missing data mechanisms",
    "text": "Missing data mechanisms\nMissing data mechanisms (like sampling mechanisms) are characterized by the probabilities that observations go missing.\n\nFor dataset \\(X = \\{x_{ij}\\}\\) comprising\n\n\\(n\\) rows/observations\n\\(p\\) columns/variables\n\n\n\ndenote the probability that a value goes missing as:\n\\[\nq_{ij} = P(x_{ij} \\text{ is missing})\n\\]"
  },
  {
    "objectID": "slides/week3-sampling.html#missing-completely-at-random",
    "href": "slides/week3-sampling.html#missing-completely-at-random",
    "title": "Sampling and missingness",
    "section": "Missing completely at random",
    "text": "Missing completely at random\nData are missing completely at random (MCAR) if the probabilities of missing entries are uniformly equal.\n\n\\[\nq_{ij} = q\n\\quad\\text{for all}\\quad\ni = 1, \\dots, n\n\\quad\\text{and}\\quad\nj = 1, \\dots, p\n\\]\n\n\nThis implies that the cause of missingness is unrelated to the data: missing values can be ignored. This is the easiest scenario to handle."
  },
  {
    "objectID": "slides/week3-sampling.html#missing-at-random",
    "href": "slides/week3-sampling.html#missing-at-random",
    "title": "Sampling and missingness",
    "section": "Missing at random",
    "text": "Missing at random\nData are missing at random (MAR) if the probabilities of missing entries depend on observed data.\n\n\\[\nq_{ij} = f(\\mathbf{x}_i)\n\\]\n\n\nThis implies that information about the cause of missingness is captured within the dataset. As a result:\n\nit is possible to estimate \\(q_{ij}\\)\nbias corrections using inverse probability weighting can be implemented"
  },
  {
    "objectID": "slides/week3-sampling.html#missing-not-at-random",
    "href": "slides/week3-sampling.html#missing-not-at-random",
    "title": "Sampling and missingness",
    "section": "Missing not at random",
    "text": "Missing not at random\nData are missing not at random (MNAR) if the probabilities of missing entries depend on unobserved data.\n\n\\[\nq_{ij} = f(z_i, x{ij}) \\quad z_i \\text{ unknown}\n\\]\n\n\nThis implies that information about the cause of missingness is unavailable. This is the most complicated scenario."
  },
  {
    "objectID": "slides/week3-sampling.html#assessing-the-missing-data-mechanism",
    "href": "slides/week3-sampling.html#assessing-the-missing-data-mechanism",
    "title": "Sampling and missingness",
    "section": "Assessing the missing data mechanism",
    "text": "Assessing the missing data mechanism\nImportantly, there is no easy diagnostic check to distinguish MCAR, MAR, and MNAR without measuring some of the missing data.\n\nSo in practice, usually one has to make an informed assumption based on knowledge of the data collection process."
  },
  {
    "objectID": "slides/week3-sampling.html#example-gdp-data",
    "href": "slides/week3-sampling.html#example-gdp-data",
    "title": "Sampling and missingness",
    "section": "Example: GDP data",
    "text": "Example: GDP data\nIn the GDP growth data, growth measurements are missing for many countries before a certain year.\n\nWe might be able to hypothesize about why – perhaps a country didn’t exist or didn’t keep reliable records for a period of time.However, the data as they are contain no additional information that might explain the cause of missingness.\n\n\nSo these data are MNAR."
  },
  {
    "objectID": "slides/week3-sampling.html#simple-fixes",
    "href": "slides/week3-sampling.html#simple-fixes",
    "title": "Sampling and missingness",
    "section": "Simple fixes",
    "text": "Simple fixes\nThe easiest approach to missing data is to drop observations with missing values: df.dropna().\n\nImplicitly assumes data are MCAR\nInduces bias if data are MAR or MNAR\n\n\nAnother simple fix is mean imputation, filling in missing values with the mean of the corresponding variable: df.fillna().\n\nOnly a good idea if a very small proportion of values are missing\nInduces bias if data are MAR or MNAR"
  },
  {
    "objectID": "slides/week3-sampling.html#perils-of-mean-imputation",
    "href": "slides/week3-sampling.html#perils-of-mean-imputation",
    "title": "Sampling and missingness",
    "section": "Perils of mean imputation",
    "text": "Perils of mean imputation\n\nImputing too many missing values distorts the distribution of sample values."
  },
  {
    "objectID": "slides/week3-sampling.html#other-common-approaches-to-missingness",
    "href": "slides/week3-sampling.html#other-common-approaches-to-missingness",
    "title": "Sampling and missingness",
    "section": "Other common approaches to missingness",
    "text": "Other common approaches to missingness\nWhen data are MCAR or MAR, one can:\n\nmodel the probability of missingness and apply bias corrections to estimated quantities using inverse probability weighting\nmodel the variables with missing observations as functions of the other variables and perform model-based imputation"
  },
  {
    "objectID": "slides/week3-sampling.html#dos-and-donts",
    "href": "slides/week3-sampling.html#dos-and-donts",
    "title": "Sampling and missingness",
    "section": "Do’s and don’t’s",
    "text": "Do’s and don’t’s\nDo:\n\nAlways check for missing values upon import.\n\nTabulate the proportion of observations with missingness\nTabulate the proportion of values for each variable that are missing\n\nTake time to find out the reasons data are missing.\n\nDetermine which outcomes are coded as missing.\nInvestigate the physical mechanisms involved.\n\nReport missing data if they are present.\n\nDon’t:\n\nRely on software defaults for handling missing values.\nDrop missing values if data are not MCAR."
  }
]