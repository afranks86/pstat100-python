---
title: "Simple linear regression"
author: "PSTAT100 Spring 2023"
date: "Week 7, Lecture 1"
format: 
    revealjs:
        smaller: true
        incremental: true
        slide-number: true
        scrollable: true
        code-fold: true
jupyter: python3
execute:
    echo: true
    eval: false
---



## Residual distribution

Have a look at the histogram of the residuals (with a KDE curve):

<img src = 'figures/fig5a-residhist.png' style = 'width: 400px'>

Does this look like any probability density function you encountered in 120A?

## Residual distribution

The residual distribution is pretty well-approximated by the *normal* or *Gaussian* distribution:

<img src = 'figures/fig5b-residnormal.png' style = 'width: 400px'>

## The error model

This phenomenon -- nearly normal residuals -- is pretty robust. 

So a standard **error model** for the residuals is that they are independent normal random variables. This is written as:

$$e_i \stackrel{iid}{\sim} N\left(0, \sigma^2\right)$$

This is an important modification because it induces a probability distribution on $y_i$.

In other words, it makes the linear description of $Y$ into a statistical model.

## The simple linear model

Now we're in a position to state the **simple linear model**:

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i \quad\begin{cases} i = 1, \dots, n \\\epsilon_i \sim N\left(0,\sigma^2\right)\end{cases}$$

**Terminology**
* $y_i$ is the _**response variable**_
* $x_i$ is the _**explanatory variable**_
* $\epsilon_i$ is the _**error**_
* $\beta_0, \beta_1, \sigma^2$ are the _**model parameters**_
    + $\beta_0$ is the _**intercept**_
    + $\beta_1$ is the _**coefficient**_
    + $\sigma^2$ is the _**error variance**_

## Model implications

Treating the error term as random has a number of implications.

* (Normality) The response is a normal random variable: $y_i \sim N\left(\beta_0 + \beta_1 x_i, \sigma^2\right)$

* (Linearity) The mean response is linear in $X$: $\mathbb{E}y_i = \beta_0 + \beta_1 x_i$

* (Constant variance) The response has variance: $\text{var}y_i = \sigma^2$

* (Independence) The observations are independent (because the errors are): $y_i \perp y_j$

These are the **assumptions** of the linear model.

*Aside*: other error distributions, or conditions that don't assume a specific distribution, are possible.

## Estimates

The OLS solution has a number of optimality properties with respect to the simple linear model -- in other words, it's the best estimate of the parameters $\beta_0, \beta_1$ under many conditions.

You've already seen how to compute the OLS estimates. These are typically denoted by the corresponding paramater with a hat:

$$\hat{\beta} = \left[\begin{array}{c}\hat{\beta}_0 \\ \hat{\beta}_1 \end{array}\right] = (\mathbf{X'X})^{-1}\mathbf{X'y}$$

An estimate of the error variance is:

$$\hat{\sigma}^2 = \frac{1}{n - 2} \sum_{i = 1}^n \left(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i\right)^2 = \frac{1}{n - 2}\left(\mathbf{y} - \mathbf{X}\hat{\beta}\right)'\left(\mathbf{y} - \mathbf{X}\hat{\beta}\right)$$

## Fitted values and residuals

Once estimates are computed, the projections of the data points onto the line are known as **fitted values**: they are _**the estimated values of the response variable for each data point**_.

These are typically denoted $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$ and computed as:

```{python}
# fitted values
fitted = slr.predict(x)
```

The **model residuals** are then _**the difference between observed and fitted values**_: $y_i - \hat{y}_i$.

## Parameter interpretations

Let's go back to the SEDA example. The parameter estimates were:

```{python}
ols
```

Since $\mathbb{E}y_i = \beta_0 + \beta_1 x_i$, these are interpreted as follows.

* (Intercept) When median district income is 1 dollar ($x_i = 0$), the mean achievement gap ($\mathbb{E}y_i$) is estimated to be 1.356 standard deviations of the national average in favor of girls.
    + (Not of particular interest here because no districts have a median income of 1 USD.)

* (Slope) Every doubling of median income is associated with an estimated increase in the mean achievement gap of 0.084 standard deviations of the national average in favor of boys.
    + $\hat{\beta}_1\log (2x) = \hat{\beta}_1\log x + \hat{\beta}_1 \log 2$

```{python}
#| tags: []
ols[1]*np.log(2)
```

## General parameter interpretations

There is some general language for interpreting the parameter estimates:

* (Intercept) [at $x_i = 0$] the mean [response variable] is estimated to be [$\hat{\beta}_0$ units].

* (Slope) Every [one-unit increase in $x_i$] is associated with an estimated change in mean [response variable] of [$\hat{\beta}_1$ units].

You can use this standard language as a formulaic template for interpreting estimated parameters.

## Uncertainty quantification

A great benefit of the simple linear model relative to a best-fit line is that the error variance allows for *uncertainty quantification*.

What that means is that one can describe precisely:

* variation in the estimates (*i.e.*, estimated model reliability);

* variation in predictions made using the estimated model (*i.e.*, predictive reliability).

## Understanding variation in estimates

What would happen to the estimates if they were computed from a different sample?

We can explore this idea a little by calculating OLS estimates from *subsamples* of the dataset.

<img src = 'figures/fig6-subsamples.png' style = 'width: 400px'>

The lines are pretty similar, but they change a bit from subsample to subsample.

So, a useful question is: *by how much should one expect the estimates to change depending on the data they are fit to?*

## Variance of parameter estimates

Under the simple linear model, the estimated parameters have calculable variances.

It can be shown that:

$$\left[\begin{array}{cc} 
    \text{var}\hat{\beta}_0 & \text{cov}\left(\hat{\beta}_0, \hat{\beta}_1\right) \\ 
    \text{cov}\left(\hat{\beta}_0, \hat{\beta}_1\right) &\text{var}\hat{\beta}_1
    \end{array}\right]
   = \sigma^2 \left(\mathbf{X'X}\right)^{-1}$$
   

So the variances can be *estimated* by plugging in $\color{red}{\hat{\sigma}^2}$. The estimated standard deviations are known as *standard errors*:

$$\text{SE}(\hat{\beta}_0) = \sqrt{\color{red}{\hat{\sigma}^2}(\mathbf{X'X})^{-1}_{11}} \qquad\text{and}\qquad \text{SE}(\hat{\beta}_1) = \sqrt{\color{red}{\hat{\sigma}^2}(\mathbf{X'X})^{-1}_{22}}$$

About 95% of the time, the true values will be within 2SE of any particular estimates.

## So was the gap estimate a fluke?

```{python}
# residuals
resid = y - fitted

# residual SE
n = len(x)
p = 2
resid_se = np.sqrt(resid.var()*(n - 1)/(n - p))

# coefficient variances/covariances
x_mx = np.vstack([np.repeat(1, n), x[:, 0]]).transpose()
coef_vcov = np.linalg.inv(x_mx.transpose().dot(x_mx))*(resid_se**2)

# coefficient standard errors
coef_se = np.sqrt(coef_vcov.diagonal())

# coefficient intervals
np.vstack([ols + 2*coef_se, ols - 2*coef_se])
```

## Visual display of uncertainty quantification

Often it's easier to get the message across with a plot. 

It's fairly common practice to add a *band* around the plotted line to indicate estimated variability.

<img src = 'figures/fig7-uncertaintyband.png' style = 'width: 500px'>

## Prediction

Predictions for a district not in the dataset can be calculated by simply plugging in the explanatory variable for the new observation. 

If for example, we'd like to predict the mean achievement gap in math for a district with a median income of 86K, we'd use:
$$x_{new} = \log(86000)$$
And compute:
$$\hat{y}_{new} = \hat{\beta}_0 + \hat{\beta}_1\log(86000) = \left[\begin{array}{cc} 1 &\log(86000) \end{array}\right]\left[\begin{array}{c}\hat{\beta}_0 \\ \hat{\beta}_1\end{array}\right]$$

```{python}
#| slideshow: {slide_type: fragment}
#| tags: []
# prediction
newobs = np.array([1, np.log(86000)])
pred = ols.dot(newobs)
pred
```

## Prediction uncertainty

The variance of a predicted observation is given by:
$$\text{var}(\hat{y}_{new}) = \sigma^2\left( 1 + \mathbf{x}_{new}'(\mathbf{X'X})^{-1}\mathbf{x}_{new}\right)$$

So the estimated standard deviation of the prediction is:
$$SE\left(\hat{y}_{new}\right) = \sqrt{\hat{\sigma}^2\left( 1 + \mathbf{x}_{new}'(\mathbf{X'X})^{-1}\mathbf{x}_{new}\right)}$$

```{python}
#| slideshow: {slide_type: fragment}
#| tags: []
pred_se = np.sqrt((resid_se**2)*(1 + newobs.dot(xtx_inv).dot(newobs)))
pred_se
```

Again, about 95% of the time the true value will be within 2SE of the estimate. So our *uncertainty* about the prediction is: 

```{python}
#| slideshow: {slide_type: fragment}
#| tags: []
[pred - 2*pred_se, pred + 2*pred_se]
```

## Prediction uncertainty

The prediction uncertainty is considerable, but consistent with the variability we see in the data.

<img src = 'figures/fig8-prediction.png' style = 'width: 500px'>

# Summary

This was our first week on statistical models.

* A statistical model is a probability distribution representing a data-generating process.

* The distributions you know and love from PSTAT120A are all simple models.

Our focus was on the **simple linear model**, according to which _**one variable of interest is a linear function of another variable and a random error**_.

* Parameters are estimated by minimizing residual variance (least squares).

* The model assumes normality, linearity, constant variance, and independence.

* Useful for both prediction and inference.

* Estimated variance allows for uncertainty quantification.

Next week we'll discuss extending this model to cases with *multiple* explanatory variables.

